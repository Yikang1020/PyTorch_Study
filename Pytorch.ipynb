{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NN().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.292950  [   64/60000]\n",
      "loss: 2.296004  [ 6464/60000]\n",
      "loss: 2.299907  [12864/60000]\n",
      "loss: 2.310343  [19264/60000]\n",
      "loss: 2.309770  [25664/60000]\n",
      "loss: 2.311912  [32064/60000]\n",
      "loss: 2.303707  [38464/60000]\n",
      "loss: 2.302188  [44864/60000]\n",
      "loss: 2.303886  [51264/60000]\n",
      "loss: 2.328280  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 2.306381 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.292950  [   64/60000]\n",
      "loss: 2.296004  [ 6464/60000]\n",
      "loss: 2.299907  [12864/60000]\n",
      "loss: 2.310343  [19264/60000]\n",
      "loss: 2.309770  [25664/60000]\n",
      "loss: 2.311912  [32064/60000]\n",
      "loss: 2.303707  [38464/60000]\n",
      "loss: 2.302188  [44864/60000]\n",
      "loss: 2.303886  [51264/60000]\n",
      "loss: 2.328280  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 2.306381 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.292950  [   64/60000]\n",
      "loss: 2.296004  [ 6464/60000]\n",
      "loss: 2.299907  [12864/60000]\n",
      "loss: 2.310343  [19264/60000]\n",
      "loss: 2.309770  [25664/60000]\n",
      "loss: 2.311912  [32064/60000]\n",
      "loss: 2.303707  [38464/60000]\n",
      "loss: 2.302188  [44864/60000]\n",
      "loss: 2.303886  [51264/60000]\n",
      "loss: 2.328280  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 2.306381 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.292950  [   64/60000]\n",
      "loss: 2.296004  [ 6464/60000]\n",
      "loss: 2.299907  [12864/60000]\n",
      "loss: 2.310343  [19264/60000]\n",
      "loss: 2.309770  [25664/60000]\n",
      "loss: 2.311912  [32064/60000]\n",
      "loss: 2.303707  [38464/60000]\n",
      "loss: 2.302188  [44864/60000]\n",
      "loss: 2.303886  [51264/60000]\n",
      "loss: 2.328280  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 2.306381 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.292950  [   64/60000]\n",
      "loss: 2.296004  [ 6464/60000]\n",
      "loss: 2.299907  [12864/60000]\n",
      "loss: 2.310343  [19264/60000]\n",
      "loss: 2.309770  [25664/60000]\n",
      "loss: 2.311912  [32064/60000]\n",
      "loss: 2.303707  [38464/60000]\n",
      "loss: 2.302188  [44864/60000]\n",
      "loss: 2.303886  [51264/60000]\n",
      "loss: 2.328280  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 2.306381 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument input is on cpu but expected on mps",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m---> 16\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_relu_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor for argument input is on cpu but expected on mps"
     ]
    }
   ],
   "source": [
    "# \n",
    "summary(model.to_device(), input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: linear_relu_stack.0.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0198, -0.0240, -0.0281,  ..., -0.0026,  0.0186,  0.0172],\n",
      "        [-0.0209, -0.0290,  0.0044,  ...,  0.0346, -0.0354,  0.0079],\n",
      "        [-0.0127, -0.0175, -0.0039,  ..., -0.0025, -0.0199,  0.0257],\n",
      "        ...,\n",
      "        [-0.0224,  0.0343, -0.0352,  ...,  0.0286, -0.0338,  0.0129],\n",
      "        [ 0.0190,  0.0294,  0.0186,  ..., -0.0056, -0.0158,  0.0142],\n",
      "        [-0.0340, -0.0249, -0.0080,  ...,  0.0003, -0.0034, -0.0322]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512, 784])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.0.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([-2.8885e-02,  3.0724e-02,  2.9379e-02,  1.8816e-02, -2.6011e-02,\n",
      "         3.3328e-03, -8.3296e-03,  2.3787e-02,  1.0857e-02, -1.3107e-02,\n",
      "        -1.2003e-02, -2.3429e-02,  2.5075e-02, -2.0783e-02,  3.2708e-02,\n",
      "        -3.0691e-03, -2.0024e-02, -3.4753e-02,  3.2016e-02, -1.2303e-02,\n",
      "         3.3830e-02,  2.0743e-02,  1.3980e-02,  1.5055e-02, -2.6451e-02,\n",
      "        -1.3471e-02, -6.7056e-03, -3.0503e-02,  3.6014e-02,  3.1312e-02,\n",
      "         5.1512e-03,  1.1884e-02,  3.5617e-02, -2.7165e-02, -2.5750e-03,\n",
      "        -4.1776e-04,  4.0322e-02,  2.3939e-02, -1.2212e-02, -1.6722e-02,\n",
      "         3.3738e-02, -2.0785e-02, -2.2053e-02,  3.7438e-02,  2.5169e-02,\n",
      "         1.6158e-02, -2.7755e-02,  1.1634e-02,  5.5876e-03,  3.6070e-02,\n",
      "         1.4774e-02, -3.1028e-02,  3.3399e-02, -2.2150e-02,  3.4895e-02,\n",
      "         2.8568e-03,  3.2175e-02, -1.1923e-02, -1.9742e-02,  1.8329e-02,\n",
      "        -1.4004e-02, -1.7513e-03,  2.2808e-02, -1.2054e-02, -2.7491e-02,\n",
      "        -2.4403e-02, -3.0005e-02,  2.1457e-02, -2.7426e-02,  1.7093e-03,\n",
      "        -6.1711e-03,  1.9490e-02, -2.9483e-02, -1.5260e-02,  9.6598e-03,\n",
      "         3.5024e-02,  2.8639e-02,  8.2873e-03,  6.6051e-03,  2.4135e-02,\n",
      "        -1.7530e-02,  3.6306e-02,  2.8074e-02,  9.3366e-03,  1.1191e-02,\n",
      "        -6.3301e-03,  1.1603e-02,  3.9490e-03, -1.2593e-02, -2.5030e-02,\n",
      "        -1.0173e-02, -2.1718e-02,  3.4006e-02,  4.6784e-03,  4.6822e-03,\n",
      "        -2.0531e-02, -2.0127e-02,  2.7868e-02, -1.6329e-03,  2.4791e-02,\n",
      "        -1.7827e-02, -1.8382e-02, -3.3869e-02, -2.4621e-02,  1.1351e-02,\n",
      "         1.5103e-02, -1.6036e-02, -1.6023e-02,  2.5024e-02, -7.4925e-04,\n",
      "        -5.4683e-03, -1.0623e-03,  4.1791e-02, -5.8408e-03,  2.0713e-02,\n",
      "         6.8200e-03,  2.1469e-02,  1.4076e-02, -1.3525e-02, -1.6181e-02,\n",
      "         1.3109e-02, -1.7925e-02, -1.5160e-02, -2.3532e-02, -1.1491e-02,\n",
      "        -1.1569e-02,  3.1583e-02, -2.3851e-02, -2.4035e-02, -2.8872e-02,\n",
      "         6.3931e-03,  1.0111e-02,  2.1893e-02, -1.6499e-02, -1.9316e-02,\n",
      "         2.5782e-02, -2.2701e-02,  1.0442e-02,  1.0493e-02, -2.5850e-02,\n",
      "        -2.6655e-02, -2.6080e-02,  7.1040e-03,  1.4631e-02, -2.9316e-03,\n",
      "        -1.0732e-02,  3.4686e-02,  6.5262e-03, -3.2515e-02,  7.0410e-03,\n",
      "        -3.1962e-03,  2.2169e-02,  1.8243e-02,  2.3066e-02,  8.5594e-03,\n",
      "        -1.0576e-02, -6.9825e-03, -2.8902e-02, -7.8992e-03, -3.4704e-02,\n",
      "         1.2152e-02,  1.1768e-02, -2.7994e-02,  3.1563e-03, -1.4162e-02,\n",
      "         2.1577e-03,  2.7977e-02,  2.3065e-02,  2.0058e-03, -1.4845e-02,\n",
      "         1.8814e-02, -3.0981e-02,  4.1021e-03, -1.5057e-02, -3.2262e-03,\n",
      "        -2.8094e-02, -6.2735e-03, -1.2439e-02,  1.6888e-02,  1.4186e-02,\n",
      "         2.5471e-03, -4.9098e-03, -2.4063e-02,  6.8739e-03,  5.5835e-03,\n",
      "        -2.0138e-03, -3.0230e-02,  8.9935e-03, -3.1582e-03, -2.6402e-03,\n",
      "         6.9623e-03,  1.4445e-02,  1.2577e-02, -3.0398e-02, -4.1235e-03,\n",
      "         3.4546e-02, -1.7241e-02, -1.5494e-02,  3.6180e-02, -4.2242e-03,\n",
      "         7.8436e-03,  2.5121e-02,  3.5520e-02,  5.6604e-03, -1.3966e-02,\n",
      "         2.3481e-02,  3.5543e-02, -1.6268e-02,  2.7687e-02,  1.3167e-02,\n",
      "        -2.5286e-02,  2.5845e-02, -1.4561e-02,  2.1928e-02, -7.3988e-03,\n",
      "         1.5272e-02,  4.3983e-03, -9.9506e-03, -1.9154e-02, -1.4455e-02,\n",
      "         3.3836e-02, -9.5681e-03, -1.2928e-02, -1.3505e-02,  3.5628e-02,\n",
      "         3.3122e-02,  3.4281e-02,  3.2445e-03,  1.0230e-02,  3.1368e-02,\n",
      "         1.0250e-02,  3.5164e-02, -8.1767e-03, -8.6655e-03,  5.5308e-04,\n",
      "        -6.4203e-05, -3.2897e-02, -3.4336e-04,  1.6520e-02,  1.3061e-02,\n",
      "         1.1053e-02, -1.1236e-02,  2.2638e-02, -2.9824e-02,  2.9866e-02,\n",
      "         1.3034e-02,  6.1655e-03,  3.6948e-02, -4.8140e-03, -3.1251e-02,\n",
      "         3.0594e-02,  7.0358e-03, -1.6830e-02, -4.2707e-03,  1.9405e-02,\n",
      "        -2.0211e-02, -3.2937e-02,  2.9705e-02, -4.6424e-03, -1.1979e-03,\n",
      "        -4.9173e-03,  2.3950e-02, -1.6355e-03,  1.8959e-02, -1.5052e-02,\n",
      "        -1.7156e-02,  2.6852e-02,  1.1895e-02, -1.6650e-02,  2.5973e-02,\n",
      "         2.5724e-02, -1.9650e-02, -2.6373e-02,  3.0847e-02, -9.9067e-03,\n",
      "         3.0696e-03,  1.8842e-02,  1.4721e-03,  2.4017e-02,  4.5269e-03,\n",
      "         3.0340e-02,  5.4461e-03, -9.0629e-03, -2.9003e-02, -2.8581e-02,\n",
      "         2.9850e-03,  9.4316e-03, -2.5414e-02, -5.2373e-03,  1.4917e-02,\n",
      "        -1.3218e-02, -4.2471e-03,  1.4559e-03, -2.0003e-02,  9.7698e-03,\n",
      "         2.8398e-02, -2.2471e-02,  3.8406e-03,  4.4285e-03,  1.2415e-03,\n",
      "         2.7740e-02,  2.7810e-02,  3.1017e-02,  2.6432e-03,  1.8180e-02,\n",
      "         7.0939e-03,  3.0187e-02,  2.6374e-02, -8.2700e-03, -5.5931e-03,\n",
      "        -1.0230e-03,  1.2614e-03, -7.2946e-03,  3.5336e-02, -3.6630e-03,\n",
      "         3.7084e-02,  3.0300e-02, -2.8261e-02, -1.4402e-02,  3.8279e-02,\n",
      "        -4.2088e-03, -3.4139e-04,  3.2320e-02, -3.4863e-02, -2.7873e-02,\n",
      "        -8.1019e-03,  3.0302e-02, -4.0284e-03,  2.9326e-02,  4.7302e-03,\n",
      "         2.1705e-02,  2.4218e-02, -1.8512e-02, -1.1455e-02,  1.0339e-02,\n",
      "        -1.2048e-03, -7.4692e-03,  2.9824e-03,  2.2866e-02,  2.2639e-02,\n",
      "         7.9727e-03, -3.1644e-02,  1.3154e-02,  1.8240e-02,  2.3594e-02,\n",
      "         2.1785e-02,  7.6093e-03,  1.1939e-02,  1.6937e-02,  9.6077e-03,\n",
      "         3.0290e-02, -2.0593e-02,  7.1623e-03,  1.1760e-02,  3.6298e-02,\n",
      "        -1.2266e-02,  1.6029e-02, -2.7689e-02,  1.3499e-02,  1.9047e-02,\n",
      "         3.3577e-02, -8.9625e-04,  1.4711e-02,  3.4088e-02, -9.8882e-03,\n",
      "        -2.0196e-02, -3.7565e-03,  3.2150e-03, -9.8563e-04, -1.1741e-02,\n",
      "        -2.3580e-03, -1.7564e-02, -1.9057e-02,  1.5487e-03, -1.3509e-02,\n",
      "         2.6124e-04,  6.3576e-03,  3.9402e-02, -2.0310e-02,  1.6905e-02,\n",
      "        -1.0779e-02, -5.9845e-03,  2.6781e-02,  2.9004e-02, -2.5973e-02,\n",
      "        -7.0098e-03,  9.4057e-03, -3.0828e-02, -1.0302e-02,  3.2541e-02,\n",
      "         5.2583e-03, -2.3972e-02,  5.2427e-03,  3.3820e-02, -7.5161e-03,\n",
      "        -3.0183e-02,  1.3372e-02, -1.0813e-02, -2.7164e-02, -1.0489e-02,\n",
      "         2.6978e-02,  4.1580e-02,  1.9987e-03, -8.5008e-03,  6.9476e-03,\n",
      "         2.8399e-02,  1.7234e-02, -1.8533e-02, -1.8766e-02,  3.7408e-02,\n",
      "         8.3304e-03, -1.5013e-03, -3.1446e-02,  4.8599e-03, -3.2452e-02,\n",
      "        -2.7679e-02,  6.7277e-04,  1.0489e-02,  2.5651e-02,  1.7761e-02,\n",
      "        -7.8822e-03,  6.4094e-03,  2.5847e-02,  1.6605e-02,  1.8394e-02,\n",
      "         5.2180e-03, -2.0659e-02, -2.1515e-02, -3.2425e-02, -1.8166e-02,\n",
      "        -2.1502e-02,  5.8326e-03, -2.9944e-02, -2.7053e-02,  8.5739e-03,\n",
      "        -1.0595e-03,  2.9983e-03, -1.2467e-02, -1.8524e-02,  2.7569e-02,\n",
      "        -5.8544e-03,  2.5351e-02, -2.0476e-02,  2.4097e-02, -3.0785e-02,\n",
      "        -2.4125e-02,  8.4826e-03,  3.6627e-02,  1.4092e-02, -3.1241e-02,\n",
      "         1.4641e-02, -9.2336e-03,  3.2551e-02,  3.6745e-03,  1.2485e-02,\n",
      "         1.0133e-02, -2.5361e-02, -3.8566e-03,  2.2444e-02, -8.8475e-03,\n",
      "        -2.3687e-02, -6.7430e-03, -1.4400e-02, -5.0400e-03, -2.9173e-02,\n",
      "        -1.3624e-02,  4.3945e-03,  1.6449e-02,  8.2914e-03,  9.6643e-03,\n",
      "        -1.8844e-02, -2.3437e-02, -6.4097e-03,  3.6470e-02,  6.2956e-03,\n",
      "        -2.1506e-02, -1.6173e-02,  2.0377e-02,  1.1097e-02, -1.9339e-02,\n",
      "         2.5463e-02,  3.0768e-02, -2.5754e-02, -3.7889e-03,  8.0438e-03,\n",
      "        -3.3928e-02,  4.3208e-03, -9.5974e-03,  1.7085e-02,  4.1546e-03,\n",
      "        -2.2891e-02, -1.3194e-02,  1.4833e-04, -1.5695e-03, -2.1336e-02,\n",
      "        -1.9836e-02,  1.0900e-02,  2.1443e-02,  2.2002e-02,  2.6338e-02,\n",
      "        -1.1083e-02, -2.0195e-02,  2.5126e-02,  3.4052e-02, -1.6206e-02,\n",
      "        -2.2948e-02,  2.6781e-02, -1.6364e-02,  2.6838e-02, -3.3768e-02,\n",
      "        -7.8922e-03, -1.7609e-02], device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0243, -0.0260,  0.0411,  ...,  0.0092, -0.0310, -0.0160],\n",
      "        [-0.0020, -0.0180,  0.0329,  ..., -0.0138,  0.0131, -0.0147],\n",
      "        [ 0.0244, -0.0315,  0.0422,  ...,  0.0165,  0.0201,  0.0302],\n",
      "        ...,\n",
      "        [-0.0096, -0.0282, -0.0193,  ..., -0.0136,  0.0162, -0.0244],\n",
      "        [ 0.0410, -0.0399,  0.0150,  ..., -0.0260,  0.0107, -0.0170],\n",
      "        [-0.0079, -0.0319, -0.0365,  ...,  0.0231,  0.0038, -0.0056]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512, 512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([ 0.0229, -0.0127, -0.0110, -0.0433, -0.0371, -0.0429,  0.0219,  0.0418,\n",
      "        -0.0069,  0.0366, -0.0199,  0.0164,  0.0279, -0.0311, -0.0042, -0.0179,\n",
      "         0.0192, -0.0104, -0.0349, -0.0180, -0.0337, -0.0136,  0.0202,  0.0451,\n",
      "        -0.0183,  0.0165,  0.0378, -0.0404, -0.0102, -0.0192, -0.0204,  0.0354,\n",
      "         0.0062, -0.0243,  0.0288,  0.0246, -0.0108,  0.0261, -0.0340, -0.0313,\n",
      "        -0.0158,  0.0084, -0.0121,  0.0295,  0.0070,  0.0430, -0.0136, -0.0180,\n",
      "         0.0292, -0.0409,  0.0218, -0.0064,  0.0024, -0.0006,  0.0112,  0.0418,\n",
      "        -0.0113,  0.0255, -0.0260, -0.0004, -0.0126,  0.0247,  0.0133,  0.0104,\n",
      "         0.0172,  0.0247,  0.0024,  0.0153,  0.0012,  0.0029, -0.0143,  0.0069,\n",
      "         0.0156, -0.0367, -0.0438, -0.0011,  0.0237,  0.0225,  0.0313, -0.0072,\n",
      "         0.0476,  0.0137,  0.0344, -0.0019,  0.0313,  0.0312, -0.0257, -0.0208,\n",
      "         0.0003,  0.0103,  0.0011,  0.0330, -0.0378,  0.0265, -0.0117,  0.0312,\n",
      "         0.0049, -0.0176, -0.0103,  0.0294, -0.0348, -0.0154, -0.0237, -0.0225,\n",
      "        -0.0231, -0.0113, -0.0233, -0.0195, -0.0112,  0.0380,  0.0342, -0.0050,\n",
      "         0.0246,  0.0057,  0.0474,  0.0276,  0.0430, -0.0050, -0.0197, -0.0322,\n",
      "        -0.0330, -0.0121, -0.0342,  0.0422, -0.0329, -0.0245,  0.0375, -0.0112,\n",
      "         0.0461,  0.0214,  0.0164,  0.0371,  0.0156,  0.0040,  0.0093,  0.0130,\n",
      "         0.0458, -0.0133, -0.0146,  0.0306,  0.0391, -0.0051, -0.0034, -0.0025,\n",
      "        -0.0033,  0.0216,  0.0354,  0.0355,  0.0356, -0.0164, -0.0033, -0.0283,\n",
      "        -0.0300, -0.0018, -0.0063, -0.0115,  0.0244,  0.0458, -0.0248, -0.0080,\n",
      "        -0.0320,  0.0462, -0.0409,  0.0096,  0.0167,  0.0008, -0.0306,  0.0400,\n",
      "        -0.0249,  0.0021,  0.0068, -0.0065,  0.0176,  0.0122, -0.0247,  0.0410,\n",
      "         0.0311,  0.0050,  0.0359, -0.0142,  0.0361,  0.0181, -0.0174,  0.0403,\n",
      "         0.0203,  0.0110,  0.0361,  0.0193,  0.0149,  0.0262, -0.0291,  0.0208,\n",
      "        -0.0350,  0.0307,  0.0305, -0.0239, -0.0414,  0.0014, -0.0318,  0.0043,\n",
      "         0.0427,  0.0146, -0.0005, -0.0388, -0.0314, -0.0002,  0.0157,  0.0316,\n",
      "         0.0039,  0.0304,  0.0315, -0.0054, -0.0129,  0.0441, -0.0159, -0.0020,\n",
      "         0.0183, -0.0037, -0.0322, -0.0148, -0.0162, -0.0176,  0.0056, -0.0001,\n",
      "        -0.0192, -0.0375, -0.0131, -0.0035, -0.0095,  0.0030,  0.0348, -0.0073,\n",
      "         0.0024,  0.0321, -0.0075, -0.0418,  0.0344,  0.0196, -0.0185, -0.0412,\n",
      "         0.0014,  0.0373, -0.0444,  0.0070, -0.0043, -0.0450,  0.0399,  0.0140,\n",
      "        -0.0375,  0.0079,  0.0311,  0.0283, -0.0026,  0.0156, -0.0438,  0.0249,\n",
      "         0.0246,  0.0142,  0.0241,  0.0374, -0.0039,  0.0347, -0.0442,  0.0301,\n",
      "         0.0035,  0.0415, -0.0143, -0.0115, -0.0207, -0.0128, -0.0322,  0.0174,\n",
      "         0.0207,  0.0273, -0.0009, -0.0330, -0.0173,  0.0229,  0.0358,  0.0357,\n",
      "         0.0060,  0.0437, -0.0033, -0.0124, -0.0334,  0.0002, -0.0350,  0.0084,\n",
      "         0.0045,  0.0148, -0.0370, -0.0022,  0.0116, -0.0365,  0.0063, -0.0118,\n",
      "         0.0408, -0.0172, -0.0054,  0.0120,  0.0001, -0.0400, -0.0057, -0.0173,\n",
      "        -0.0427,  0.0275,  0.0052, -0.0166,  0.0035, -0.0079, -0.0152, -0.0129,\n",
      "        -0.0135, -0.0163, -0.0061,  0.0304, -0.0159, -0.0249, -0.0122, -0.0179,\n",
      "         0.0106,  0.0111,  0.0181,  0.0007,  0.0157,  0.0084,  0.0200, -0.0347,\n",
      "        -0.0191,  0.0193, -0.0264, -0.0189, -0.0056, -0.0145, -0.0309, -0.0075,\n",
      "        -0.0119,  0.0063, -0.0434, -0.0003,  0.0215, -0.0133,  0.0353,  0.0124,\n",
      "         0.0381, -0.0232,  0.0189, -0.0025,  0.0326, -0.0198, -0.0417,  0.0101,\n",
      "         0.0169, -0.0150, -0.0345, -0.0086, -0.0377, -0.0331,  0.0373,  0.0046,\n",
      "        -0.0210, -0.0237,  0.0022,  0.0264, -0.0144,  0.0440,  0.0318,  0.0212,\n",
      "         0.0338,  0.0213,  0.0012, -0.0363, -0.0137, -0.0124, -0.0070,  0.0273,\n",
      "        -0.0293,  0.0189,  0.0029, -0.0189, -0.0303,  0.0284, -0.0288, -0.0278,\n",
      "        -0.0024, -0.0067,  0.0095, -0.0433,  0.0396, -0.0447,  0.0454, -0.0150,\n",
      "         0.0137,  0.0222, -0.0419, -0.0298, -0.0181, -0.0255,  0.0301,  0.0091,\n",
      "         0.0049,  0.0107,  0.0071, -0.0440,  0.0259, -0.0430, -0.0051,  0.0441,\n",
      "         0.0449,  0.0130,  0.0374,  0.0330, -0.0216, -0.0309,  0.0147, -0.0390,\n",
      "        -0.0283, -0.0111,  0.0133, -0.0416,  0.0042, -0.0125, -0.0225,  0.0006,\n",
      "         0.0133, -0.0212, -0.0001,  0.0339,  0.0448, -0.0243, -0.0127,  0.0321,\n",
      "         0.0429, -0.0198,  0.0096, -0.0344, -0.0333, -0.0437, -0.0188,  0.0020,\n",
      "         0.0378, -0.0329, -0.0290, -0.0348,  0.0417, -0.0395, -0.0432,  0.0386,\n",
      "        -0.0055,  0.0476, -0.0362,  0.0272,  0.0366, -0.0187,  0.0309,  0.0528,\n",
      "        -0.0199, -0.0188, -0.0408, -0.0384,  0.0309, -0.0352, -0.0246, -0.0389,\n",
      "        -0.0030, -0.0434,  0.0206, -0.0371, -0.0405, -0.0218, -0.0190, -0.0308,\n",
      "         0.0206, -0.0348, -0.0057,  0.0233, -0.0277,  0.0061, -0.0213, -0.0046,\n",
      "         0.0165, -0.0080,  0.0299,  0.0172,  0.0451, -0.0050,  0.0302,  0.0428,\n",
      "        -0.0021,  0.0365,  0.0245, -0.0442, -0.0359, -0.0078,  0.0214, -0.0008,\n",
      "         0.0436, -0.0366,  0.0389,  0.0187, -0.0203,  0.0246,  0.0153,  0.0082,\n",
      "        -0.0039, -0.0364,  0.0096, -0.0174,  0.0395, -0.0437, -0.0338, -0.0081],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0111, -0.0099,  0.0434,  ..., -0.0018, -0.0055, -0.0068],\n",
      "        [ 0.0382, -0.0060,  0.0218,  ..., -0.0421, -0.0082, -0.0253],\n",
      "        [ 0.0442, -0.0180, -0.0276,  ...,  0.0168, -0.0266, -0.0434],\n",
      "        ...,\n",
      "        [-0.0370, -0.0414,  0.0311,  ..., -0.0035,  0.0178,  0.0112],\n",
      "        [-0.0197,  0.0046, -0.0029,  ..., -0.0168,  0.0298,  0.0347],\n",
      "        [ 0.0007, -0.0411, -0.0018,  ..., -0.0355, -0.0071,  0.0419]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512, 512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([ 1.7318e-02, -7.0786e-03,  5.1649e-02, -4.2493e-02, -3.2445e-02,\n",
      "        -1.8458e-02, -2.1479e-02,  3.8223e-02,  2.1537e-02,  2.0985e-02,\n",
      "         2.0438e-02, -1.2273e-02, -1.4187e-02,  4.1451e-02, -3.9474e-02,\n",
      "        -1.8199e-02,  1.9097e-02,  3.5826e-02, -4.3577e-03,  1.9929e-02,\n",
      "         2.4040e-03, -4.2782e-02, -9.7647e-03, -3.8371e-02,  3.8025e-02,\n",
      "        -2.2879e-02,  2.1958e-02, -3.4801e-02, -2.8919e-02,  9.4573e-03,\n",
      "        -1.4233e-02, -1.4332e-02,  1.6035e-03,  2.7010e-02,  2.5684e-02,\n",
      "        -2.2337e-03, -1.3826e-02, -1.7289e-02,  2.9542e-02,  3.6123e-02,\n",
      "        -3.6980e-02,  3.8812e-02,  7.9216e-03,  1.3671e-02,  2.6618e-02,\n",
      "        -4.8472e-04, -4.3931e-03, -1.1935e-02,  5.8692e-02,  3.2235e-02,\n",
      "         1.5881e-02, -1.2363e-02, -2.8166e-02, -1.9381e-02,  3.2088e-03,\n",
      "        -3.0191e-02,  4.3764e-02,  2.0722e-02,  4.9771e-03, -2.0293e-02,\n",
      "        -3.2640e-02, -1.5983e-02, -3.1433e-03,  2.8204e-02, -4.5101e-02,\n",
      "        -5.7470e-04,  3.4318e-02, -5.3816e-03,  1.4172e-03,  3.9414e-02,\n",
      "        -7.5540e-03,  2.8211e-02, -2.4182e-02, -4.1158e-03,  3.7236e-02,\n",
      "        -1.3803e-02, -1.2562e-02,  1.1383e-02,  3.6048e-02, -3.6543e-02,\n",
      "         3.5460e-03, -4.1006e-02,  1.9022e-02,  1.2054e-02, -3.3377e-02,\n",
      "         4.0055e-02,  5.7994e-02, -3.7058e-02,  4.2803e-02,  1.9083e-02,\n",
      "         6.9325e-03, -3.2383e-02, -1.4928e-02,  3.7061e-02, -1.8199e-02,\n",
      "         3.3508e-02, -2.8972e-02, -2.2529e-02, -9.1809e-03, -4.0686e-02,\n",
      "         2.9859e-02, -3.0697e-02,  1.6391e-03, -1.3039e-02, -6.2488e-04,\n",
      "        -1.7751e-02, -9.3508e-03, -1.1345e-02, -7.5826e-03,  3.0295e-02,\n",
      "         4.8416e-04,  2.2127e-02, -1.1442e-03,  9.9213e-03,  4.7730e-03,\n",
      "         1.6236e-02,  8.3317e-03,  5.1639e-02, -2.1522e-02,  2.9450e-03,\n",
      "        -3.2094e-02,  5.0561e-02,  2.2980e-02,  1.2752e-02,  2.8797e-02,\n",
      "        -8.3027e-03,  2.8510e-02, -1.3843e-02, -2.0121e-02,  4.5677e-02,\n",
      "         4.4034e-02,  2.9158e-02, -5.6465e-03, -3.7396e-02,  1.6665e-02,\n",
      "         3.8218e-02, -1.8409e-02, -1.7208e-02, -1.6851e-03, -4.1556e-02,\n",
      "         1.2530e-02,  6.5586e-03, -1.8510e-02,  3.8474e-02,  4.5585e-02,\n",
      "        -2.4897e-02, -2.4011e-02, -2.2820e-02, -2.0322e-02, -4.5782e-02,\n",
      "         4.0973e-02, -1.5832e-02,  4.9533e-02,  3.7102e-02,  4.1319e-02,\n",
      "         1.9629e-02, -1.2717e-02,  1.4209e-02, -4.2959e-02,  3.6806e-02,\n",
      "         3.2707e-02, -9.2717e-03, -3.2657e-02,  1.6861e-02,  4.5318e-02,\n",
      "         4.6716e-02,  3.5540e-02, -3.6700e-02,  1.3706e-02,  1.6120e-03,\n",
      "         2.1601e-02,  3.9209e-02, -3.0604e-02,  3.8311e-02, -7.1445e-03,\n",
      "         2.9508e-02,  5.2320e-02, -3.8854e-03,  1.1641e-03,  1.1876e-02,\n",
      "         4.0344e-02,  1.7832e-02, -4.0858e-02,  2.8024e-02,  2.5785e-02,\n",
      "        -7.5955e-03,  2.6049e-02, -1.6633e-02, -3.1560e-02, -2.3155e-02,\n",
      "        -1.5899e-02, -2.7093e-02, -3.2596e-02,  5.0151e-02,  3.4519e-02,\n",
      "         1.2227e-02,  2.9517e-05, -3.6431e-02,  1.9964e-03,  4.2379e-02,\n",
      "        -2.2897e-02, -4.0681e-02, -1.1113e-02, -2.3959e-02,  1.4136e-03,\n",
      "         1.1196e-02,  1.7967e-04, -3.8054e-02, -3.7807e-02, -1.5872e-02,\n",
      "         3.0692e-02,  2.7421e-02, -2.8844e-02,  2.1159e-02,  3.9894e-02,\n",
      "        -2.2613e-02, -2.0598e-02, -8.7994e-03,  3.5445e-03,  2.0905e-02,\n",
      "         5.0495e-02,  1.3552e-02, -3.5824e-02,  2.7904e-02, -7.2040e-03,\n",
      "        -3.4532e-03,  2.2455e-02,  1.4797e-02,  5.5636e-03, -2.9176e-02,\n",
      "         1.3649e-02,  3.1527e-02, -2.4141e-02,  3.8623e-02,  1.1454e-02,\n",
      "        -3.3364e-02,  6.1380e-03,  2.3510e-03, -2.2466e-02, -3.4898e-02,\n",
      "        -1.6551e-02, -3.4108e-02,  4.0410e-02,  2.8591e-02,  1.4965e-02,\n",
      "        -4.0670e-02,  4.9752e-02, -2.3617e-02,  3.5341e-02, -2.3345e-02,\n",
      "        -1.3766e-02,  1.8356e-02, -3.8041e-02, -8.8572e-03, -4.1481e-02,\n",
      "        -3.5497e-02, -4.3750e-03, -4.5579e-02,  2.0809e-02,  3.7414e-02,\n",
      "         9.8663e-03, -3.6341e-03, -1.1312e-02, -2.2951e-02,  1.9252e-02,\n",
      "         7.5982e-03,  3.3520e-02, -3.8377e-02, -1.6939e-02, -1.1220e-02,\n",
      "        -3.2882e-02, -2.9923e-02,  2.7348e-02,  2.0158e-02, -4.2816e-02,\n",
      "         1.2838e-03,  7.2118e-04, -2.1677e-02,  2.6818e-02,  5.0560e-02,\n",
      "        -1.4425e-02,  8.4564e-03,  3.4376e-03, -5.5125e-03,  7.6474e-03,\n",
      "        -3.9116e-02, -2.8546e-02,  1.7744e-02, -3.0328e-02, -1.2231e-02,\n",
      "         1.4378e-03,  1.3724e-02,  2.0308e-02, -4.8399e-02,  2.0390e-02,\n",
      "        -1.2923e-02, -1.1333e-02,  2.7757e-02,  3.3632e-02,  6.7620e-02,\n",
      "        -1.1302e-02, -3.6600e-02,  2.3489e-02, -2.3861e-02, -4.0567e-02,\n",
      "        -1.9069e-02, -2.5359e-02, -4.1630e-02,  3.0156e-02, -1.8986e-02,\n",
      "         1.0664e-02, -3.4734e-02, -1.1103e-02,  1.8290e-02,  1.5657e-02,\n",
      "        -4.0499e-02,  1.0822e-02, -1.3418e-02, -2.0384e-02,  3.6871e-02,\n",
      "        -9.6542e-03, -2.9336e-02,  3.9981e-02, -9.9299e-03,  1.1755e-02,\n",
      "         1.8593e-02, -1.6314e-02,  3.7931e-02,  2.0290e-02, -1.7872e-02,\n",
      "        -3.4333e-02, -2.9049e-02, -3.2904e-03, -2.5979e-02,  2.0239e-02,\n",
      "         1.5943e-03,  2.8937e-02,  3.8040e-02,  3.6422e-02,  3.8323e-02,\n",
      "        -3.9405e-02, -4.2706e-02,  5.0973e-02, -3.9356e-02,  2.8759e-02,\n",
      "         2.8232e-02, -1.9673e-02,  7.0297e-03, -3.8596e-02, -3.8849e-02,\n",
      "         5.1346e-02, -4.1865e-02,  6.9250e-03, -2.7650e-02, -3.0347e-02,\n",
      "         1.8172e-02,  1.5477e-02,  5.2746e-02,  2.7849e-02, -2.0724e-02,\n",
      "         1.8695e-02,  3.5093e-02,  1.5005e-02,  2.8715e-02, -4.7202e-03,\n",
      "        -3.8569e-02,  8.8188e-03, -1.1842e-02,  2.4387e-02, -7.7057e-03,\n",
      "         3.0523e-02,  6.0297e-02,  4.8035e-03,  2.3994e-02,  3.7991e-02,\n",
      "        -3.8168e-02, -3.5550e-02,  1.5020e-02,  3.0560e-02,  2.2268e-02,\n",
      "        -3.0583e-02,  8.9375e-03, -2.9819e-02, -2.6468e-02, -8.9838e-03,\n",
      "         4.3387e-02, -7.1571e-03,  6.2081e-03, -3.3014e-02,  2.2477e-02,\n",
      "        -4.3030e-02, -2.4145e-02,  3.8630e-03,  6.6563e-03, -4.8912e-03,\n",
      "        -2.2387e-02,  4.6273e-02, -1.0038e-02,  2.9692e-02,  2.0669e-02,\n",
      "        -8.6690e-03,  3.6143e-02,  1.6267e-03,  5.6556e-04,  6.4012e-02,\n",
      "        -1.3351e-02, -1.6570e-02,  1.4989e-02,  3.9805e-02, -1.4299e-02,\n",
      "        -2.0390e-02,  2.0996e-02,  3.4990e-02,  2.3646e-02, -3.3207e-02,\n",
      "         1.1094e-02,  1.9215e-02, -1.9237e-03, -1.9658e-03,  4.1990e-02,\n",
      "        -1.1653e-02, -1.5857e-02,  4.1246e-02,  5.1535e-02, -2.8662e-02,\n",
      "         4.6660e-02, -3.3733e-03, -8.9742e-03,  2.2768e-02,  4.4153e-02,\n",
      "         2.2175e-02,  1.5134e-02, -2.2706e-02,  2.2160e-02,  2.5405e-02,\n",
      "        -2.8442e-02,  2.7418e-02, -1.0027e-02, -2.1351e-02,  2.5567e-02,\n",
      "        -8.0402e-03,  3.0763e-02,  1.7126e-02,  8.4807e-03, -2.9678e-03,\n",
      "         3.8959e-02, -2.7971e-02,  3.2955e-02,  6.7448e-04,  4.0772e-02,\n",
      "         3.2623e-02, -4.2135e-02,  4.1348e-02, -4.3031e-02,  3.9530e-02,\n",
      "         3.8369e-02,  4.3948e-02, -3.4167e-02, -4.1421e-02, -5.8035e-03,\n",
      "         1.9297e-02,  2.8286e-02, -1.9136e-02,  9.0776e-03,  4.5863e-02,\n",
      "         8.0319e-03,  1.7295e-02, -3.0505e-02,  2.4401e-02,  4.6866e-02,\n",
      "         2.9642e-02, -9.0860e-03,  8.3195e-03,  3.8180e-02, -2.5324e-02,\n",
      "         3.0338e-02,  2.3351e-02,  1.6309e-02, -4.4416e-02, -1.9997e-02,\n",
      "         3.8197e-02,  2.5456e-02,  3.1958e-02,  1.8765e-03,  3.4278e-03,\n",
      "        -1.5653e-02,  1.6024e-02, -2.7073e-02, -1.0107e-02, -1.4037e-02,\n",
      "         8.9892e-03,  2.0674e-02, -1.9008e-02, -3.0292e-02, -1.9484e-02,\n",
      "        -2.2011e-02, -2.3906e-03, -2.3464e-02,  3.6241e-02,  9.5316e-03,\n",
      "         2.3328e-02,  2.9730e-02,  2.9578e-03, -2.3735e-02, -1.4557e-02,\n",
      "         1.1157e-02,  4.1269e-02, -6.2547e-03, -4.2742e-02, -1.4737e-02,\n",
      "        -1.8577e-02, -9.7954e-03], device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.6.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0318, -0.0446,  0.0377,  ..., -0.0204,  0.0379,  0.0403],\n",
      "        [-0.0184, -0.0006,  0.0501,  ..., -0.0427,  0.0310,  0.0684],\n",
      "        [-0.0067,  0.0278, -0.0425,  ...,  0.0328,  0.0382, -0.0118],\n",
      "        ...,\n",
      "        [ 0.0414,  0.0268,  0.0280,  ...,  0.0235, -0.0305, -0.0345],\n",
      "        [-0.0050,  0.0375, -0.0408,  ..., -0.0232, -0.0317,  0.0017],\n",
      "        [ 0.0291, -0.0076, -0.0110,  ..., -0.0194, -0.0420, -0.0927]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([10, 512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.6.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([-0.0687,  0.0135, -0.0485,  0.0083, -0.0418,  0.1179,  0.0036,  0.0991,\n",
      "        -0.0287, -0.0035], device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([10])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summarize the model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter value: {param}\")\n",
    "    print(f\"Parameter shape: {param.shape}\")\n",
    "    print(f\"Requires gradient: {param.requires_grad}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('linear_relu_stack.0.weight', tensor([[ 0.0198, -0.0240, -0.0281,  ..., -0.0026,  0.0186,  0.0172],\n",
       "        [-0.0209, -0.0290,  0.0044,  ...,  0.0346, -0.0354,  0.0079],\n",
       "        [-0.0127, -0.0175, -0.0039,  ..., -0.0025, -0.0199,  0.0257],\n",
       "        ...,\n",
       "        [-0.0224,  0.0343, -0.0352,  ...,  0.0286, -0.0338,  0.0129],\n",
       "        [ 0.0190,  0.0294,  0.0186,  ..., -0.0056, -0.0158,  0.0142],\n",
       "        [-0.0340, -0.0249, -0.0080,  ...,  0.0003, -0.0034, -0.0322]],\n",
       "       device='mps:0')), ('linear_relu_stack.0.bias', tensor([-2.8885e-02,  3.0724e-02,  2.9379e-02,  1.8816e-02, -2.6011e-02,\n",
       "         3.3328e-03, -8.3296e-03,  2.3787e-02,  1.0857e-02, -1.3107e-02,\n",
       "        -1.2003e-02, -2.3429e-02,  2.5075e-02, -2.0783e-02,  3.2708e-02,\n",
       "        -3.0691e-03, -2.0024e-02, -3.4753e-02,  3.2016e-02, -1.2303e-02,\n",
       "         3.3830e-02,  2.0743e-02,  1.3980e-02,  1.5055e-02, -2.6451e-02,\n",
       "        -1.3471e-02, -6.7056e-03, -3.0503e-02,  3.6014e-02,  3.1312e-02,\n",
       "         5.1512e-03,  1.1884e-02,  3.5617e-02, -2.7165e-02, -2.5750e-03,\n",
       "        -4.1776e-04,  4.0322e-02,  2.3939e-02, -1.2212e-02, -1.6722e-02,\n",
       "         3.3738e-02, -2.0785e-02, -2.2053e-02,  3.7438e-02,  2.5169e-02,\n",
       "         1.6158e-02, -2.7755e-02,  1.1634e-02,  5.5876e-03,  3.6070e-02,\n",
       "         1.4774e-02, -3.1028e-02,  3.3399e-02, -2.2150e-02,  3.4895e-02,\n",
       "         2.8568e-03,  3.2175e-02, -1.1923e-02, -1.9742e-02,  1.8329e-02,\n",
       "        -1.4004e-02, -1.7513e-03,  2.2808e-02, -1.2054e-02, -2.7491e-02,\n",
       "        -2.4403e-02, -3.0005e-02,  2.1457e-02, -2.7426e-02,  1.7093e-03,\n",
       "        -6.1711e-03,  1.9490e-02, -2.9483e-02, -1.5260e-02,  9.6598e-03,\n",
       "         3.5024e-02,  2.8639e-02,  8.2873e-03,  6.6051e-03,  2.4135e-02,\n",
       "        -1.7530e-02,  3.6306e-02,  2.8074e-02,  9.3366e-03,  1.1191e-02,\n",
       "        -6.3301e-03,  1.1603e-02,  3.9490e-03, -1.2593e-02, -2.5030e-02,\n",
       "        -1.0173e-02, -2.1718e-02,  3.4006e-02,  4.6784e-03,  4.6822e-03,\n",
       "        -2.0531e-02, -2.0127e-02,  2.7868e-02, -1.6329e-03,  2.4791e-02,\n",
       "        -1.7827e-02, -1.8382e-02, -3.3869e-02, -2.4621e-02,  1.1351e-02,\n",
       "         1.5103e-02, -1.6036e-02, -1.6023e-02,  2.5024e-02, -7.4925e-04,\n",
       "        -5.4683e-03, -1.0623e-03,  4.1791e-02, -5.8408e-03,  2.0713e-02,\n",
       "         6.8200e-03,  2.1469e-02,  1.4076e-02, -1.3525e-02, -1.6181e-02,\n",
       "         1.3109e-02, -1.7925e-02, -1.5160e-02, -2.3532e-02, -1.1491e-02,\n",
       "        -1.1569e-02,  3.1583e-02, -2.3851e-02, -2.4035e-02, -2.8872e-02,\n",
       "         6.3931e-03,  1.0111e-02,  2.1893e-02, -1.6499e-02, -1.9316e-02,\n",
       "         2.5782e-02, -2.2701e-02,  1.0442e-02,  1.0493e-02, -2.5850e-02,\n",
       "        -2.6655e-02, -2.6080e-02,  7.1040e-03,  1.4631e-02, -2.9316e-03,\n",
       "        -1.0732e-02,  3.4686e-02,  6.5262e-03, -3.2515e-02,  7.0410e-03,\n",
       "        -3.1962e-03,  2.2169e-02,  1.8243e-02,  2.3066e-02,  8.5594e-03,\n",
       "        -1.0576e-02, -6.9825e-03, -2.8902e-02, -7.8992e-03, -3.4704e-02,\n",
       "         1.2152e-02,  1.1768e-02, -2.7994e-02,  3.1563e-03, -1.4162e-02,\n",
       "         2.1577e-03,  2.7977e-02,  2.3065e-02,  2.0058e-03, -1.4845e-02,\n",
       "         1.8814e-02, -3.0981e-02,  4.1021e-03, -1.5057e-02, -3.2262e-03,\n",
       "        -2.8094e-02, -6.2735e-03, -1.2439e-02,  1.6888e-02,  1.4186e-02,\n",
       "         2.5471e-03, -4.9098e-03, -2.4063e-02,  6.8739e-03,  5.5835e-03,\n",
       "        -2.0138e-03, -3.0230e-02,  8.9935e-03, -3.1582e-03, -2.6402e-03,\n",
       "         6.9623e-03,  1.4445e-02,  1.2577e-02, -3.0398e-02, -4.1235e-03,\n",
       "         3.4546e-02, -1.7241e-02, -1.5494e-02,  3.6180e-02, -4.2242e-03,\n",
       "         7.8436e-03,  2.5121e-02,  3.5520e-02,  5.6604e-03, -1.3966e-02,\n",
       "         2.3481e-02,  3.5543e-02, -1.6268e-02,  2.7687e-02,  1.3167e-02,\n",
       "        -2.5286e-02,  2.5845e-02, -1.4561e-02,  2.1928e-02, -7.3988e-03,\n",
       "         1.5272e-02,  4.3983e-03, -9.9506e-03, -1.9154e-02, -1.4455e-02,\n",
       "         3.3836e-02, -9.5681e-03, -1.2928e-02, -1.3505e-02,  3.5628e-02,\n",
       "         3.3122e-02,  3.4281e-02,  3.2445e-03,  1.0230e-02,  3.1368e-02,\n",
       "         1.0250e-02,  3.5164e-02, -8.1767e-03, -8.6655e-03,  5.5308e-04,\n",
       "        -6.4203e-05, -3.2897e-02, -3.4336e-04,  1.6520e-02,  1.3061e-02,\n",
       "         1.1053e-02, -1.1236e-02,  2.2638e-02, -2.9824e-02,  2.9866e-02,\n",
       "         1.3034e-02,  6.1655e-03,  3.6948e-02, -4.8140e-03, -3.1251e-02,\n",
       "         3.0594e-02,  7.0358e-03, -1.6830e-02, -4.2707e-03,  1.9405e-02,\n",
       "        -2.0211e-02, -3.2937e-02,  2.9705e-02, -4.6424e-03, -1.1979e-03,\n",
       "        -4.9173e-03,  2.3950e-02, -1.6355e-03,  1.8959e-02, -1.5052e-02,\n",
       "        -1.7156e-02,  2.6852e-02,  1.1895e-02, -1.6650e-02,  2.5973e-02,\n",
       "         2.5724e-02, -1.9650e-02, -2.6373e-02,  3.0847e-02, -9.9067e-03,\n",
       "         3.0696e-03,  1.8842e-02,  1.4721e-03,  2.4017e-02,  4.5269e-03,\n",
       "         3.0340e-02,  5.4461e-03, -9.0629e-03, -2.9003e-02, -2.8581e-02,\n",
       "         2.9850e-03,  9.4316e-03, -2.5414e-02, -5.2373e-03,  1.4917e-02,\n",
       "        -1.3218e-02, -4.2471e-03,  1.4559e-03, -2.0003e-02,  9.7698e-03,\n",
       "         2.8398e-02, -2.2471e-02,  3.8406e-03,  4.4285e-03,  1.2415e-03,\n",
       "         2.7740e-02,  2.7810e-02,  3.1017e-02,  2.6432e-03,  1.8180e-02,\n",
       "         7.0939e-03,  3.0187e-02,  2.6374e-02, -8.2700e-03, -5.5931e-03,\n",
       "        -1.0230e-03,  1.2614e-03, -7.2946e-03,  3.5336e-02, -3.6630e-03,\n",
       "         3.7084e-02,  3.0300e-02, -2.8261e-02, -1.4402e-02,  3.8279e-02,\n",
       "        -4.2088e-03, -3.4139e-04,  3.2320e-02, -3.4863e-02, -2.7873e-02,\n",
       "        -8.1019e-03,  3.0302e-02, -4.0284e-03,  2.9326e-02,  4.7302e-03,\n",
       "         2.1705e-02,  2.4218e-02, -1.8512e-02, -1.1455e-02,  1.0339e-02,\n",
       "        -1.2048e-03, -7.4692e-03,  2.9824e-03,  2.2866e-02,  2.2639e-02,\n",
       "         7.9727e-03, -3.1644e-02,  1.3154e-02,  1.8240e-02,  2.3594e-02,\n",
       "         2.1785e-02,  7.6093e-03,  1.1939e-02,  1.6937e-02,  9.6077e-03,\n",
       "         3.0290e-02, -2.0593e-02,  7.1623e-03,  1.1760e-02,  3.6298e-02,\n",
       "        -1.2266e-02,  1.6029e-02, -2.7689e-02,  1.3499e-02,  1.9047e-02,\n",
       "         3.3577e-02, -8.9625e-04,  1.4711e-02,  3.4088e-02, -9.8882e-03,\n",
       "        -2.0196e-02, -3.7565e-03,  3.2150e-03, -9.8563e-04, -1.1741e-02,\n",
       "        -2.3580e-03, -1.7564e-02, -1.9057e-02,  1.5487e-03, -1.3509e-02,\n",
       "         2.6124e-04,  6.3576e-03,  3.9402e-02, -2.0310e-02,  1.6905e-02,\n",
       "        -1.0779e-02, -5.9845e-03,  2.6781e-02,  2.9004e-02, -2.5973e-02,\n",
       "        -7.0098e-03,  9.4057e-03, -3.0828e-02, -1.0302e-02,  3.2541e-02,\n",
       "         5.2583e-03, -2.3972e-02,  5.2427e-03,  3.3820e-02, -7.5161e-03,\n",
       "        -3.0183e-02,  1.3372e-02, -1.0813e-02, -2.7164e-02, -1.0489e-02,\n",
       "         2.6978e-02,  4.1580e-02,  1.9987e-03, -8.5008e-03,  6.9476e-03,\n",
       "         2.8399e-02,  1.7234e-02, -1.8533e-02, -1.8766e-02,  3.7408e-02,\n",
       "         8.3304e-03, -1.5013e-03, -3.1446e-02,  4.8599e-03, -3.2452e-02,\n",
       "        -2.7679e-02,  6.7277e-04,  1.0489e-02,  2.5651e-02,  1.7761e-02,\n",
       "        -7.8822e-03,  6.4094e-03,  2.5847e-02,  1.6605e-02,  1.8394e-02,\n",
       "         5.2180e-03, -2.0659e-02, -2.1515e-02, -3.2425e-02, -1.8166e-02,\n",
       "        -2.1502e-02,  5.8326e-03, -2.9944e-02, -2.7053e-02,  8.5739e-03,\n",
       "        -1.0595e-03,  2.9983e-03, -1.2467e-02, -1.8524e-02,  2.7569e-02,\n",
       "        -5.8544e-03,  2.5351e-02, -2.0476e-02,  2.4097e-02, -3.0785e-02,\n",
       "        -2.4125e-02,  8.4826e-03,  3.6627e-02,  1.4092e-02, -3.1241e-02,\n",
       "         1.4641e-02, -9.2336e-03,  3.2551e-02,  3.6745e-03,  1.2485e-02,\n",
       "         1.0133e-02, -2.5361e-02, -3.8566e-03,  2.2444e-02, -8.8475e-03,\n",
       "        -2.3687e-02, -6.7430e-03, -1.4400e-02, -5.0400e-03, -2.9173e-02,\n",
       "        -1.3624e-02,  4.3945e-03,  1.6449e-02,  8.2914e-03,  9.6643e-03,\n",
       "        -1.8844e-02, -2.3437e-02, -6.4097e-03,  3.6470e-02,  6.2956e-03,\n",
       "        -2.1506e-02, -1.6173e-02,  2.0377e-02,  1.1097e-02, -1.9339e-02,\n",
       "         2.5463e-02,  3.0768e-02, -2.5754e-02, -3.7889e-03,  8.0438e-03,\n",
       "        -3.3928e-02,  4.3208e-03, -9.5974e-03,  1.7085e-02,  4.1546e-03,\n",
       "        -2.2891e-02, -1.3194e-02,  1.4833e-04, -1.5695e-03, -2.1336e-02,\n",
       "        -1.9836e-02,  1.0900e-02,  2.1443e-02,  2.2002e-02,  2.6338e-02,\n",
       "        -1.1083e-02, -2.0195e-02,  2.5126e-02,  3.4052e-02, -1.6206e-02,\n",
       "        -2.2948e-02,  2.6781e-02, -1.6364e-02,  2.6838e-02, -3.3768e-02,\n",
       "        -7.8922e-03, -1.7609e-02], device='mps:0')), ('linear_relu_stack.2.weight', tensor([[ 0.0243, -0.0260,  0.0411,  ...,  0.0092, -0.0310, -0.0160],\n",
       "        [-0.0020, -0.0180,  0.0329,  ..., -0.0138,  0.0131, -0.0147],\n",
       "        [ 0.0244, -0.0315,  0.0422,  ...,  0.0165,  0.0201,  0.0302],\n",
       "        ...,\n",
       "        [-0.0096, -0.0282, -0.0193,  ..., -0.0136,  0.0162, -0.0244],\n",
       "        [ 0.0410, -0.0399,  0.0150,  ..., -0.0260,  0.0107, -0.0170],\n",
       "        [-0.0079, -0.0319, -0.0365,  ...,  0.0231,  0.0038, -0.0056]],\n",
       "       device='mps:0')), ('linear_relu_stack.2.bias', tensor([ 0.0229, -0.0127, -0.0110, -0.0433, -0.0371, -0.0429,  0.0219,  0.0418,\n",
       "        -0.0069,  0.0366, -0.0199,  0.0164,  0.0279, -0.0311, -0.0042, -0.0179,\n",
       "         0.0192, -0.0104, -0.0349, -0.0180, -0.0337, -0.0136,  0.0202,  0.0451,\n",
       "        -0.0183,  0.0165,  0.0378, -0.0404, -0.0102, -0.0192, -0.0204,  0.0354,\n",
       "         0.0062, -0.0243,  0.0288,  0.0246, -0.0108,  0.0261, -0.0340, -0.0313,\n",
       "        -0.0158,  0.0084, -0.0121,  0.0295,  0.0070,  0.0430, -0.0136, -0.0180,\n",
       "         0.0292, -0.0409,  0.0218, -0.0064,  0.0024, -0.0006,  0.0112,  0.0418,\n",
       "        -0.0113,  0.0255, -0.0260, -0.0004, -0.0126,  0.0247,  0.0133,  0.0104,\n",
       "         0.0172,  0.0247,  0.0024,  0.0153,  0.0012,  0.0029, -0.0143,  0.0069,\n",
       "         0.0156, -0.0367, -0.0438, -0.0011,  0.0237,  0.0225,  0.0313, -0.0072,\n",
       "         0.0476,  0.0137,  0.0344, -0.0019,  0.0313,  0.0312, -0.0257, -0.0208,\n",
       "         0.0003,  0.0103,  0.0011,  0.0330, -0.0378,  0.0265, -0.0117,  0.0312,\n",
       "         0.0049, -0.0176, -0.0103,  0.0294, -0.0348, -0.0154, -0.0237, -0.0225,\n",
       "        -0.0231, -0.0113, -0.0233, -0.0195, -0.0112,  0.0380,  0.0342, -0.0050,\n",
       "         0.0246,  0.0057,  0.0474,  0.0276,  0.0430, -0.0050, -0.0197, -0.0322,\n",
       "        -0.0330, -0.0121, -0.0342,  0.0422, -0.0329, -0.0245,  0.0375, -0.0112,\n",
       "         0.0461,  0.0214,  0.0164,  0.0371,  0.0156,  0.0040,  0.0093,  0.0130,\n",
       "         0.0458, -0.0133, -0.0146,  0.0306,  0.0391, -0.0051, -0.0034, -0.0025,\n",
       "        -0.0033,  0.0216,  0.0354,  0.0355,  0.0356, -0.0164, -0.0033, -0.0283,\n",
       "        -0.0300, -0.0018, -0.0063, -0.0115,  0.0244,  0.0458, -0.0248, -0.0080,\n",
       "        -0.0320,  0.0462, -0.0409,  0.0096,  0.0167,  0.0008, -0.0306,  0.0400,\n",
       "        -0.0249,  0.0021,  0.0068, -0.0065,  0.0176,  0.0122, -0.0247,  0.0410,\n",
       "         0.0311,  0.0050,  0.0359, -0.0142,  0.0361,  0.0181, -0.0174,  0.0403,\n",
       "         0.0203,  0.0110,  0.0361,  0.0193,  0.0149,  0.0262, -0.0291,  0.0208,\n",
       "        -0.0350,  0.0307,  0.0305, -0.0239, -0.0414,  0.0014, -0.0318,  0.0043,\n",
       "         0.0427,  0.0146, -0.0005, -0.0388, -0.0314, -0.0002,  0.0157,  0.0316,\n",
       "         0.0039,  0.0304,  0.0315, -0.0054, -0.0129,  0.0441, -0.0159, -0.0020,\n",
       "         0.0183, -0.0037, -0.0322, -0.0148, -0.0162, -0.0176,  0.0056, -0.0001,\n",
       "        -0.0192, -0.0375, -0.0131, -0.0035, -0.0095,  0.0030,  0.0348, -0.0073,\n",
       "         0.0024,  0.0321, -0.0075, -0.0418,  0.0344,  0.0196, -0.0185, -0.0412,\n",
       "         0.0014,  0.0373, -0.0444,  0.0070, -0.0043, -0.0450,  0.0399,  0.0140,\n",
       "        -0.0375,  0.0079,  0.0311,  0.0283, -0.0026,  0.0156, -0.0438,  0.0249,\n",
       "         0.0246,  0.0142,  0.0241,  0.0374, -0.0039,  0.0347, -0.0442,  0.0301,\n",
       "         0.0035,  0.0415, -0.0143, -0.0115, -0.0207, -0.0128, -0.0322,  0.0174,\n",
       "         0.0207,  0.0273, -0.0009, -0.0330, -0.0173,  0.0229,  0.0358,  0.0357,\n",
       "         0.0060,  0.0437, -0.0033, -0.0124, -0.0334,  0.0002, -0.0350,  0.0084,\n",
       "         0.0045,  0.0148, -0.0370, -0.0022,  0.0116, -0.0365,  0.0063, -0.0118,\n",
       "         0.0408, -0.0172, -0.0054,  0.0120,  0.0001, -0.0400, -0.0057, -0.0173,\n",
       "        -0.0427,  0.0275,  0.0052, -0.0166,  0.0035, -0.0079, -0.0152, -0.0129,\n",
       "        -0.0135, -0.0163, -0.0061,  0.0304, -0.0159, -0.0249, -0.0122, -0.0179,\n",
       "         0.0106,  0.0111,  0.0181,  0.0007,  0.0157,  0.0084,  0.0200, -0.0347,\n",
       "        -0.0191,  0.0193, -0.0264, -0.0189, -0.0056, -0.0145, -0.0309, -0.0075,\n",
       "        -0.0119,  0.0063, -0.0434, -0.0003,  0.0215, -0.0133,  0.0353,  0.0124,\n",
       "         0.0381, -0.0232,  0.0189, -0.0025,  0.0326, -0.0198, -0.0417,  0.0101,\n",
       "         0.0169, -0.0150, -0.0345, -0.0086, -0.0377, -0.0331,  0.0373,  0.0046,\n",
       "        -0.0210, -0.0237,  0.0022,  0.0264, -0.0144,  0.0440,  0.0318,  0.0212,\n",
       "         0.0338,  0.0213,  0.0012, -0.0363, -0.0137, -0.0124, -0.0070,  0.0273,\n",
       "        -0.0293,  0.0189,  0.0029, -0.0189, -0.0303,  0.0284, -0.0288, -0.0278,\n",
       "        -0.0024, -0.0067,  0.0095, -0.0433,  0.0396, -0.0447,  0.0454, -0.0150,\n",
       "         0.0137,  0.0222, -0.0419, -0.0298, -0.0181, -0.0255,  0.0301,  0.0091,\n",
       "         0.0049,  0.0107,  0.0071, -0.0440,  0.0259, -0.0430, -0.0051,  0.0441,\n",
       "         0.0449,  0.0130,  0.0374,  0.0330, -0.0216, -0.0309,  0.0147, -0.0390,\n",
       "        -0.0283, -0.0111,  0.0133, -0.0416,  0.0042, -0.0125, -0.0225,  0.0006,\n",
       "         0.0133, -0.0212, -0.0001,  0.0339,  0.0448, -0.0243, -0.0127,  0.0321,\n",
       "         0.0429, -0.0198,  0.0096, -0.0344, -0.0333, -0.0437, -0.0188,  0.0020,\n",
       "         0.0378, -0.0329, -0.0290, -0.0348,  0.0417, -0.0395, -0.0432,  0.0386,\n",
       "        -0.0055,  0.0476, -0.0362,  0.0272,  0.0366, -0.0187,  0.0309,  0.0528,\n",
       "        -0.0199, -0.0188, -0.0408, -0.0384,  0.0309, -0.0352, -0.0246, -0.0389,\n",
       "        -0.0030, -0.0434,  0.0206, -0.0371, -0.0405, -0.0218, -0.0190, -0.0308,\n",
       "         0.0206, -0.0348, -0.0057,  0.0233, -0.0277,  0.0061, -0.0213, -0.0046,\n",
       "         0.0165, -0.0080,  0.0299,  0.0172,  0.0451, -0.0050,  0.0302,  0.0428,\n",
       "        -0.0021,  0.0365,  0.0245, -0.0442, -0.0359, -0.0078,  0.0214, -0.0008,\n",
       "         0.0436, -0.0366,  0.0389,  0.0187, -0.0203,  0.0246,  0.0153,  0.0082,\n",
       "        -0.0039, -0.0364,  0.0096, -0.0174,  0.0395, -0.0437, -0.0338, -0.0081],\n",
       "       device='mps:0')), ('linear_relu_stack.4.weight', tensor([[ 0.0111, -0.0099,  0.0434,  ..., -0.0018, -0.0055, -0.0068],\n",
       "        [ 0.0382, -0.0060,  0.0218,  ..., -0.0421, -0.0082, -0.0253],\n",
       "        [ 0.0442, -0.0180, -0.0276,  ...,  0.0168, -0.0266, -0.0434],\n",
       "        ...,\n",
       "        [-0.0370, -0.0414,  0.0311,  ..., -0.0035,  0.0178,  0.0112],\n",
       "        [-0.0197,  0.0046, -0.0029,  ..., -0.0168,  0.0298,  0.0347],\n",
       "        [ 0.0007, -0.0411, -0.0018,  ..., -0.0355, -0.0071,  0.0419]],\n",
       "       device='mps:0')), ('linear_relu_stack.4.bias', tensor([ 1.7318e-02, -7.0786e-03,  5.1649e-02, -4.2493e-02, -3.2445e-02,\n",
       "        -1.8458e-02, -2.1479e-02,  3.8223e-02,  2.1537e-02,  2.0985e-02,\n",
       "         2.0438e-02, -1.2273e-02, -1.4187e-02,  4.1451e-02, -3.9474e-02,\n",
       "        -1.8199e-02,  1.9097e-02,  3.5826e-02, -4.3577e-03,  1.9929e-02,\n",
       "         2.4040e-03, -4.2782e-02, -9.7647e-03, -3.8371e-02,  3.8025e-02,\n",
       "        -2.2879e-02,  2.1958e-02, -3.4801e-02, -2.8919e-02,  9.4573e-03,\n",
       "        -1.4233e-02, -1.4332e-02,  1.6035e-03,  2.7010e-02,  2.5684e-02,\n",
       "        -2.2337e-03, -1.3826e-02, -1.7289e-02,  2.9542e-02,  3.6123e-02,\n",
       "        -3.6980e-02,  3.8812e-02,  7.9216e-03,  1.3671e-02,  2.6618e-02,\n",
       "        -4.8472e-04, -4.3931e-03, -1.1935e-02,  5.8692e-02,  3.2235e-02,\n",
       "         1.5881e-02, -1.2363e-02, -2.8166e-02, -1.9381e-02,  3.2088e-03,\n",
       "        -3.0191e-02,  4.3764e-02,  2.0722e-02,  4.9771e-03, -2.0293e-02,\n",
       "        -3.2640e-02, -1.5983e-02, -3.1433e-03,  2.8204e-02, -4.5101e-02,\n",
       "        -5.7470e-04,  3.4318e-02, -5.3816e-03,  1.4172e-03,  3.9414e-02,\n",
       "        -7.5540e-03,  2.8211e-02, -2.4182e-02, -4.1158e-03,  3.7236e-02,\n",
       "        -1.3803e-02, -1.2562e-02,  1.1383e-02,  3.6048e-02, -3.6543e-02,\n",
       "         3.5460e-03, -4.1006e-02,  1.9022e-02,  1.2054e-02, -3.3377e-02,\n",
       "         4.0055e-02,  5.7994e-02, -3.7058e-02,  4.2803e-02,  1.9083e-02,\n",
       "         6.9325e-03, -3.2383e-02, -1.4928e-02,  3.7061e-02, -1.8199e-02,\n",
       "         3.3508e-02, -2.8972e-02, -2.2529e-02, -9.1809e-03, -4.0686e-02,\n",
       "         2.9859e-02, -3.0697e-02,  1.6391e-03, -1.3039e-02, -6.2488e-04,\n",
       "        -1.7751e-02, -9.3508e-03, -1.1345e-02, -7.5826e-03,  3.0295e-02,\n",
       "         4.8416e-04,  2.2127e-02, -1.1442e-03,  9.9213e-03,  4.7730e-03,\n",
       "         1.6236e-02,  8.3317e-03,  5.1639e-02, -2.1522e-02,  2.9450e-03,\n",
       "        -3.2094e-02,  5.0561e-02,  2.2980e-02,  1.2752e-02,  2.8797e-02,\n",
       "        -8.3027e-03,  2.8510e-02, -1.3843e-02, -2.0121e-02,  4.5677e-02,\n",
       "         4.4034e-02,  2.9158e-02, -5.6465e-03, -3.7396e-02,  1.6665e-02,\n",
       "         3.8218e-02, -1.8409e-02, -1.7208e-02, -1.6851e-03, -4.1556e-02,\n",
       "         1.2530e-02,  6.5586e-03, -1.8510e-02,  3.8474e-02,  4.5585e-02,\n",
       "        -2.4897e-02, -2.4011e-02, -2.2820e-02, -2.0322e-02, -4.5782e-02,\n",
       "         4.0973e-02, -1.5832e-02,  4.9533e-02,  3.7102e-02,  4.1319e-02,\n",
       "         1.9629e-02, -1.2717e-02,  1.4209e-02, -4.2959e-02,  3.6806e-02,\n",
       "         3.2707e-02, -9.2717e-03, -3.2657e-02,  1.6861e-02,  4.5318e-02,\n",
       "         4.6716e-02,  3.5540e-02, -3.6700e-02,  1.3706e-02,  1.6120e-03,\n",
       "         2.1601e-02,  3.9209e-02, -3.0604e-02,  3.8311e-02, -7.1445e-03,\n",
       "         2.9508e-02,  5.2320e-02, -3.8854e-03,  1.1641e-03,  1.1876e-02,\n",
       "         4.0344e-02,  1.7832e-02, -4.0858e-02,  2.8024e-02,  2.5785e-02,\n",
       "        -7.5955e-03,  2.6049e-02, -1.6633e-02, -3.1560e-02, -2.3155e-02,\n",
       "        -1.5899e-02, -2.7093e-02, -3.2596e-02,  5.0151e-02,  3.4519e-02,\n",
       "         1.2227e-02,  2.9517e-05, -3.6431e-02,  1.9964e-03,  4.2379e-02,\n",
       "        -2.2897e-02, -4.0681e-02, -1.1113e-02, -2.3959e-02,  1.4136e-03,\n",
       "         1.1196e-02,  1.7967e-04, -3.8054e-02, -3.7807e-02, -1.5872e-02,\n",
       "         3.0692e-02,  2.7421e-02, -2.8844e-02,  2.1159e-02,  3.9894e-02,\n",
       "        -2.2613e-02, -2.0598e-02, -8.7994e-03,  3.5445e-03,  2.0905e-02,\n",
       "         5.0495e-02,  1.3552e-02, -3.5824e-02,  2.7904e-02, -7.2040e-03,\n",
       "        -3.4532e-03,  2.2455e-02,  1.4797e-02,  5.5636e-03, -2.9176e-02,\n",
       "         1.3649e-02,  3.1527e-02, -2.4141e-02,  3.8623e-02,  1.1454e-02,\n",
       "        -3.3364e-02,  6.1380e-03,  2.3510e-03, -2.2466e-02, -3.4898e-02,\n",
       "        -1.6551e-02, -3.4108e-02,  4.0410e-02,  2.8591e-02,  1.4965e-02,\n",
       "        -4.0670e-02,  4.9752e-02, -2.3617e-02,  3.5341e-02, -2.3345e-02,\n",
       "        -1.3766e-02,  1.8356e-02, -3.8041e-02, -8.8572e-03, -4.1481e-02,\n",
       "        -3.5497e-02, -4.3750e-03, -4.5579e-02,  2.0809e-02,  3.7414e-02,\n",
       "         9.8663e-03, -3.6341e-03, -1.1312e-02, -2.2951e-02,  1.9252e-02,\n",
       "         7.5982e-03,  3.3520e-02, -3.8377e-02, -1.6939e-02, -1.1220e-02,\n",
       "        -3.2882e-02, -2.9923e-02,  2.7348e-02,  2.0158e-02, -4.2816e-02,\n",
       "         1.2838e-03,  7.2118e-04, -2.1677e-02,  2.6818e-02,  5.0560e-02,\n",
       "        -1.4425e-02,  8.4564e-03,  3.4376e-03, -5.5125e-03,  7.6474e-03,\n",
       "        -3.9116e-02, -2.8546e-02,  1.7744e-02, -3.0328e-02, -1.2231e-02,\n",
       "         1.4378e-03,  1.3724e-02,  2.0308e-02, -4.8399e-02,  2.0390e-02,\n",
       "        -1.2923e-02, -1.1333e-02,  2.7757e-02,  3.3632e-02,  6.7620e-02,\n",
       "        -1.1302e-02, -3.6600e-02,  2.3489e-02, -2.3861e-02, -4.0567e-02,\n",
       "        -1.9069e-02, -2.5359e-02, -4.1630e-02,  3.0156e-02, -1.8986e-02,\n",
       "         1.0664e-02, -3.4734e-02, -1.1103e-02,  1.8290e-02,  1.5657e-02,\n",
       "        -4.0499e-02,  1.0822e-02, -1.3418e-02, -2.0384e-02,  3.6871e-02,\n",
       "        -9.6542e-03, -2.9336e-02,  3.9981e-02, -9.9299e-03,  1.1755e-02,\n",
       "         1.8593e-02, -1.6314e-02,  3.7931e-02,  2.0290e-02, -1.7872e-02,\n",
       "        -3.4333e-02, -2.9049e-02, -3.2904e-03, -2.5979e-02,  2.0239e-02,\n",
       "         1.5943e-03,  2.8937e-02,  3.8040e-02,  3.6422e-02,  3.8323e-02,\n",
       "        -3.9405e-02, -4.2706e-02,  5.0973e-02, -3.9356e-02,  2.8759e-02,\n",
       "         2.8232e-02, -1.9673e-02,  7.0297e-03, -3.8596e-02, -3.8849e-02,\n",
       "         5.1346e-02, -4.1865e-02,  6.9250e-03, -2.7650e-02, -3.0347e-02,\n",
       "         1.8172e-02,  1.5477e-02,  5.2746e-02,  2.7849e-02, -2.0724e-02,\n",
       "         1.8695e-02,  3.5093e-02,  1.5005e-02,  2.8715e-02, -4.7202e-03,\n",
       "        -3.8569e-02,  8.8188e-03, -1.1842e-02,  2.4387e-02, -7.7057e-03,\n",
       "         3.0523e-02,  6.0297e-02,  4.8035e-03,  2.3994e-02,  3.7991e-02,\n",
       "        -3.8168e-02, -3.5550e-02,  1.5020e-02,  3.0560e-02,  2.2268e-02,\n",
       "        -3.0583e-02,  8.9375e-03, -2.9819e-02, -2.6468e-02, -8.9838e-03,\n",
       "         4.3387e-02, -7.1571e-03,  6.2081e-03, -3.3014e-02,  2.2477e-02,\n",
       "        -4.3030e-02, -2.4145e-02,  3.8630e-03,  6.6563e-03, -4.8912e-03,\n",
       "        -2.2387e-02,  4.6273e-02, -1.0038e-02,  2.9692e-02,  2.0669e-02,\n",
       "        -8.6690e-03,  3.6143e-02,  1.6267e-03,  5.6556e-04,  6.4012e-02,\n",
       "        -1.3351e-02, -1.6570e-02,  1.4989e-02,  3.9805e-02, -1.4299e-02,\n",
       "        -2.0390e-02,  2.0996e-02,  3.4990e-02,  2.3646e-02, -3.3207e-02,\n",
       "         1.1094e-02,  1.9215e-02, -1.9237e-03, -1.9658e-03,  4.1990e-02,\n",
       "        -1.1653e-02, -1.5857e-02,  4.1246e-02,  5.1535e-02, -2.8662e-02,\n",
       "         4.6660e-02, -3.3733e-03, -8.9742e-03,  2.2768e-02,  4.4153e-02,\n",
       "         2.2175e-02,  1.5134e-02, -2.2706e-02,  2.2160e-02,  2.5405e-02,\n",
       "        -2.8442e-02,  2.7418e-02, -1.0027e-02, -2.1351e-02,  2.5567e-02,\n",
       "        -8.0402e-03,  3.0763e-02,  1.7126e-02,  8.4807e-03, -2.9678e-03,\n",
       "         3.8959e-02, -2.7971e-02,  3.2955e-02,  6.7448e-04,  4.0772e-02,\n",
       "         3.2623e-02, -4.2135e-02,  4.1348e-02, -4.3031e-02,  3.9530e-02,\n",
       "         3.8369e-02,  4.3948e-02, -3.4167e-02, -4.1421e-02, -5.8035e-03,\n",
       "         1.9297e-02,  2.8286e-02, -1.9136e-02,  9.0776e-03,  4.5863e-02,\n",
       "         8.0319e-03,  1.7295e-02, -3.0505e-02,  2.4401e-02,  4.6866e-02,\n",
       "         2.9642e-02, -9.0860e-03,  8.3195e-03,  3.8180e-02, -2.5324e-02,\n",
       "         3.0338e-02,  2.3351e-02,  1.6309e-02, -4.4416e-02, -1.9997e-02,\n",
       "         3.8197e-02,  2.5456e-02,  3.1958e-02,  1.8765e-03,  3.4278e-03,\n",
       "        -1.5653e-02,  1.6024e-02, -2.7073e-02, -1.0107e-02, -1.4037e-02,\n",
       "         8.9892e-03,  2.0674e-02, -1.9008e-02, -3.0292e-02, -1.9484e-02,\n",
       "        -2.2011e-02, -2.3906e-03, -2.3464e-02,  3.6241e-02,  9.5316e-03,\n",
       "         2.3328e-02,  2.9730e-02,  2.9578e-03, -2.3735e-02, -1.4557e-02,\n",
       "         1.1157e-02,  4.1269e-02, -6.2547e-03, -4.2742e-02, -1.4737e-02,\n",
       "        -1.8577e-02, -9.7954e-03], device='mps:0')), ('linear_relu_stack.6.weight', tensor([[ 0.0318, -0.0446,  0.0377,  ..., -0.0204,  0.0379,  0.0403],\n",
       "        [-0.0184, -0.0006,  0.0501,  ..., -0.0427,  0.0310,  0.0684],\n",
       "        [-0.0067,  0.0278, -0.0425,  ...,  0.0328,  0.0382, -0.0118],\n",
       "        ...,\n",
       "        [ 0.0414,  0.0268,  0.0280,  ...,  0.0235, -0.0305, -0.0345],\n",
       "        [-0.0050,  0.0375, -0.0408,  ..., -0.0232, -0.0317,  0.0017],\n",
       "        [ 0.0291, -0.0076, -0.0110,  ..., -0.0194, -0.0420, -0.0927]],\n",
       "       device='mps:0')), ('linear_relu_stack.6.bias', tensor([-0.0687,  0.0135, -0.0485,  0.0083, -0.0418,  0.1179,  0.0036,  0.0991,\n",
       "        -0.0287, -0.0035], device='mps:0'))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 932362\n",
      "Trainable parameters: 932362\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
