{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NN().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302351  [   64/60000]\n",
      "loss: 2.288742  [ 6464/60000]\n",
      "loss: 2.268091  [12864/60000]\n",
      "loss: 2.265815  [19264/60000]\n",
      "loss: 2.253532  [25664/60000]\n",
      "loss: 2.216333  [32064/60000]\n",
      "loss: 2.228212  [38464/60000]\n",
      "loss: 2.193891  [44864/60000]\n",
      "loss: 2.181459  [51264/60000]\n",
      "loss: 2.160450  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 2.152003 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.158452  [   64/60000]\n",
      "loss: 2.150958  [ 6464/60000]\n",
      "loss: 2.092839  [12864/60000]\n",
      "loss: 2.110216  [19264/60000]\n",
      "loss: 2.066475  [25664/60000]\n",
      "loss: 1.995857  [32064/60000]\n",
      "loss: 2.028120  [38464/60000]\n",
      "loss: 1.948215  [44864/60000]\n",
      "loss: 1.941154  [51264/60000]\n",
      "loss: 1.882324  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.877513 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.906847  [   64/60000]\n",
      "loss: 1.880323  [ 6464/60000]\n",
      "loss: 1.765201  [12864/60000]\n",
      "loss: 1.804523  [19264/60000]\n",
      "loss: 1.701948  [25664/60000]\n",
      "loss: 1.646124  [32064/60000]\n",
      "loss: 1.667796  [38464/60000]\n",
      "loss: 1.571906  [44864/60000]\n",
      "loss: 1.585684  [51264/60000]\n",
      "loss: 1.488857  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.508101 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.574257  [   64/60000]\n",
      "loss: 1.538799  [ 6464/60000]\n",
      "loss: 1.394840  [12864/60000]\n",
      "loss: 1.464943  [19264/60000]\n",
      "loss: 1.351686  [25664/60000]\n",
      "loss: 1.342576  [32064/60000]\n",
      "loss: 1.355729  [38464/60000]\n",
      "loss: 1.285301  [44864/60000]\n",
      "loss: 1.312473  [51264/60000]\n",
      "loss: 1.215715  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.245796 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.325553  [   64/60000]\n",
      "loss: 1.302765  [ 6464/60000]\n",
      "loss: 1.141890  [12864/60000]\n",
      "loss: 1.246793  [19264/60000]\n",
      "loss: 1.127021  [25664/60000]\n",
      "loss: 1.145415  [32064/60000]\n",
      "loss: 1.166795  [38464/60000]\n",
      "loss: 1.107579  [44864/60000]\n",
      "loss: 1.139747  [51264/60000]\n",
      "loss: 1.056744  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.081928 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# check whether the model is on the device\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                  [-1, 512]         262,656\n",
      "              ReLU-5                  [-1, 512]               0\n",
      "            Linear-6                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.55\n",
      "Estimated Total Size (MB): 2.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Temporarily move the model to CPU for summary\n",
    "model_cpu = model.to(\"cpu\")\n",
    "# Run summary on the CPU (since torchsummary only supports 'cpu' or 'cuda')\n",
    "summary(model_cpu, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of NN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: linear_relu_stack.0.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0198, -0.0240, -0.0281,  ..., -0.0026,  0.0186,  0.0172],\n",
      "        [-0.0209, -0.0290,  0.0044,  ...,  0.0346, -0.0354,  0.0079],\n",
      "        [-0.0127, -0.0175, -0.0039,  ..., -0.0025, -0.0199,  0.0257],\n",
      "        ...,\n",
      "        [-0.0224,  0.0343, -0.0352,  ...,  0.0286, -0.0338,  0.0129],\n",
      "        [ 0.0190,  0.0294,  0.0186,  ..., -0.0056, -0.0158,  0.0142],\n",
      "        [-0.0340, -0.0249, -0.0080,  ...,  0.0003, -0.0034, -0.0322]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512, 784])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.0.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([-2.8885e-02,  3.0724e-02,  2.9379e-02,  1.8816e-02, -2.6011e-02,\n",
      "         3.3328e-03, -8.3296e-03,  2.3787e-02,  1.0857e-02, -1.3107e-02,\n",
      "        -1.2003e-02, -2.3429e-02,  2.5075e-02, -2.0783e-02,  3.2708e-02,\n",
      "        -3.0691e-03, -2.0024e-02, -3.4753e-02,  3.2016e-02, -1.2303e-02,\n",
      "         3.3830e-02,  2.0743e-02,  1.3980e-02,  1.5055e-02, -2.6451e-02,\n",
      "        -1.3471e-02, -6.7056e-03, -3.0503e-02,  3.6014e-02,  3.1312e-02,\n",
      "         5.1512e-03,  1.1884e-02,  3.5617e-02, -2.7165e-02, -2.5750e-03,\n",
      "        -4.1776e-04,  4.0322e-02,  2.3939e-02, -1.2212e-02, -1.6722e-02,\n",
      "         3.3738e-02, -2.0785e-02, -2.2053e-02,  3.7438e-02,  2.5169e-02,\n",
      "         1.6158e-02, -2.7755e-02,  1.1634e-02,  5.5876e-03,  3.6070e-02,\n",
      "         1.4774e-02, -3.1028e-02,  3.3399e-02, -2.2150e-02,  3.4895e-02,\n",
      "         2.8568e-03,  3.2175e-02, -1.1923e-02, -1.9742e-02,  1.8329e-02,\n",
      "        -1.4004e-02, -1.7513e-03,  2.2808e-02, -1.2054e-02, -2.7491e-02,\n",
      "        -2.4403e-02, -3.0005e-02,  2.1457e-02, -2.7426e-02,  1.7093e-03,\n",
      "        -6.1711e-03,  1.9490e-02, -2.9483e-02, -1.5260e-02,  9.6598e-03,\n",
      "         3.5024e-02,  2.8639e-02,  8.2873e-03,  6.6051e-03,  2.4135e-02,\n",
      "        -1.7530e-02,  3.6306e-02,  2.8074e-02,  9.3366e-03,  1.1191e-02,\n",
      "        -6.3301e-03,  1.1603e-02,  3.9490e-03, -1.2593e-02, -2.5030e-02,\n",
      "        -1.0173e-02, -2.1718e-02,  3.4006e-02,  4.6784e-03,  4.6822e-03,\n",
      "        -2.0531e-02, -2.0127e-02,  2.7868e-02, -1.6329e-03,  2.4791e-02,\n",
      "        -1.7827e-02, -1.8382e-02, -3.3869e-02, -2.4621e-02,  1.1351e-02,\n",
      "         1.5103e-02, -1.6036e-02, -1.6023e-02,  2.5024e-02, -7.4925e-04,\n",
      "        -5.4683e-03, -1.0623e-03,  4.1791e-02, -5.8408e-03,  2.0713e-02,\n",
      "         6.8200e-03,  2.1469e-02,  1.4076e-02, -1.3525e-02, -1.6181e-02,\n",
      "         1.3109e-02, -1.7925e-02, -1.5160e-02, -2.3532e-02, -1.1491e-02,\n",
      "        -1.1569e-02,  3.1583e-02, -2.3851e-02, -2.4035e-02, -2.8872e-02,\n",
      "         6.3931e-03,  1.0111e-02,  2.1893e-02, -1.6499e-02, -1.9316e-02,\n",
      "         2.5782e-02, -2.2701e-02,  1.0442e-02,  1.0493e-02, -2.5850e-02,\n",
      "        -2.6655e-02, -2.6080e-02,  7.1040e-03,  1.4631e-02, -2.9316e-03,\n",
      "        -1.0732e-02,  3.4686e-02,  6.5262e-03, -3.2515e-02,  7.0410e-03,\n",
      "        -3.1962e-03,  2.2169e-02,  1.8243e-02,  2.3066e-02,  8.5594e-03,\n",
      "        -1.0576e-02, -6.9825e-03, -2.8902e-02, -7.8992e-03, -3.4704e-02,\n",
      "         1.2152e-02,  1.1768e-02, -2.7994e-02,  3.1563e-03, -1.4162e-02,\n",
      "         2.1577e-03,  2.7977e-02,  2.3065e-02,  2.0058e-03, -1.4845e-02,\n",
      "         1.8814e-02, -3.0981e-02,  4.1021e-03, -1.5057e-02, -3.2262e-03,\n",
      "        -2.8094e-02, -6.2735e-03, -1.2439e-02,  1.6888e-02,  1.4186e-02,\n",
      "         2.5471e-03, -4.9098e-03, -2.4063e-02,  6.8739e-03,  5.5835e-03,\n",
      "        -2.0138e-03, -3.0230e-02,  8.9935e-03, -3.1582e-03, -2.6402e-03,\n",
      "         6.9623e-03,  1.4445e-02,  1.2577e-02, -3.0398e-02, -4.1235e-03,\n",
      "         3.4546e-02, -1.7241e-02, -1.5494e-02,  3.6180e-02, -4.2242e-03,\n",
      "         7.8436e-03,  2.5121e-02,  3.5520e-02,  5.6604e-03, -1.3966e-02,\n",
      "         2.3481e-02,  3.5543e-02, -1.6268e-02,  2.7687e-02,  1.3167e-02,\n",
      "        -2.5286e-02,  2.5845e-02, -1.4561e-02,  2.1928e-02, -7.3988e-03,\n",
      "         1.5272e-02,  4.3983e-03, -9.9506e-03, -1.9154e-02, -1.4455e-02,\n",
      "         3.3836e-02, -9.5681e-03, -1.2928e-02, -1.3505e-02,  3.5628e-02,\n",
      "         3.3122e-02,  3.4281e-02,  3.2445e-03,  1.0230e-02,  3.1368e-02,\n",
      "         1.0250e-02,  3.5164e-02, -8.1767e-03, -8.6655e-03,  5.5308e-04,\n",
      "        -6.4203e-05, -3.2897e-02, -3.4336e-04,  1.6520e-02,  1.3061e-02,\n",
      "         1.1053e-02, -1.1236e-02,  2.2638e-02, -2.9824e-02,  2.9866e-02,\n",
      "         1.3034e-02,  6.1655e-03,  3.6948e-02, -4.8140e-03, -3.1251e-02,\n",
      "         3.0594e-02,  7.0358e-03, -1.6830e-02, -4.2707e-03,  1.9405e-02,\n",
      "        -2.0211e-02, -3.2937e-02,  2.9705e-02, -4.6424e-03, -1.1979e-03,\n",
      "        -4.9173e-03,  2.3950e-02, -1.6355e-03,  1.8959e-02, -1.5052e-02,\n",
      "        -1.7156e-02,  2.6852e-02,  1.1895e-02, -1.6650e-02,  2.5973e-02,\n",
      "         2.5724e-02, -1.9650e-02, -2.6373e-02,  3.0847e-02, -9.9067e-03,\n",
      "         3.0696e-03,  1.8842e-02,  1.4721e-03,  2.4017e-02,  4.5269e-03,\n",
      "         3.0340e-02,  5.4461e-03, -9.0629e-03, -2.9003e-02, -2.8581e-02,\n",
      "         2.9850e-03,  9.4316e-03, -2.5414e-02, -5.2373e-03,  1.4917e-02,\n",
      "        -1.3218e-02, -4.2471e-03,  1.4559e-03, -2.0003e-02,  9.7698e-03,\n",
      "         2.8398e-02, -2.2471e-02,  3.8406e-03,  4.4285e-03,  1.2415e-03,\n",
      "         2.7740e-02,  2.7810e-02,  3.1017e-02,  2.6432e-03,  1.8180e-02,\n",
      "         7.0939e-03,  3.0187e-02,  2.6374e-02, -8.2700e-03, -5.5931e-03,\n",
      "        -1.0230e-03,  1.2614e-03, -7.2946e-03,  3.5336e-02, -3.6630e-03,\n",
      "         3.7084e-02,  3.0300e-02, -2.8261e-02, -1.4402e-02,  3.8279e-02,\n",
      "        -4.2088e-03, -3.4139e-04,  3.2320e-02, -3.4863e-02, -2.7873e-02,\n",
      "        -8.1019e-03,  3.0302e-02, -4.0284e-03,  2.9326e-02,  4.7302e-03,\n",
      "         2.1705e-02,  2.4218e-02, -1.8512e-02, -1.1455e-02,  1.0339e-02,\n",
      "        -1.2048e-03, -7.4692e-03,  2.9824e-03,  2.2866e-02,  2.2639e-02,\n",
      "         7.9727e-03, -3.1644e-02,  1.3154e-02,  1.8240e-02,  2.3594e-02,\n",
      "         2.1785e-02,  7.6093e-03,  1.1939e-02,  1.6937e-02,  9.6077e-03,\n",
      "         3.0290e-02, -2.0593e-02,  7.1623e-03,  1.1760e-02,  3.6298e-02,\n",
      "        -1.2266e-02,  1.6029e-02, -2.7689e-02,  1.3499e-02,  1.9047e-02,\n",
      "         3.3577e-02, -8.9625e-04,  1.4711e-02,  3.4088e-02, -9.8882e-03,\n",
      "        -2.0196e-02, -3.7565e-03,  3.2150e-03, -9.8563e-04, -1.1741e-02,\n",
      "        -2.3580e-03, -1.7564e-02, -1.9057e-02,  1.5487e-03, -1.3509e-02,\n",
      "         2.6124e-04,  6.3576e-03,  3.9402e-02, -2.0310e-02,  1.6905e-02,\n",
      "        -1.0779e-02, -5.9845e-03,  2.6781e-02,  2.9004e-02, -2.5973e-02,\n",
      "        -7.0098e-03,  9.4057e-03, -3.0828e-02, -1.0302e-02,  3.2541e-02,\n",
      "         5.2583e-03, -2.3972e-02,  5.2427e-03,  3.3820e-02, -7.5161e-03,\n",
      "        -3.0183e-02,  1.3372e-02, -1.0813e-02, -2.7164e-02, -1.0489e-02,\n",
      "         2.6978e-02,  4.1580e-02,  1.9987e-03, -8.5008e-03,  6.9476e-03,\n",
      "         2.8399e-02,  1.7234e-02, -1.8533e-02, -1.8766e-02,  3.7408e-02,\n",
      "         8.3304e-03, -1.5013e-03, -3.1446e-02,  4.8599e-03, -3.2452e-02,\n",
      "        -2.7679e-02,  6.7277e-04,  1.0489e-02,  2.5651e-02,  1.7761e-02,\n",
      "        -7.8822e-03,  6.4094e-03,  2.5847e-02,  1.6605e-02,  1.8394e-02,\n",
      "         5.2180e-03, -2.0659e-02, -2.1515e-02, -3.2425e-02, -1.8166e-02,\n",
      "        -2.1502e-02,  5.8326e-03, -2.9944e-02, -2.7053e-02,  8.5739e-03,\n",
      "        -1.0595e-03,  2.9983e-03, -1.2467e-02, -1.8524e-02,  2.7569e-02,\n",
      "        -5.8544e-03,  2.5351e-02, -2.0476e-02,  2.4097e-02, -3.0785e-02,\n",
      "        -2.4125e-02,  8.4826e-03,  3.6627e-02,  1.4092e-02, -3.1241e-02,\n",
      "         1.4641e-02, -9.2336e-03,  3.2551e-02,  3.6745e-03,  1.2485e-02,\n",
      "         1.0133e-02, -2.5361e-02, -3.8566e-03,  2.2444e-02, -8.8475e-03,\n",
      "        -2.3687e-02, -6.7430e-03, -1.4400e-02, -5.0400e-03, -2.9173e-02,\n",
      "        -1.3624e-02,  4.3945e-03,  1.6449e-02,  8.2914e-03,  9.6643e-03,\n",
      "        -1.8844e-02, -2.3437e-02, -6.4097e-03,  3.6470e-02,  6.2956e-03,\n",
      "        -2.1506e-02, -1.6173e-02,  2.0377e-02,  1.1097e-02, -1.9339e-02,\n",
      "         2.5463e-02,  3.0768e-02, -2.5754e-02, -3.7889e-03,  8.0438e-03,\n",
      "        -3.3928e-02,  4.3208e-03, -9.5974e-03,  1.7085e-02,  4.1546e-03,\n",
      "        -2.2891e-02, -1.3194e-02,  1.4833e-04, -1.5695e-03, -2.1336e-02,\n",
      "        -1.9836e-02,  1.0900e-02,  2.1443e-02,  2.2002e-02,  2.6338e-02,\n",
      "        -1.1083e-02, -2.0195e-02,  2.5126e-02,  3.4052e-02, -1.6206e-02,\n",
      "        -2.2948e-02,  2.6781e-02, -1.6364e-02,  2.6838e-02, -3.3768e-02,\n",
      "        -7.8922e-03, -1.7609e-02], device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0243, -0.0260,  0.0411,  ...,  0.0092, -0.0310, -0.0160],\n",
      "        [-0.0020, -0.0180,  0.0329,  ..., -0.0138,  0.0131, -0.0147],\n",
      "        [ 0.0244, -0.0315,  0.0422,  ...,  0.0165,  0.0201,  0.0302],\n",
      "        ...,\n",
      "        [-0.0096, -0.0282, -0.0193,  ..., -0.0136,  0.0162, -0.0244],\n",
      "        [ 0.0410, -0.0399,  0.0150,  ..., -0.0260,  0.0107, -0.0170],\n",
      "        [-0.0079, -0.0319, -0.0365,  ...,  0.0231,  0.0038, -0.0056]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512, 512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([ 0.0229, -0.0127, -0.0110, -0.0433, -0.0371, -0.0429,  0.0219,  0.0418,\n",
      "        -0.0069,  0.0366, -0.0199,  0.0164,  0.0279, -0.0311, -0.0042, -0.0179,\n",
      "         0.0192, -0.0104, -0.0349, -0.0180, -0.0337, -0.0136,  0.0202,  0.0451,\n",
      "        -0.0183,  0.0165,  0.0378, -0.0404, -0.0102, -0.0192, -0.0204,  0.0354,\n",
      "         0.0062, -0.0243,  0.0288,  0.0246, -0.0108,  0.0261, -0.0340, -0.0313,\n",
      "        -0.0158,  0.0084, -0.0121,  0.0295,  0.0070,  0.0430, -0.0136, -0.0180,\n",
      "         0.0292, -0.0409,  0.0218, -0.0064,  0.0024, -0.0006,  0.0112,  0.0418,\n",
      "        -0.0113,  0.0255, -0.0260, -0.0004, -0.0126,  0.0247,  0.0133,  0.0104,\n",
      "         0.0172,  0.0247,  0.0024,  0.0153,  0.0012,  0.0029, -0.0143,  0.0069,\n",
      "         0.0156, -0.0367, -0.0438, -0.0011,  0.0237,  0.0225,  0.0313, -0.0072,\n",
      "         0.0476,  0.0137,  0.0344, -0.0019,  0.0313,  0.0312, -0.0257, -0.0208,\n",
      "         0.0003,  0.0103,  0.0011,  0.0330, -0.0378,  0.0265, -0.0117,  0.0312,\n",
      "         0.0049, -0.0176, -0.0103,  0.0294, -0.0348, -0.0154, -0.0237, -0.0225,\n",
      "        -0.0231, -0.0113, -0.0233, -0.0195, -0.0112,  0.0380,  0.0342, -0.0050,\n",
      "         0.0246,  0.0057,  0.0474,  0.0276,  0.0430, -0.0050, -0.0197, -0.0322,\n",
      "        -0.0330, -0.0121, -0.0342,  0.0422, -0.0329, -0.0245,  0.0375, -0.0112,\n",
      "         0.0461,  0.0214,  0.0164,  0.0371,  0.0156,  0.0040,  0.0093,  0.0130,\n",
      "         0.0458, -0.0133, -0.0146,  0.0306,  0.0391, -0.0051, -0.0034, -0.0025,\n",
      "        -0.0033,  0.0216,  0.0354,  0.0355,  0.0356, -0.0164, -0.0033, -0.0283,\n",
      "        -0.0300, -0.0018, -0.0063, -0.0115,  0.0244,  0.0458, -0.0248, -0.0080,\n",
      "        -0.0320,  0.0462, -0.0409,  0.0096,  0.0167,  0.0008, -0.0306,  0.0400,\n",
      "        -0.0249,  0.0021,  0.0068, -0.0065,  0.0176,  0.0122, -0.0247,  0.0410,\n",
      "         0.0311,  0.0050,  0.0359, -0.0142,  0.0361,  0.0181, -0.0174,  0.0403,\n",
      "         0.0203,  0.0110,  0.0361,  0.0193,  0.0149,  0.0262, -0.0291,  0.0208,\n",
      "        -0.0350,  0.0307,  0.0305, -0.0239, -0.0414,  0.0014, -0.0318,  0.0043,\n",
      "         0.0427,  0.0146, -0.0005, -0.0388, -0.0314, -0.0002,  0.0157,  0.0316,\n",
      "         0.0039,  0.0304,  0.0315, -0.0054, -0.0129,  0.0441, -0.0159, -0.0020,\n",
      "         0.0183, -0.0037, -0.0322, -0.0148, -0.0162, -0.0176,  0.0056, -0.0001,\n",
      "        -0.0192, -0.0375, -0.0131, -0.0035, -0.0095,  0.0030,  0.0348, -0.0073,\n",
      "         0.0024,  0.0321, -0.0075, -0.0418,  0.0344,  0.0196, -0.0185, -0.0412,\n",
      "         0.0014,  0.0373, -0.0444,  0.0070, -0.0043, -0.0450,  0.0399,  0.0140,\n",
      "        -0.0375,  0.0079,  0.0311,  0.0283, -0.0026,  0.0156, -0.0438,  0.0249,\n",
      "         0.0246,  0.0142,  0.0241,  0.0374, -0.0039,  0.0347, -0.0442,  0.0301,\n",
      "         0.0035,  0.0415, -0.0143, -0.0115, -0.0207, -0.0128, -0.0322,  0.0174,\n",
      "         0.0207,  0.0273, -0.0009, -0.0330, -0.0173,  0.0229,  0.0358,  0.0357,\n",
      "         0.0060,  0.0437, -0.0033, -0.0124, -0.0334,  0.0002, -0.0350,  0.0084,\n",
      "         0.0045,  0.0148, -0.0370, -0.0022,  0.0116, -0.0365,  0.0063, -0.0118,\n",
      "         0.0408, -0.0172, -0.0054,  0.0120,  0.0001, -0.0400, -0.0057, -0.0173,\n",
      "        -0.0427,  0.0275,  0.0052, -0.0166,  0.0035, -0.0079, -0.0152, -0.0129,\n",
      "        -0.0135, -0.0163, -0.0061,  0.0304, -0.0159, -0.0249, -0.0122, -0.0179,\n",
      "         0.0106,  0.0111,  0.0181,  0.0007,  0.0157,  0.0084,  0.0200, -0.0347,\n",
      "        -0.0191,  0.0193, -0.0264, -0.0189, -0.0056, -0.0145, -0.0309, -0.0075,\n",
      "        -0.0119,  0.0063, -0.0434, -0.0003,  0.0215, -0.0133,  0.0353,  0.0124,\n",
      "         0.0381, -0.0232,  0.0189, -0.0025,  0.0326, -0.0198, -0.0417,  0.0101,\n",
      "         0.0169, -0.0150, -0.0345, -0.0086, -0.0377, -0.0331,  0.0373,  0.0046,\n",
      "        -0.0210, -0.0237,  0.0022,  0.0264, -0.0144,  0.0440,  0.0318,  0.0212,\n",
      "         0.0338,  0.0213,  0.0012, -0.0363, -0.0137, -0.0124, -0.0070,  0.0273,\n",
      "        -0.0293,  0.0189,  0.0029, -0.0189, -0.0303,  0.0284, -0.0288, -0.0278,\n",
      "        -0.0024, -0.0067,  0.0095, -0.0433,  0.0396, -0.0447,  0.0454, -0.0150,\n",
      "         0.0137,  0.0222, -0.0419, -0.0298, -0.0181, -0.0255,  0.0301,  0.0091,\n",
      "         0.0049,  0.0107,  0.0071, -0.0440,  0.0259, -0.0430, -0.0051,  0.0441,\n",
      "         0.0449,  0.0130,  0.0374,  0.0330, -0.0216, -0.0309,  0.0147, -0.0390,\n",
      "        -0.0283, -0.0111,  0.0133, -0.0416,  0.0042, -0.0125, -0.0225,  0.0006,\n",
      "         0.0133, -0.0212, -0.0001,  0.0339,  0.0448, -0.0243, -0.0127,  0.0321,\n",
      "         0.0429, -0.0198,  0.0096, -0.0344, -0.0333, -0.0437, -0.0188,  0.0020,\n",
      "         0.0378, -0.0329, -0.0290, -0.0348,  0.0417, -0.0395, -0.0432,  0.0386,\n",
      "        -0.0055,  0.0476, -0.0362,  0.0272,  0.0366, -0.0187,  0.0309,  0.0528,\n",
      "        -0.0199, -0.0188, -0.0408, -0.0384,  0.0309, -0.0352, -0.0246, -0.0389,\n",
      "        -0.0030, -0.0434,  0.0206, -0.0371, -0.0405, -0.0218, -0.0190, -0.0308,\n",
      "         0.0206, -0.0348, -0.0057,  0.0233, -0.0277,  0.0061, -0.0213, -0.0046,\n",
      "         0.0165, -0.0080,  0.0299,  0.0172,  0.0451, -0.0050,  0.0302,  0.0428,\n",
      "        -0.0021,  0.0365,  0.0245, -0.0442, -0.0359, -0.0078,  0.0214, -0.0008,\n",
      "         0.0436, -0.0366,  0.0389,  0.0187, -0.0203,  0.0246,  0.0153,  0.0082,\n",
      "        -0.0039, -0.0364,  0.0096, -0.0174,  0.0395, -0.0437, -0.0338, -0.0081],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0111, -0.0099,  0.0434,  ..., -0.0018, -0.0055, -0.0068],\n",
      "        [ 0.0382, -0.0060,  0.0218,  ..., -0.0421, -0.0082, -0.0253],\n",
      "        [ 0.0442, -0.0180, -0.0276,  ...,  0.0168, -0.0266, -0.0434],\n",
      "        ...,\n",
      "        [-0.0370, -0.0414,  0.0311,  ..., -0.0035,  0.0178,  0.0112],\n",
      "        [-0.0197,  0.0046, -0.0029,  ..., -0.0168,  0.0298,  0.0347],\n",
      "        [ 0.0007, -0.0411, -0.0018,  ..., -0.0355, -0.0071,  0.0419]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512, 512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([ 1.7318e-02, -7.0786e-03,  5.1649e-02, -4.2493e-02, -3.2445e-02,\n",
      "        -1.8458e-02, -2.1479e-02,  3.8223e-02,  2.1537e-02,  2.0985e-02,\n",
      "         2.0438e-02, -1.2273e-02, -1.4187e-02,  4.1451e-02, -3.9474e-02,\n",
      "        -1.8199e-02,  1.9097e-02,  3.5826e-02, -4.3577e-03,  1.9929e-02,\n",
      "         2.4040e-03, -4.2782e-02, -9.7647e-03, -3.8371e-02,  3.8025e-02,\n",
      "        -2.2879e-02,  2.1958e-02, -3.4801e-02, -2.8919e-02,  9.4573e-03,\n",
      "        -1.4233e-02, -1.4332e-02,  1.6035e-03,  2.7010e-02,  2.5684e-02,\n",
      "        -2.2337e-03, -1.3826e-02, -1.7289e-02,  2.9542e-02,  3.6123e-02,\n",
      "        -3.6980e-02,  3.8812e-02,  7.9216e-03,  1.3671e-02,  2.6618e-02,\n",
      "        -4.8472e-04, -4.3931e-03, -1.1935e-02,  5.8692e-02,  3.2235e-02,\n",
      "         1.5881e-02, -1.2363e-02, -2.8166e-02, -1.9381e-02,  3.2088e-03,\n",
      "        -3.0191e-02,  4.3764e-02,  2.0722e-02,  4.9771e-03, -2.0293e-02,\n",
      "        -3.2640e-02, -1.5983e-02, -3.1433e-03,  2.8204e-02, -4.5101e-02,\n",
      "        -5.7470e-04,  3.4318e-02, -5.3816e-03,  1.4172e-03,  3.9414e-02,\n",
      "        -7.5540e-03,  2.8211e-02, -2.4182e-02, -4.1158e-03,  3.7236e-02,\n",
      "        -1.3803e-02, -1.2562e-02,  1.1383e-02,  3.6048e-02, -3.6543e-02,\n",
      "         3.5460e-03, -4.1006e-02,  1.9022e-02,  1.2054e-02, -3.3377e-02,\n",
      "         4.0055e-02,  5.7994e-02, -3.7058e-02,  4.2803e-02,  1.9083e-02,\n",
      "         6.9325e-03, -3.2383e-02, -1.4928e-02,  3.7061e-02, -1.8199e-02,\n",
      "         3.3508e-02, -2.8972e-02, -2.2529e-02, -9.1809e-03, -4.0686e-02,\n",
      "         2.9859e-02, -3.0697e-02,  1.6391e-03, -1.3039e-02, -6.2488e-04,\n",
      "        -1.7751e-02, -9.3508e-03, -1.1345e-02, -7.5826e-03,  3.0295e-02,\n",
      "         4.8416e-04,  2.2127e-02, -1.1442e-03,  9.9213e-03,  4.7730e-03,\n",
      "         1.6236e-02,  8.3317e-03,  5.1639e-02, -2.1522e-02,  2.9450e-03,\n",
      "        -3.2094e-02,  5.0561e-02,  2.2980e-02,  1.2752e-02,  2.8797e-02,\n",
      "        -8.3027e-03,  2.8510e-02, -1.3843e-02, -2.0121e-02,  4.5677e-02,\n",
      "         4.4034e-02,  2.9158e-02, -5.6465e-03, -3.7396e-02,  1.6665e-02,\n",
      "         3.8218e-02, -1.8409e-02, -1.7208e-02, -1.6851e-03, -4.1556e-02,\n",
      "         1.2530e-02,  6.5586e-03, -1.8510e-02,  3.8474e-02,  4.5585e-02,\n",
      "        -2.4897e-02, -2.4011e-02, -2.2820e-02, -2.0322e-02, -4.5782e-02,\n",
      "         4.0973e-02, -1.5832e-02,  4.9533e-02,  3.7102e-02,  4.1319e-02,\n",
      "         1.9629e-02, -1.2717e-02,  1.4209e-02, -4.2959e-02,  3.6806e-02,\n",
      "         3.2707e-02, -9.2717e-03, -3.2657e-02,  1.6861e-02,  4.5318e-02,\n",
      "         4.6716e-02,  3.5540e-02, -3.6700e-02,  1.3706e-02,  1.6120e-03,\n",
      "         2.1601e-02,  3.9209e-02, -3.0604e-02,  3.8311e-02, -7.1445e-03,\n",
      "         2.9508e-02,  5.2320e-02, -3.8854e-03,  1.1641e-03,  1.1876e-02,\n",
      "         4.0344e-02,  1.7832e-02, -4.0858e-02,  2.8024e-02,  2.5785e-02,\n",
      "        -7.5955e-03,  2.6049e-02, -1.6633e-02, -3.1560e-02, -2.3155e-02,\n",
      "        -1.5899e-02, -2.7093e-02, -3.2596e-02,  5.0151e-02,  3.4519e-02,\n",
      "         1.2227e-02,  2.9517e-05, -3.6431e-02,  1.9964e-03,  4.2379e-02,\n",
      "        -2.2897e-02, -4.0681e-02, -1.1113e-02, -2.3959e-02,  1.4136e-03,\n",
      "         1.1196e-02,  1.7967e-04, -3.8054e-02, -3.7807e-02, -1.5872e-02,\n",
      "         3.0692e-02,  2.7421e-02, -2.8844e-02,  2.1159e-02,  3.9894e-02,\n",
      "        -2.2613e-02, -2.0598e-02, -8.7994e-03,  3.5445e-03,  2.0905e-02,\n",
      "         5.0495e-02,  1.3552e-02, -3.5824e-02,  2.7904e-02, -7.2040e-03,\n",
      "        -3.4532e-03,  2.2455e-02,  1.4797e-02,  5.5636e-03, -2.9176e-02,\n",
      "         1.3649e-02,  3.1527e-02, -2.4141e-02,  3.8623e-02,  1.1454e-02,\n",
      "        -3.3364e-02,  6.1380e-03,  2.3510e-03, -2.2466e-02, -3.4898e-02,\n",
      "        -1.6551e-02, -3.4108e-02,  4.0410e-02,  2.8591e-02,  1.4965e-02,\n",
      "        -4.0670e-02,  4.9752e-02, -2.3617e-02,  3.5341e-02, -2.3345e-02,\n",
      "        -1.3766e-02,  1.8356e-02, -3.8041e-02, -8.8572e-03, -4.1481e-02,\n",
      "        -3.5497e-02, -4.3750e-03, -4.5579e-02,  2.0809e-02,  3.7414e-02,\n",
      "         9.8663e-03, -3.6341e-03, -1.1312e-02, -2.2951e-02,  1.9252e-02,\n",
      "         7.5982e-03,  3.3520e-02, -3.8377e-02, -1.6939e-02, -1.1220e-02,\n",
      "        -3.2882e-02, -2.9923e-02,  2.7348e-02,  2.0158e-02, -4.2816e-02,\n",
      "         1.2838e-03,  7.2118e-04, -2.1677e-02,  2.6818e-02,  5.0560e-02,\n",
      "        -1.4425e-02,  8.4564e-03,  3.4376e-03, -5.5125e-03,  7.6474e-03,\n",
      "        -3.9116e-02, -2.8546e-02,  1.7744e-02, -3.0328e-02, -1.2231e-02,\n",
      "         1.4378e-03,  1.3724e-02,  2.0308e-02, -4.8399e-02,  2.0390e-02,\n",
      "        -1.2923e-02, -1.1333e-02,  2.7757e-02,  3.3632e-02,  6.7620e-02,\n",
      "        -1.1302e-02, -3.6600e-02,  2.3489e-02, -2.3861e-02, -4.0567e-02,\n",
      "        -1.9069e-02, -2.5359e-02, -4.1630e-02,  3.0156e-02, -1.8986e-02,\n",
      "         1.0664e-02, -3.4734e-02, -1.1103e-02,  1.8290e-02,  1.5657e-02,\n",
      "        -4.0499e-02,  1.0822e-02, -1.3418e-02, -2.0384e-02,  3.6871e-02,\n",
      "        -9.6542e-03, -2.9336e-02,  3.9981e-02, -9.9299e-03,  1.1755e-02,\n",
      "         1.8593e-02, -1.6314e-02,  3.7931e-02,  2.0290e-02, -1.7872e-02,\n",
      "        -3.4333e-02, -2.9049e-02, -3.2904e-03, -2.5979e-02,  2.0239e-02,\n",
      "         1.5943e-03,  2.8937e-02,  3.8040e-02,  3.6422e-02,  3.8323e-02,\n",
      "        -3.9405e-02, -4.2706e-02,  5.0973e-02, -3.9356e-02,  2.8759e-02,\n",
      "         2.8232e-02, -1.9673e-02,  7.0297e-03, -3.8596e-02, -3.8849e-02,\n",
      "         5.1346e-02, -4.1865e-02,  6.9250e-03, -2.7650e-02, -3.0347e-02,\n",
      "         1.8172e-02,  1.5477e-02,  5.2746e-02,  2.7849e-02, -2.0724e-02,\n",
      "         1.8695e-02,  3.5093e-02,  1.5005e-02,  2.8715e-02, -4.7202e-03,\n",
      "        -3.8569e-02,  8.8188e-03, -1.1842e-02,  2.4387e-02, -7.7057e-03,\n",
      "         3.0523e-02,  6.0297e-02,  4.8035e-03,  2.3994e-02,  3.7991e-02,\n",
      "        -3.8168e-02, -3.5550e-02,  1.5020e-02,  3.0560e-02,  2.2268e-02,\n",
      "        -3.0583e-02,  8.9375e-03, -2.9819e-02, -2.6468e-02, -8.9838e-03,\n",
      "         4.3387e-02, -7.1571e-03,  6.2081e-03, -3.3014e-02,  2.2477e-02,\n",
      "        -4.3030e-02, -2.4145e-02,  3.8630e-03,  6.6563e-03, -4.8912e-03,\n",
      "        -2.2387e-02,  4.6273e-02, -1.0038e-02,  2.9692e-02,  2.0669e-02,\n",
      "        -8.6690e-03,  3.6143e-02,  1.6267e-03,  5.6556e-04,  6.4012e-02,\n",
      "        -1.3351e-02, -1.6570e-02,  1.4989e-02,  3.9805e-02, -1.4299e-02,\n",
      "        -2.0390e-02,  2.0996e-02,  3.4990e-02,  2.3646e-02, -3.3207e-02,\n",
      "         1.1094e-02,  1.9215e-02, -1.9237e-03, -1.9658e-03,  4.1990e-02,\n",
      "        -1.1653e-02, -1.5857e-02,  4.1246e-02,  5.1535e-02, -2.8662e-02,\n",
      "         4.6660e-02, -3.3733e-03, -8.9742e-03,  2.2768e-02,  4.4153e-02,\n",
      "         2.2175e-02,  1.5134e-02, -2.2706e-02,  2.2160e-02,  2.5405e-02,\n",
      "        -2.8442e-02,  2.7418e-02, -1.0027e-02, -2.1351e-02,  2.5567e-02,\n",
      "        -8.0402e-03,  3.0763e-02,  1.7126e-02,  8.4807e-03, -2.9678e-03,\n",
      "         3.8959e-02, -2.7971e-02,  3.2955e-02,  6.7448e-04,  4.0772e-02,\n",
      "         3.2623e-02, -4.2135e-02,  4.1348e-02, -4.3031e-02,  3.9530e-02,\n",
      "         3.8369e-02,  4.3948e-02, -3.4167e-02, -4.1421e-02, -5.8035e-03,\n",
      "         1.9297e-02,  2.8286e-02, -1.9136e-02,  9.0776e-03,  4.5863e-02,\n",
      "         8.0319e-03,  1.7295e-02, -3.0505e-02,  2.4401e-02,  4.6866e-02,\n",
      "         2.9642e-02, -9.0860e-03,  8.3195e-03,  3.8180e-02, -2.5324e-02,\n",
      "         3.0338e-02,  2.3351e-02,  1.6309e-02, -4.4416e-02, -1.9997e-02,\n",
      "         3.8197e-02,  2.5456e-02,  3.1958e-02,  1.8765e-03,  3.4278e-03,\n",
      "        -1.5653e-02,  1.6024e-02, -2.7073e-02, -1.0107e-02, -1.4037e-02,\n",
      "         8.9892e-03,  2.0674e-02, -1.9008e-02, -3.0292e-02, -1.9484e-02,\n",
      "        -2.2011e-02, -2.3906e-03, -2.3464e-02,  3.6241e-02,  9.5316e-03,\n",
      "         2.3328e-02,  2.9730e-02,  2.9578e-03, -2.3735e-02, -1.4557e-02,\n",
      "         1.1157e-02,  4.1269e-02, -6.2547e-03, -4.2742e-02, -1.4737e-02,\n",
      "        -1.8577e-02, -9.7954e-03], device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.6.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0318, -0.0446,  0.0377,  ..., -0.0204,  0.0379,  0.0403],\n",
      "        [-0.0184, -0.0006,  0.0501,  ..., -0.0427,  0.0310,  0.0684],\n",
      "        [-0.0067,  0.0278, -0.0425,  ...,  0.0328,  0.0382, -0.0118],\n",
      "        ...,\n",
      "        [ 0.0414,  0.0268,  0.0280,  ...,  0.0235, -0.0305, -0.0345],\n",
      "        [-0.0050,  0.0375, -0.0408,  ..., -0.0232, -0.0317,  0.0017],\n",
      "        [ 0.0291, -0.0076, -0.0110,  ..., -0.0194, -0.0420, -0.0927]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([10, 512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.6.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([-0.0687,  0.0135, -0.0485,  0.0083, -0.0418,  0.1179,  0.0036,  0.0991,\n",
      "        -0.0287, -0.0035], device='mps:0', requires_grad=True)\n",
      "Parameter shape: torch.Size([10])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summarize the model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter value: {param}\")\n",
    "    print(f\"Parameter shape: {param.shape}\")\n",
    "    print(f\"Requires gradient: {param.requires_grad}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0066,  0.0215,  0.0193,  ...,  0.0235,  0.0274, -0.0279],\n",
       "                      [-0.0350, -0.0250, -0.0346,  ..., -0.0334,  0.0346, -0.0344],\n",
       "                      [-0.0188, -0.0136,  0.0166,  ...,  0.0122,  0.0054,  0.0343],\n",
       "                      ...,\n",
       "                      [-0.0079,  0.0078,  0.0078,  ..., -0.0295,  0.0314,  0.0287],\n",
       "                      [-0.0233,  0.0336,  0.0211,  ..., -0.0098,  0.0105,  0.0038],\n",
       "                      [-0.0138, -0.0130,  0.0253,  ...,  0.0225,  0.0158,  0.0037]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 0.0294,  0.0148,  0.0076, -0.0061,  0.0032,  0.0023, -0.0267,  0.0253,\n",
       "                       0.0117, -0.0197,  0.0110, -0.0230, -0.0116,  0.0078, -0.0291, -0.0293,\n",
       "                       0.0187,  0.0368, -0.0078,  0.0149, -0.0291, -0.0277,  0.0263, -0.0027,\n",
       "                       0.0327,  0.0137, -0.0033,  0.0350,  0.0359,  0.0151,  0.0226, -0.0191,\n",
       "                       0.0154,  0.0027,  0.0309,  0.0260,  0.0124,  0.0073, -0.0041,  0.0293,\n",
       "                       0.0156,  0.0199,  0.0206, -0.0115,  0.0074, -0.0291,  0.0312,  0.0119,\n",
       "                       0.0263, -0.0178, -0.0193, -0.0271,  0.0346,  0.0315, -0.0332,  0.0379,\n",
       "                      -0.0312, -0.0110, -0.0231,  0.0036, -0.0379, -0.0207,  0.0305,  0.0285,\n",
       "                       0.0437, -0.0161,  0.0275,  0.0166, -0.0242, -0.0295, -0.0118, -0.0025,\n",
       "                       0.0279,  0.0146,  0.0135, -0.0148, -0.0286, -0.0274, -0.0232,  0.0093,\n",
       "                      -0.0057, -0.0093, -0.0225,  0.0229,  0.0035, -0.0166,  0.0031,  0.0089,\n",
       "                       0.0248,  0.0212,  0.0318, -0.0127,  0.0187,  0.0328,  0.0008, -0.0186,\n",
       "                      -0.0223, -0.0238,  0.0233, -0.0213, -0.0145,  0.0074,  0.0277,  0.0275,\n",
       "                       0.0115,  0.0221, -0.0155,  0.0341,  0.0147,  0.0215,  0.0172,  0.0119,\n",
       "                      -0.0209, -0.0014,  0.0059, -0.0147,  0.0118,  0.0108,  0.0269, -0.0213,\n",
       "                       0.0271,  0.0214, -0.0284, -0.0186, -0.0021, -0.0314, -0.0149,  0.0009,\n",
       "                       0.0177, -0.0227, -0.0081, -0.0350,  0.0062,  0.0079, -0.0022, -0.0270,\n",
       "                       0.0150, -0.0223, -0.0029,  0.0058,  0.0330, -0.0121,  0.0293, -0.0249,\n",
       "                      -0.0216,  0.0106,  0.0137,  0.0134, -0.0329,  0.0054,  0.0476,  0.0223,\n",
       "                       0.0127,  0.0367, -0.0383, -0.0305,  0.0321, -0.0264,  0.0133,  0.0270,\n",
       "                       0.0189,  0.0265,  0.0159, -0.0320,  0.0077,  0.0180,  0.0152,  0.0039,\n",
       "                      -0.0165, -0.0050, -0.0175, -0.0105,  0.0212,  0.0009, -0.0183,  0.0229,\n",
       "                      -0.0127, -0.0024,  0.0016, -0.0150, -0.0056,  0.0314,  0.0057, -0.0018,\n",
       "                      -0.0007,  0.0079,  0.0204,  0.0330,  0.0238,  0.0062, -0.0277, -0.0155,\n",
       "                       0.0133, -0.0310,  0.0151, -0.0015,  0.0035, -0.0189,  0.0187,  0.0213,\n",
       "                       0.0258, -0.0032, -0.0313,  0.0070, -0.0052, -0.0025,  0.0069,  0.0098,\n",
       "                      -0.0102,  0.0120,  0.0169,  0.0182,  0.0260, -0.0176,  0.0101,  0.0158,\n",
       "                       0.0079,  0.0409, -0.0252,  0.0332, -0.0247, -0.0158,  0.0188,  0.0316,\n",
       "                       0.0071,  0.0137, -0.0134, -0.0341,  0.0377, -0.0119,  0.0312, -0.0154,\n",
       "                       0.0319,  0.0270, -0.0182, -0.0253, -0.0098,  0.0244, -0.0249, -0.0070,\n",
       "                       0.0017,  0.0394, -0.0257,  0.0095,  0.0003,  0.0002,  0.0073,  0.0043,\n",
       "                      -0.0017,  0.0335,  0.0023, -0.0082, -0.0264,  0.0329,  0.0182, -0.0201,\n",
       "                      -0.0037,  0.0382, -0.0026,  0.0286, -0.0066, -0.0159, -0.0027,  0.0063,\n",
       "                      -0.0048, -0.0208,  0.0227,  0.0168, -0.0252,  0.0194, -0.0147, -0.0005,\n",
       "                      -0.0050, -0.0281, -0.0179,  0.0126, -0.0152,  0.0273,  0.0205,  0.0142,\n",
       "                       0.0162, -0.0072, -0.0161,  0.0169, -0.0017, -0.0063,  0.0135, -0.0122,\n",
       "                      -0.0190, -0.0039,  0.0387, -0.0026,  0.0020,  0.0280,  0.0184, -0.0304,\n",
       "                      -0.0218, -0.0004, -0.0012,  0.0263,  0.0328, -0.0024,  0.0182,  0.0059,\n",
       "                      -0.0021,  0.0254, -0.0119, -0.0249, -0.0198, -0.0337,  0.0033, -0.0175,\n",
       "                      -0.0108,  0.0145, -0.0218,  0.0129, -0.0238, -0.0116, -0.0147, -0.0203,\n",
       "                       0.0128,  0.0091, -0.0260, -0.0215, -0.0140,  0.0207,  0.0100,  0.0287,\n",
       "                       0.0131, -0.0141,  0.0228,  0.0089,  0.0243,  0.0349, -0.0342,  0.0255,\n",
       "                      -0.0011,  0.0148,  0.0023, -0.0242,  0.0179, -0.0218,  0.0095, -0.0171,\n",
       "                       0.0272,  0.0070,  0.0182, -0.0166,  0.0267,  0.0294, -0.0272,  0.0242,\n",
       "                       0.0118,  0.0201, -0.0033,  0.0294, -0.0079,  0.0392,  0.0044,  0.0218,\n",
       "                       0.0288, -0.0306, -0.0208,  0.0043, -0.0274, -0.0268, -0.0183, -0.0011,\n",
       "                       0.0192,  0.0355,  0.0147, -0.0140,  0.0244,  0.0022, -0.0265,  0.0037,\n",
       "                       0.0215,  0.0001, -0.0344,  0.0400,  0.0173,  0.0178,  0.0169,  0.0103,\n",
       "                       0.0218, -0.0210,  0.0297,  0.0347, -0.0235, -0.0293,  0.0274, -0.0005,\n",
       "                      -0.0278,  0.0047,  0.0027,  0.0147,  0.0334,  0.0171,  0.0252, -0.0080,\n",
       "                      -0.0257, -0.0259, -0.0198, -0.0351,  0.0196, -0.0024,  0.0153,  0.0029,\n",
       "                       0.0162,  0.0385, -0.0020,  0.0096, -0.0208, -0.0084, -0.0296,  0.0232,\n",
       "                      -0.0340,  0.0266, -0.0235,  0.0189, -0.0363, -0.0163,  0.0204, -0.0110,\n",
       "                      -0.0202,  0.0256,  0.0383, -0.0019, -0.0061, -0.0073, -0.0123, -0.0173,\n",
       "                      -0.0238,  0.0154, -0.0089, -0.0266, -0.0152, -0.0119, -0.0308,  0.0171,\n",
       "                       0.0227, -0.0141,  0.0200, -0.0126,  0.0084,  0.0026, -0.0034, -0.0220,\n",
       "                       0.0244, -0.0040,  0.0446, -0.0120, -0.0137,  0.0101,  0.0289,  0.0218,\n",
       "                      -0.0173, -0.0097,  0.0099,  0.0059, -0.0310,  0.0150, -0.0184,  0.0298,\n",
       "                      -0.0278, -0.0235, -0.0231,  0.0280,  0.0108, -0.0168,  0.0298, -0.0036,\n",
       "                      -0.0266,  0.0053,  0.0229,  0.0124, -0.0169,  0.0099,  0.0308,  0.0340,\n",
       "                      -0.0142, -0.0214,  0.0087, -0.0186, -0.0168,  0.0280,  0.0301,  0.0278,\n",
       "                      -0.0101, -0.0197, -0.0104, -0.0365, -0.0083,  0.0071, -0.0190,  0.0109,\n",
       "                      -0.0259, -0.0303,  0.0210,  0.0042, -0.0004, -0.0206,  0.0141,  0.0202,\n",
       "                       0.0042,  0.0257,  0.0176,  0.0268, -0.0123, -0.0329,  0.0076, -0.0237])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0074,  0.0169,  0.0010,  ...,  0.0047, -0.0277, -0.0420],\n",
       "                      [-0.0239, -0.0320,  0.0357,  ..., -0.0366,  0.0321,  0.0177],\n",
       "                      [-0.0295,  0.0372, -0.0088,  ..., -0.0418,  0.0187,  0.0428],\n",
       "                      ...,\n",
       "                      [-0.0146,  0.0213, -0.0117,  ...,  0.0052,  0.0115,  0.0108],\n",
       "                      [ 0.0018,  0.0293,  0.0036,  ..., -0.0256, -0.0268,  0.0457],\n",
       "                      [ 0.0424, -0.0099, -0.0012,  ..., -0.0157,  0.0424, -0.0170]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-0.0117, -0.0302, -0.0154,  0.0354, -0.0179,  0.0005, -0.0295,  0.0341,\n",
       "                      -0.0055,  0.0134, -0.0277,  0.0423, -0.0398, -0.0096, -0.0123,  0.0387,\n",
       "                       0.0026, -0.0276, -0.0349, -0.0382,  0.0364, -0.0120,  0.0011, -0.0239,\n",
       "                      -0.0225,  0.0042,  0.0293,  0.0332,  0.0409,  0.0429, -0.0232, -0.0057,\n",
       "                       0.0165, -0.0234,  0.0064,  0.0258, -0.0007,  0.0010, -0.0104, -0.0309,\n",
       "                       0.0109, -0.0179, -0.0113,  0.0155,  0.0299, -0.0431,  0.0134,  0.0440,\n",
       "                      -0.0410, -0.0236, -0.0118,  0.0173, -0.0402,  0.0531, -0.0154,  0.0227,\n",
       "                       0.0301, -0.0323,  0.0448,  0.0130, -0.0372,  0.0188, -0.0050, -0.0193,\n",
       "                       0.0061,  0.0278, -0.0218, -0.0185,  0.0113, -0.0183, -0.0231,  0.0325,\n",
       "                      -0.0115, -0.0123, -0.0400,  0.0604, -0.0041,  0.0041,  0.0419,  0.0418,\n",
       "                      -0.0079, -0.0121,  0.0257,  0.0293,  0.0059,  0.0479,  0.0057, -0.0271,\n",
       "                       0.0047, -0.0387,  0.0122,  0.0396, -0.0371, -0.0081,  0.0194, -0.0101,\n",
       "                      -0.0389, -0.0143, -0.0037, -0.0307,  0.0267, -0.0063,  0.0113,  0.0338,\n",
       "                       0.0218, -0.0239, -0.0341,  0.0454,  0.0139,  0.0249,  0.0287, -0.0094,\n",
       "                       0.0326,  0.0419, -0.0366,  0.0436,  0.0260,  0.0100,  0.0210,  0.0151,\n",
       "                      -0.0074,  0.0305,  0.0331, -0.0464,  0.0447,  0.0262, -0.0252, -0.0401,\n",
       "                       0.0009, -0.0099, -0.0042, -0.0434,  0.0403, -0.0104, -0.0173, -0.0106,\n",
       "                       0.0110, -0.0277, -0.0340,  0.0038, -0.0119,  0.0416, -0.0164, -0.0451,\n",
       "                       0.0481, -0.0260, -0.0243,  0.0239, -0.0118, -0.0212, -0.0348, -0.0314,\n",
       "                      -0.0310,  0.0059,  0.0242,  0.0262, -0.0355,  0.0259, -0.0318, -0.0312,\n",
       "                       0.0407,  0.0173, -0.0056,  0.0198,  0.0280,  0.0498,  0.0396,  0.0370,\n",
       "                      -0.0236, -0.0137,  0.0156,  0.0401,  0.0087,  0.0394, -0.0261,  0.0227,\n",
       "                       0.0265, -0.0080,  0.0317,  0.0188, -0.0380,  0.0290, -0.0391, -0.0181,\n",
       "                       0.0403,  0.0451, -0.0079,  0.0095,  0.0160,  0.0279,  0.0015, -0.0409,\n",
       "                       0.0190, -0.0144, -0.0145, -0.0287,  0.0036,  0.0495,  0.0210, -0.0273,\n",
       "                      -0.0014, -0.0184, -0.0476,  0.0598, -0.0195, -0.0152,  0.0538, -0.0310,\n",
       "                       0.0033, -0.0402, -0.0283,  0.0398, -0.0128, -0.0400,  0.0355,  0.0473,\n",
       "                       0.0228,  0.0144, -0.0203,  0.0257, -0.0393, -0.0067, -0.0020, -0.0156,\n",
       "                      -0.0261,  0.0073,  0.0287,  0.0181, -0.0319,  0.0003,  0.0218, -0.0050,\n",
       "                      -0.0002,  0.0444,  0.0037,  0.0408,  0.0208, -0.0093,  0.0062,  0.0160,\n",
       "                       0.0468, -0.0053,  0.0309,  0.0447,  0.0369,  0.0308,  0.0358,  0.0291,\n",
       "                      -0.0178,  0.0389,  0.0038,  0.0322, -0.0264, -0.0014,  0.0372,  0.0232,\n",
       "                      -0.0120, -0.0047, -0.0215,  0.0391,  0.0130,  0.0142, -0.0015, -0.0230,\n",
       "                      -0.0116, -0.0024, -0.0142, -0.0267, -0.0377,  0.0104, -0.0354, -0.0290,\n",
       "                       0.0094, -0.0115,  0.0254,  0.0125, -0.0254,  0.0063,  0.0406,  0.0064,\n",
       "                       0.0196, -0.0018,  0.0073,  0.0106,  0.0044,  0.0196,  0.0177,  0.0214,\n",
       "                      -0.0351, -0.0418, -0.0060, -0.0205,  0.0320,  0.0388,  0.0400,  0.0335,\n",
       "                      -0.0029,  0.0024, -0.0184,  0.0082,  0.0263, -0.0048, -0.0332, -0.0274,\n",
       "                      -0.0227,  0.0292, -0.0050, -0.0032,  0.0074, -0.0272, -0.0352, -0.0075,\n",
       "                      -0.0202, -0.0014, -0.0379,  0.0261, -0.0079,  0.0196, -0.0253,  0.0197,\n",
       "                      -0.0137, -0.0291,  0.0085,  0.0042, -0.0078, -0.0231, -0.0427, -0.0229,\n",
       "                      -0.0261, -0.0433,  0.0462,  0.0141, -0.0146,  0.0385, -0.0175, -0.0222,\n",
       "                       0.0213,  0.0508,  0.0341, -0.0014, -0.0172, -0.0402,  0.0576, -0.0325,\n",
       "                      -0.0112, -0.0208,  0.0191, -0.0193,  0.0189,  0.0137,  0.0162, -0.0165,\n",
       "                       0.0073,  0.0346, -0.0343,  0.0305, -0.0008, -0.0126,  0.0004,  0.0384,\n",
       "                       0.0095, -0.0074, -0.0384,  0.0201,  0.0316,  0.0218, -0.0422,  0.0197,\n",
       "                      -0.0131, -0.0241,  0.0126,  0.0170, -0.0308,  0.0380, -0.0275, -0.0049,\n",
       "                      -0.0402, -0.0364, -0.0215,  0.0205,  0.0055,  0.0083, -0.0153, -0.0072,\n",
       "                       0.0308, -0.0240, -0.0350,  0.0085,  0.0157,  0.0191, -0.0414, -0.0405,\n",
       "                       0.0129,  0.0401,  0.0360, -0.0085,  0.0231, -0.0396,  0.0487, -0.0098,\n",
       "                       0.0366, -0.0461,  0.0286,  0.0510,  0.0040, -0.0288,  0.0239,  0.0495,\n",
       "                       0.0290,  0.0265,  0.0102,  0.0384,  0.0350, -0.0197, -0.0026,  0.0263,\n",
       "                      -0.0046,  0.0399, -0.0094, -0.0306,  0.0241,  0.0353, -0.0352, -0.0366,\n",
       "                      -0.0214,  0.0442,  0.0186, -0.0094,  0.0181, -0.0235,  0.0501,  0.0096,\n",
       "                      -0.0403,  0.0141,  0.0103,  0.0223,  0.0224, -0.0330, -0.0172,  0.0294,\n",
       "                      -0.0054,  0.0259, -0.0216,  0.0450, -0.0217,  0.0200, -0.0226,  0.0241,\n",
       "                       0.0115,  0.0086,  0.0230,  0.0336,  0.0292, -0.0297,  0.0033,  0.0328,\n",
       "                      -0.0396, -0.0024,  0.0027,  0.0259,  0.0042,  0.0269,  0.0278, -0.0234,\n",
       "                       0.0292, -0.0208,  0.0278,  0.0290,  0.0093,  0.0285, -0.0222, -0.0325,\n",
       "                      -0.0380, -0.0189,  0.0219,  0.0224, -0.0037, -0.0069,  0.0473, -0.0015,\n",
       "                      -0.0125,  0.0056, -0.0154, -0.0173, -0.0138, -0.0007,  0.0373,  0.0393,\n",
       "                       0.0168, -0.0270, -0.0009,  0.0153, -0.0106,  0.0330, -0.0169,  0.0075,\n",
       "                       0.0085,  0.0278,  0.0238,  0.0375,  0.0453,  0.0116,  0.0422,  0.0388,\n",
       "                      -0.0182, -0.0315, -0.0422, -0.0247,  0.0425, -0.0349, -0.0203,  0.0045])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.0497, -0.0118, -0.0232,  ...,  0.0186,  0.0035, -0.0337],\n",
       "                      [-0.0719, -0.0198, -0.0169,  ...,  0.0061,  0.0067, -0.0116],\n",
       "                      [ 0.0338,  0.0123,  0.0228,  ...,  0.0143, -0.0137, -0.0174],\n",
       "                      ...,\n",
       "                      [-0.0033, -0.0437, -0.0401,  ..., -0.0301,  0.0496,  0.0760],\n",
       "                      [ 0.0179, -0.0015,  0.0091,  ..., -0.0423,  0.0080,  0.0417],\n",
       "                      [ 0.0623, -0.0405, -0.0067,  ..., -0.0056,  0.0554,  0.0636]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0007,  0.0151, -0.0863,  0.0081, -0.1013,  0.1416, -0.0108,  0.0880,\n",
       "                      -0.0713, -0.0129]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: linear_relu_stack.0.weight\n",
      "Parameter value: tensor([[-0.0066,  0.0215,  0.0193,  ...,  0.0235,  0.0274, -0.0279],\n",
      "        [-0.0350, -0.0250, -0.0346,  ..., -0.0334,  0.0346, -0.0344],\n",
      "        [-0.0188, -0.0136,  0.0166,  ...,  0.0122,  0.0054,  0.0343],\n",
      "        ...,\n",
      "        [-0.0079,  0.0078,  0.0078,  ..., -0.0295,  0.0314,  0.0287],\n",
      "        [-0.0233,  0.0336,  0.0211,  ..., -0.0098,  0.0105,  0.0038],\n",
      "        [-0.0138, -0.0130,  0.0253,  ...,  0.0225,  0.0158,  0.0037]])\n",
      "Parameter shape: torch.Size([512, 784])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.0.bias\n",
      "Parameter value: tensor([ 0.0294,  0.0148,  0.0076, -0.0061,  0.0032,  0.0023, -0.0267,  0.0253,\n",
      "         0.0117, -0.0197,  0.0110, -0.0230, -0.0116,  0.0078, -0.0291, -0.0293,\n",
      "         0.0187,  0.0368, -0.0078,  0.0149, -0.0291, -0.0277,  0.0263, -0.0027,\n",
      "         0.0327,  0.0137, -0.0033,  0.0350,  0.0359,  0.0151,  0.0226, -0.0191,\n",
      "         0.0154,  0.0027,  0.0309,  0.0260,  0.0124,  0.0073, -0.0041,  0.0293,\n",
      "         0.0156,  0.0199,  0.0206, -0.0115,  0.0074, -0.0291,  0.0312,  0.0119,\n",
      "         0.0263, -0.0178, -0.0193, -0.0271,  0.0346,  0.0315, -0.0332,  0.0379,\n",
      "        -0.0312, -0.0110, -0.0231,  0.0036, -0.0379, -0.0207,  0.0305,  0.0285,\n",
      "         0.0437, -0.0161,  0.0275,  0.0166, -0.0242, -0.0295, -0.0118, -0.0025,\n",
      "         0.0279,  0.0146,  0.0135, -0.0148, -0.0286, -0.0274, -0.0232,  0.0093,\n",
      "        -0.0057, -0.0093, -0.0225,  0.0229,  0.0035, -0.0166,  0.0031,  0.0089,\n",
      "         0.0248,  0.0212,  0.0318, -0.0127,  0.0187,  0.0328,  0.0008, -0.0186,\n",
      "        -0.0223, -0.0238,  0.0233, -0.0213, -0.0145,  0.0074,  0.0277,  0.0275,\n",
      "         0.0115,  0.0221, -0.0155,  0.0341,  0.0147,  0.0215,  0.0172,  0.0119,\n",
      "        -0.0209, -0.0014,  0.0059, -0.0147,  0.0118,  0.0108,  0.0269, -0.0213,\n",
      "         0.0271,  0.0214, -0.0284, -0.0186, -0.0021, -0.0314, -0.0149,  0.0009,\n",
      "         0.0177, -0.0227, -0.0081, -0.0350,  0.0062,  0.0079, -0.0022, -0.0270,\n",
      "         0.0150, -0.0223, -0.0029,  0.0058,  0.0330, -0.0121,  0.0293, -0.0249,\n",
      "        -0.0216,  0.0106,  0.0137,  0.0134, -0.0329,  0.0054,  0.0476,  0.0223,\n",
      "         0.0127,  0.0367, -0.0383, -0.0305,  0.0321, -0.0264,  0.0133,  0.0270,\n",
      "         0.0189,  0.0265,  0.0159, -0.0320,  0.0077,  0.0180,  0.0152,  0.0039,\n",
      "        -0.0165, -0.0050, -0.0175, -0.0105,  0.0212,  0.0009, -0.0183,  0.0229,\n",
      "        -0.0127, -0.0024,  0.0016, -0.0150, -0.0056,  0.0314,  0.0057, -0.0018,\n",
      "        -0.0007,  0.0079,  0.0204,  0.0330,  0.0238,  0.0062, -0.0277, -0.0155,\n",
      "         0.0133, -0.0310,  0.0151, -0.0015,  0.0035, -0.0189,  0.0187,  0.0213,\n",
      "         0.0258, -0.0032, -0.0313,  0.0070, -0.0052, -0.0025,  0.0069,  0.0098,\n",
      "        -0.0102,  0.0120,  0.0169,  0.0182,  0.0260, -0.0176,  0.0101,  0.0158,\n",
      "         0.0079,  0.0409, -0.0252,  0.0332, -0.0247, -0.0158,  0.0188,  0.0316,\n",
      "         0.0071,  0.0137, -0.0134, -0.0341,  0.0377, -0.0119,  0.0312, -0.0154,\n",
      "         0.0319,  0.0270, -0.0182, -0.0253, -0.0098,  0.0244, -0.0249, -0.0070,\n",
      "         0.0017,  0.0394, -0.0257,  0.0095,  0.0003,  0.0002,  0.0073,  0.0043,\n",
      "        -0.0017,  0.0335,  0.0023, -0.0082, -0.0264,  0.0329,  0.0182, -0.0201,\n",
      "        -0.0037,  0.0382, -0.0026,  0.0286, -0.0066, -0.0159, -0.0027,  0.0063,\n",
      "        -0.0048, -0.0208,  0.0227,  0.0168, -0.0252,  0.0194, -0.0147, -0.0005,\n",
      "        -0.0050, -0.0281, -0.0179,  0.0126, -0.0152,  0.0273,  0.0205,  0.0142,\n",
      "         0.0162, -0.0072, -0.0161,  0.0169, -0.0017, -0.0063,  0.0135, -0.0122,\n",
      "        -0.0190, -0.0039,  0.0387, -0.0026,  0.0020,  0.0280,  0.0184, -0.0304,\n",
      "        -0.0218, -0.0004, -0.0012,  0.0263,  0.0328, -0.0024,  0.0182,  0.0059,\n",
      "        -0.0021,  0.0254, -0.0119, -0.0249, -0.0198, -0.0337,  0.0033, -0.0175,\n",
      "        -0.0108,  0.0145, -0.0218,  0.0129, -0.0238, -0.0116, -0.0147, -0.0203,\n",
      "         0.0128,  0.0091, -0.0260, -0.0215, -0.0140,  0.0207,  0.0100,  0.0287,\n",
      "         0.0131, -0.0141,  0.0228,  0.0089,  0.0243,  0.0349, -0.0342,  0.0255,\n",
      "        -0.0011,  0.0148,  0.0023, -0.0242,  0.0179, -0.0218,  0.0095, -0.0171,\n",
      "         0.0272,  0.0070,  0.0182, -0.0166,  0.0267,  0.0294, -0.0272,  0.0242,\n",
      "         0.0118,  0.0201, -0.0033,  0.0294, -0.0079,  0.0392,  0.0044,  0.0218,\n",
      "         0.0288, -0.0306, -0.0208,  0.0043, -0.0274, -0.0268, -0.0183, -0.0011,\n",
      "         0.0192,  0.0355,  0.0147, -0.0140,  0.0244,  0.0022, -0.0265,  0.0037,\n",
      "         0.0215,  0.0001, -0.0344,  0.0400,  0.0173,  0.0178,  0.0169,  0.0103,\n",
      "         0.0218, -0.0210,  0.0297,  0.0347, -0.0235, -0.0293,  0.0274, -0.0005,\n",
      "        -0.0278,  0.0047,  0.0027,  0.0147,  0.0334,  0.0171,  0.0252, -0.0080,\n",
      "        -0.0257, -0.0259, -0.0198, -0.0351,  0.0196, -0.0024,  0.0153,  0.0029,\n",
      "         0.0162,  0.0385, -0.0020,  0.0096, -0.0208, -0.0084, -0.0296,  0.0232,\n",
      "        -0.0340,  0.0266, -0.0235,  0.0189, -0.0363, -0.0163,  0.0204, -0.0110,\n",
      "        -0.0202,  0.0256,  0.0383, -0.0019, -0.0061, -0.0073, -0.0123, -0.0173,\n",
      "        -0.0238,  0.0154, -0.0089, -0.0266, -0.0152, -0.0119, -0.0308,  0.0171,\n",
      "         0.0227, -0.0141,  0.0200, -0.0126,  0.0084,  0.0026, -0.0034, -0.0220,\n",
      "         0.0244, -0.0040,  0.0446, -0.0120, -0.0137,  0.0101,  0.0289,  0.0218,\n",
      "        -0.0173, -0.0097,  0.0099,  0.0059, -0.0310,  0.0150, -0.0184,  0.0298,\n",
      "        -0.0278, -0.0235, -0.0231,  0.0280,  0.0108, -0.0168,  0.0298, -0.0036,\n",
      "        -0.0266,  0.0053,  0.0229,  0.0124, -0.0169,  0.0099,  0.0308,  0.0340,\n",
      "        -0.0142, -0.0214,  0.0087, -0.0186, -0.0168,  0.0280,  0.0301,  0.0278,\n",
      "        -0.0101, -0.0197, -0.0104, -0.0365, -0.0083,  0.0071, -0.0190,  0.0109,\n",
      "        -0.0259, -0.0303,  0.0210,  0.0042, -0.0004, -0.0206,  0.0141,  0.0202,\n",
      "         0.0042,  0.0257,  0.0176,  0.0268, -0.0123, -0.0329,  0.0076, -0.0237])\n",
      "Parameter shape: torch.Size([512])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.weight\n",
      "Parameter value: tensor([[ 0.0074,  0.0169,  0.0010,  ...,  0.0047, -0.0277, -0.0420],\n",
      "        [-0.0239, -0.0320,  0.0357,  ..., -0.0366,  0.0321,  0.0177],\n",
      "        [-0.0295,  0.0372, -0.0088,  ..., -0.0418,  0.0187,  0.0428],\n",
      "        ...,\n",
      "        [-0.0146,  0.0213, -0.0117,  ...,  0.0052,  0.0115,  0.0108],\n",
      "        [ 0.0018,  0.0293,  0.0036,  ..., -0.0256, -0.0268,  0.0457],\n",
      "        [ 0.0424, -0.0099, -0.0012,  ..., -0.0157,  0.0424, -0.0170]])\n",
      "Parameter shape: torch.Size([512, 512])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.bias\n",
      "Parameter value: tensor([-0.0117, -0.0302, -0.0154,  0.0354, -0.0179,  0.0005, -0.0295,  0.0341,\n",
      "        -0.0055,  0.0134, -0.0277,  0.0423, -0.0398, -0.0096, -0.0123,  0.0387,\n",
      "         0.0026, -0.0276, -0.0349, -0.0382,  0.0364, -0.0120,  0.0011, -0.0239,\n",
      "        -0.0225,  0.0042,  0.0293,  0.0332,  0.0409,  0.0429, -0.0232, -0.0057,\n",
      "         0.0165, -0.0234,  0.0064,  0.0258, -0.0007,  0.0010, -0.0104, -0.0309,\n",
      "         0.0109, -0.0179, -0.0113,  0.0155,  0.0299, -0.0431,  0.0134,  0.0440,\n",
      "        -0.0410, -0.0236, -0.0118,  0.0173, -0.0402,  0.0531, -0.0154,  0.0227,\n",
      "         0.0301, -0.0323,  0.0448,  0.0130, -0.0372,  0.0188, -0.0050, -0.0193,\n",
      "         0.0061,  0.0278, -0.0218, -0.0185,  0.0113, -0.0183, -0.0231,  0.0325,\n",
      "        -0.0115, -0.0123, -0.0400,  0.0604, -0.0041,  0.0041,  0.0419,  0.0418,\n",
      "        -0.0079, -0.0121,  0.0257,  0.0293,  0.0059,  0.0479,  0.0057, -0.0271,\n",
      "         0.0047, -0.0387,  0.0122,  0.0396, -0.0371, -0.0081,  0.0194, -0.0101,\n",
      "        -0.0389, -0.0143, -0.0037, -0.0307,  0.0267, -0.0063,  0.0113,  0.0338,\n",
      "         0.0218, -0.0239, -0.0341,  0.0454,  0.0139,  0.0249,  0.0287, -0.0094,\n",
      "         0.0326,  0.0419, -0.0366,  0.0436,  0.0260,  0.0100,  0.0210,  0.0151,\n",
      "        -0.0074,  0.0305,  0.0331, -0.0464,  0.0447,  0.0262, -0.0252, -0.0401,\n",
      "         0.0009, -0.0099, -0.0042, -0.0434,  0.0403, -0.0104, -0.0173, -0.0106,\n",
      "         0.0110, -0.0277, -0.0340,  0.0038, -0.0119,  0.0416, -0.0164, -0.0451,\n",
      "         0.0481, -0.0260, -0.0243,  0.0239, -0.0118, -0.0212, -0.0348, -0.0314,\n",
      "        -0.0310,  0.0059,  0.0242,  0.0262, -0.0355,  0.0259, -0.0318, -0.0312,\n",
      "         0.0407,  0.0173, -0.0056,  0.0198,  0.0280,  0.0498,  0.0396,  0.0370,\n",
      "        -0.0236, -0.0137,  0.0156,  0.0401,  0.0087,  0.0394, -0.0261,  0.0227,\n",
      "         0.0265, -0.0080,  0.0317,  0.0188, -0.0380,  0.0290, -0.0391, -0.0181,\n",
      "         0.0403,  0.0451, -0.0079,  0.0095,  0.0160,  0.0279,  0.0015, -0.0409,\n",
      "         0.0190, -0.0144, -0.0145, -0.0287,  0.0036,  0.0495,  0.0210, -0.0273,\n",
      "        -0.0014, -0.0184, -0.0476,  0.0598, -0.0195, -0.0152,  0.0538, -0.0310,\n",
      "         0.0033, -0.0402, -0.0283,  0.0398, -0.0128, -0.0400,  0.0355,  0.0473,\n",
      "         0.0228,  0.0144, -0.0203,  0.0257, -0.0393, -0.0067, -0.0020, -0.0156,\n",
      "        -0.0261,  0.0073,  0.0287,  0.0181, -0.0319,  0.0003,  0.0218, -0.0050,\n",
      "        -0.0002,  0.0444,  0.0037,  0.0408,  0.0208, -0.0093,  0.0062,  0.0160,\n",
      "         0.0468, -0.0053,  0.0309,  0.0447,  0.0369,  0.0308,  0.0358,  0.0291,\n",
      "        -0.0178,  0.0389,  0.0038,  0.0322, -0.0264, -0.0014,  0.0372,  0.0232,\n",
      "        -0.0120, -0.0047, -0.0215,  0.0391,  0.0130,  0.0142, -0.0015, -0.0230,\n",
      "        -0.0116, -0.0024, -0.0142, -0.0267, -0.0377,  0.0104, -0.0354, -0.0290,\n",
      "         0.0094, -0.0115,  0.0254,  0.0125, -0.0254,  0.0063,  0.0406,  0.0064,\n",
      "         0.0196, -0.0018,  0.0073,  0.0106,  0.0044,  0.0196,  0.0177,  0.0214,\n",
      "        -0.0351, -0.0418, -0.0060, -0.0205,  0.0320,  0.0388,  0.0400,  0.0335,\n",
      "        -0.0029,  0.0024, -0.0184,  0.0082,  0.0263, -0.0048, -0.0332, -0.0274,\n",
      "        -0.0227,  0.0292, -0.0050, -0.0032,  0.0074, -0.0272, -0.0352, -0.0075,\n",
      "        -0.0202, -0.0014, -0.0379,  0.0261, -0.0079,  0.0196, -0.0253,  0.0197,\n",
      "        -0.0137, -0.0291,  0.0085,  0.0042, -0.0078, -0.0231, -0.0427, -0.0229,\n",
      "        -0.0261, -0.0433,  0.0462,  0.0141, -0.0146,  0.0385, -0.0175, -0.0222,\n",
      "         0.0213,  0.0508,  0.0341, -0.0014, -0.0172, -0.0402,  0.0576, -0.0325,\n",
      "        -0.0112, -0.0208,  0.0191, -0.0193,  0.0189,  0.0137,  0.0162, -0.0165,\n",
      "         0.0073,  0.0346, -0.0343,  0.0305, -0.0008, -0.0126,  0.0004,  0.0384,\n",
      "         0.0095, -0.0074, -0.0384,  0.0201,  0.0316,  0.0218, -0.0422,  0.0197,\n",
      "        -0.0131, -0.0241,  0.0126,  0.0170, -0.0308,  0.0380, -0.0275, -0.0049,\n",
      "        -0.0402, -0.0364, -0.0215,  0.0205,  0.0055,  0.0083, -0.0153, -0.0072,\n",
      "         0.0308, -0.0240, -0.0350,  0.0085,  0.0157,  0.0191, -0.0414, -0.0405,\n",
      "         0.0129,  0.0401,  0.0360, -0.0085,  0.0231, -0.0396,  0.0487, -0.0098,\n",
      "         0.0366, -0.0461,  0.0286,  0.0510,  0.0040, -0.0288,  0.0239,  0.0495,\n",
      "         0.0290,  0.0265,  0.0102,  0.0384,  0.0350, -0.0197, -0.0026,  0.0263,\n",
      "        -0.0046,  0.0399, -0.0094, -0.0306,  0.0241,  0.0353, -0.0352, -0.0366,\n",
      "        -0.0214,  0.0442,  0.0186, -0.0094,  0.0181, -0.0235,  0.0501,  0.0096,\n",
      "        -0.0403,  0.0141,  0.0103,  0.0223,  0.0224, -0.0330, -0.0172,  0.0294,\n",
      "        -0.0054,  0.0259, -0.0216,  0.0450, -0.0217,  0.0200, -0.0226,  0.0241,\n",
      "         0.0115,  0.0086,  0.0230,  0.0336,  0.0292, -0.0297,  0.0033,  0.0328,\n",
      "        -0.0396, -0.0024,  0.0027,  0.0259,  0.0042,  0.0269,  0.0278, -0.0234,\n",
      "         0.0292, -0.0208,  0.0278,  0.0290,  0.0093,  0.0285, -0.0222, -0.0325,\n",
      "        -0.0380, -0.0189,  0.0219,  0.0224, -0.0037, -0.0069,  0.0473, -0.0015,\n",
      "        -0.0125,  0.0056, -0.0154, -0.0173, -0.0138, -0.0007,  0.0373,  0.0393,\n",
      "         0.0168, -0.0270, -0.0009,  0.0153, -0.0106,  0.0330, -0.0169,  0.0075,\n",
      "         0.0085,  0.0278,  0.0238,  0.0375,  0.0453,  0.0116,  0.0422,  0.0388,\n",
      "        -0.0182, -0.0315, -0.0422, -0.0247,  0.0425, -0.0349, -0.0203,  0.0045])\n",
      "Parameter shape: torch.Size([512])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.weight\n",
      "Parameter value: tensor([[ 0.0497, -0.0118, -0.0232,  ...,  0.0186,  0.0035, -0.0337],\n",
      "        [-0.0719, -0.0198, -0.0169,  ...,  0.0061,  0.0067, -0.0116],\n",
      "        [ 0.0338,  0.0123,  0.0228,  ...,  0.0143, -0.0137, -0.0174],\n",
      "        ...,\n",
      "        [-0.0033, -0.0437, -0.0401,  ..., -0.0301,  0.0496,  0.0760],\n",
      "        [ 0.0179, -0.0015,  0.0091,  ..., -0.0423,  0.0080,  0.0417],\n",
      "        [ 0.0623, -0.0405, -0.0067,  ..., -0.0056,  0.0554,  0.0636]])\n",
      "Parameter shape: torch.Size([10, 512])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.bias\n",
      "Parameter value: tensor([-0.0007,  0.0151, -0.0863,  0.0081, -0.1013,  0.1416, -0.0108,  0.0880,\n",
      "        -0.0713, -0.0129])\n",
      "Parameter shape: torch.Size([10])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print state_dict \n",
    "for name, param in model.state_dict().items():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter value: {param}\")\n",
    "    print(f\"Parameter shape: {param.shape}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Weights of fc1 layer: Parameter containing:\n",
      "tensor([[ 0.0179, -0.0142,  0.0200,  ...,  0.0028,  0.0268,  0.0126],\n",
      "        [-0.0270, -0.0023,  0.0217,  ...,  0.0134, -0.0088, -0.0113],\n",
      "        [-0.0118,  0.0246,  0.0357,  ..., -0.0130, -0.0279,  0.0042],\n",
      "        ...,\n",
      "        [-0.0064,  0.0213, -0.0277,  ..., -0.0039, -0.0039, -0.0044],\n",
      "        [ 0.0053, -0.0283, -0.0170,  ..., -0.0213,  0.0301,  0.0211],\n",
      "        [-0.0285,  0.0148,  0.0307,  ..., -0.0167,  0.0261,  0.0092]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Bias of fc1 layer: Parameter containing:\n",
      "tensor([ 3.0576e-02, -3.0341e-02,  3.1664e-02,  2.8869e-02,  2.9242e-02,\n",
      "         1.4737e-02, -3.4691e-02, -1.1108e-03,  1.5957e-02,  2.7175e-02,\n",
      "        -4.0652e-03,  3.0553e-02,  2.1223e-02,  3.1449e-02,  1.4513e-02,\n",
      "         1.4202e-02,  8.9528e-03, -5.7967e-04, -3.3975e-02,  3.2626e-02,\n",
      "        -1.4226e-02,  1.5918e-02, -1.3603e-02, -8.6804e-03,  2.1430e-02,\n",
      "        -3.1592e-02, -2.5172e-02, -2.4321e-02, -2.4212e-02, -7.5413e-04,\n",
      "         3.1398e-02,  2.4637e-02, -2.7261e-02, -2.6431e-02,  1.0652e-02,\n",
      "         3.2835e-03, -2.8127e-02,  2.4182e-02,  3.1540e-02, -1.2013e-02,\n",
      "         3.0864e-02, -6.2379e-04, -3.0430e-02, -1.0153e-02,  4.9206e-03,\n",
      "         2.9924e-02,  1.2759e-02,  1.6326e-02,  1.7901e-02, -4.7155e-03,\n",
      "         1.1212e-02,  2.0235e-02, -3.2460e-03,  1.0168e-02, -1.4174e-02,\n",
      "         1.5589e-02, -3.9182e-03,  2.4154e-02, -4.3243e-03, -2.2196e-02,\n",
      "         1.4422e-02,  2.4365e-02, -3.2240e-02,  3.3280e-03, -1.7596e-02,\n",
      "        -1.6617e-02, -2.1824e-02,  2.3486e-02,  2.0105e-02, -2.0184e-02,\n",
      "        -2.8478e-02,  5.8904e-03,  2.1034e-02, -3.2900e-02,  2.2918e-02,\n",
      "        -3.3067e-02,  3.2183e-02,  1.4971e-02, -3.3936e-02, -4.3074e-03,\n",
      "         9.3020e-03, -2.9845e-02,  3.1318e-02,  1.4178e-03, -2.9582e-02,\n",
      "        -2.1328e-02,  3.1898e-02, -1.4149e-02, -1.4367e-03, -8.2727e-03,\n",
      "         5.8460e-03, -1.0990e-02,  1.1131e-02,  2.5884e-02,  2.6703e-02,\n",
      "         1.8620e-02,  1.9524e-02, -2.6062e-02, -1.6857e-02, -4.2987e-03,\n",
      "        -1.1551e-02,  1.9835e-02,  5.8776e-03, -3.1590e-02, -2.7447e-02,\n",
      "        -3.0913e-02, -1.9283e-02,  2.6361e-02, -1.6478e-02, -2.1048e-02,\n",
      "         2.5796e-02, -1.5635e-02,  2.3799e-02,  2.8621e-02,  3.1075e-02,\n",
      "        -1.6764e-04,  3.5530e-02, -2.8241e-02, -4.9866e-03,  2.0735e-02,\n",
      "         2.8566e-02,  3.0881e-02,  3.0544e-02,  7.7107e-04,  1.4397e-02,\n",
      "        -4.5158e-03,  3.1040e-02, -1.3749e-02,  2.9490e-02,  2.4130e-02,\n",
      "        -9.4003e-03, -2.3377e-02, -3.2969e-02, -2.9777e-02, -2.4080e-02,\n",
      "        -3.1970e-02,  2.1156e-02, -1.0770e-02, -2.9281e-02,  1.6281e-02,\n",
      "         2.9519e-02,  3.4474e-02,  2.4375e-02,  5.2730e-03, -2.3683e-02,\n",
      "         3.4285e-02, -1.3528e-02,  2.6242e-02,  3.3775e-02, -2.2326e-02,\n",
      "        -4.9846e-03,  6.0552e-03, -2.8789e-02,  3.5276e-02,  2.1292e-02,\n",
      "        -9.9756e-03,  3.1562e-02,  1.0735e-02, -2.6591e-02,  2.7700e-02,\n",
      "        -7.6097e-03, -5.4039e-03,  2.1186e-02,  2.0759e-02,  4.2432e-03,\n",
      "        -3.4532e-02, -7.3754e-03, -3.0102e-02, -1.6745e-02,  2.1410e-02,\n",
      "         3.2595e-02,  3.4566e-02,  2.8826e-02, -2.5940e-02, -3.3482e-02,\n",
      "         8.7665e-03, -9.5408e-04,  1.1195e-02, -1.1963e-02, -2.5877e-02,\n",
      "         3.5257e-03, -2.9115e-02, -2.6015e-02,  1.7791e-02,  2.0731e-02,\n",
      "         4.2316e-03, -1.7653e-02, -2.9556e-02, -2.5605e-02,  2.2209e-02,\n",
      "         2.0975e-02,  9.2475e-03, -1.9960e-02,  2.0220e-03,  1.6572e-03,\n",
      "         1.7326e-02, -2.2923e-02, -1.1319e-02,  1.7861e-02, -1.9551e-02,\n",
      "         8.1606e-03, -1.8729e-02, -2.6611e-02,  3.2169e-02, -1.4892e-02,\n",
      "         2.0143e-02, -1.2533e-02,  1.6956e-02, -2.0623e-02, -1.3692e-02,\n",
      "        -7.5316e-03, -6.7231e-03,  1.0176e-02,  3.5004e-02, -5.2875e-03,\n",
      "         2.9227e-02, -6.5234e-03,  2.1985e-02, -3.4545e-02,  7.4384e-03,\n",
      "         1.2752e-02, -1.1260e-02, -3.0955e-02,  3.5079e-02, -2.6747e-02,\n",
      "        -3.3459e-03,  2.8224e-02,  1.7952e-02,  2.4354e-02,  1.4549e-02,\n",
      "        -3.2841e-02,  6.4004e-03, -2.5843e-02, -2.2980e-04,  8.7442e-03,\n",
      "         2.9199e-02, -2.6867e-02, -2.0157e-02, -2.8665e-02,  8.2425e-03,\n",
      "         1.0988e-03, -1.1643e-02, -6.8242e-03, -5.1934e-04,  3.5374e-02,\n",
      "         2.8831e-02, -5.3723e-03, -3.4759e-02, -1.6027e-02, -1.1646e-02,\n",
      "         9.4233e-03, -2.2822e-02, -2.1757e-03, -2.6745e-03, -1.9167e-02,\n",
      "        -2.2629e-02, -4.0688e-03, -1.7695e-02, -3.4099e-02,  2.1236e-02,\n",
      "        -1.4006e-02, -1.5245e-02, -3.1558e-02, -1.9741e-02,  1.4492e-02,\n",
      "         1.6773e-02, -3.0129e-02, -1.5181e-02,  4.3325e-03, -2.5809e-03,\n",
      "        -1.3049e-03,  2.6002e-02, -1.6536e-02, -1.6197e-02,  3.5149e-02,\n",
      "         3.2978e-02,  9.9274e-03, -3.2144e-02,  2.5254e-02,  2.3643e-02,\n",
      "         2.9080e-02, -2.9509e-02,  2.7291e-02,  1.1013e-02, -9.6671e-03,\n",
      "        -2.7249e-02, -1.4295e-02, -1.7960e-02, -2.5011e-02, -2.1847e-02,\n",
      "         1.7655e-02, -1.9766e-02, -3.0804e-03, -1.0246e-02,  3.3109e-02,\n",
      "         2.9723e-03,  1.5676e-02,  2.1882e-02, -1.2577e-02,  3.4731e-02,\n",
      "        -3.1550e-02, -7.6750e-03,  1.6486e-02,  2.6873e-02, -3.2173e-02,\n",
      "         2.0128e-02,  1.6345e-02,  1.5509e-02, -2.7098e-02, -2.0058e-02,\n",
      "        -4.6512e-03, -7.6336e-04, -1.3206e-03,  2.6532e-02, -8.1774e-03,\n",
      "        -1.5553e-02, -9.2096e-03,  2.3278e-02, -1.1019e-02,  1.4293e-02,\n",
      "         1.4541e-03,  1.0634e-02,  1.7376e-02, -1.6983e-02,  1.9127e-02,\n",
      "        -3.3624e-02,  2.0221e-02,  4.9773e-03, -3.8331e-04, -1.5782e-02,\n",
      "        -3.9744e-03,  2.4863e-02, -2.9669e-02, -7.8217e-03,  1.1144e-02,\n",
      "         8.0592e-03,  2.6369e-02, -3.1325e-03,  3.1648e-02,  1.0410e-02,\n",
      "         2.3333e-02,  6.3966e-03,  1.0602e-02,  2.5136e-02,  2.9454e-02,\n",
      "         5.1594e-03, -2.5050e-02, -1.4055e-02,  2.4921e-02,  3.4136e-02,\n",
      "        -2.4377e-02,  3.1466e-02, -1.9676e-02,  2.0374e-02, -3.3736e-03,\n",
      "         2.6117e-02, -3.1323e-03,  2.0774e-02, -1.3442e-02, -1.5603e-02,\n",
      "        -2.9156e-02,  8.3740e-03, -6.6040e-03,  2.0178e-02, -1.4051e-03,\n",
      "        -3.1625e-02, -2.3575e-02,  1.6872e-02,  2.5140e-02,  3.2090e-02,\n",
      "        -2.8982e-02, -4.5405e-03, -1.5133e-02,  1.5471e-02, -1.8622e-02,\n",
      "        -2.4512e-02, -5.3219e-03,  3.5359e-02, -3.0703e-03,  2.8105e-03,\n",
      "         1.1808e-03, -3.0978e-02, -2.8202e-02,  1.1242e-02, -2.1732e-02,\n",
      "         8.0987e-03, -1.4547e-02,  1.2269e-02,  2.5817e-02,  1.0910e-02,\n",
      "        -2.6046e-02,  3.2522e-02,  1.5491e-02,  9.9975e-03, -2.1079e-02,\n",
      "        -6.9636e-03, -1.3862e-02, -2.9622e-02,  2.8317e-03, -1.5794e-03,\n",
      "         7.1371e-03,  8.3006e-03, -1.1839e-02,  3.2415e-02,  3.2080e-02,\n",
      "        -9.2307e-03,  1.1959e-02,  7.3553e-03, -2.5259e-02, -3.2041e-02,\n",
      "         1.9456e-02, -6.5300e-03,  1.4821e-02,  3.2368e-02,  3.1728e-02,\n",
      "         1.5147e-02, -2.5273e-02,  2.7536e-02,  1.5573e-02, -1.1910e-02,\n",
      "         1.2470e-02, -2.4170e-02, -1.9401e-02,  1.4213e-02, -2.1068e-02,\n",
      "        -2.2649e-02, -1.3482e-02, -3.3749e-02,  2.0248e-03, -1.6452e-02,\n",
      "        -3.1314e-02,  8.3688e-03, -3.2496e-02,  1.0336e-02,  1.5590e-02,\n",
      "         2.9462e-02,  2.3084e-02,  1.8208e-02,  2.6780e-02,  2.6990e-02,\n",
      "         1.5081e-02,  1.8247e-02, -2.3126e-02,  2.6770e-02, -1.6655e-02,\n",
      "         2.1410e-02, -5.1797e-03, -1.7825e-02,  2.6286e-03,  3.4753e-02,\n",
      "         7.7262e-03, -3.5059e-02, -3.4380e-02, -5.3065e-03, -7.2581e-03,\n",
      "        -3.2983e-05,  1.0484e-02, -7.0026e-03,  1.6929e-02,  3.2610e-02,\n",
      "         2.9411e-02,  3.0109e-02,  4.5255e-03,  2.1484e-02, -2.6843e-02,\n",
      "        -3.2718e-02, -8.1190e-03, -1.1040e-02, -3.8717e-03,  6.4809e-03,\n",
      "        -3.2328e-02,  2.5329e-02, -1.0642e-02,  2.5890e-02, -1.8438e-02,\n",
      "         1.9203e-02, -1.1515e-02, -1.8189e-03,  1.7100e-02,  1.2579e-02,\n",
      "        -2.3561e-02, -2.3234e-03, -2.0231e-02,  1.1861e-02, -1.4284e-02,\n",
      "        -2.8186e-02,  1.8135e-02,  1.6351e-02, -1.2975e-02, -2.7347e-02,\n",
      "         1.2682e-02, -1.1882e-02, -3.2685e-02, -1.2264e-02, -8.8773e-03,\n",
      "         2.0831e-02, -2.3315e-02,  2.6884e-02, -5.2501e-04, -2.0095e-02,\n",
      "        -1.4653e-02,  1.4002e-02, -2.1491e-02, -1.8610e-02, -1.5981e-02,\n",
      "         1.8156e-02,  1.6457e-02,  3.2034e-02,  2.8755e-02, -2.9702e-03,\n",
      "        -2.5832e-02,  1.8725e-02], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Access the weights and bias of the first layer\n",
    "\n",
    "print(model.linear_relu_stack)\n",
    "\n",
    "fc1_weights = model.linear_relu_stack[0].weight\n",
    "fc1_bias = model.linear_relu_stack[0].bias\n",
    "\n",
    "print(\"Weights of fc1 layer:\", fc1_weights)\n",
    "print(\"Bias of fc1 layer:\", fc1_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 932362\n",
      "Trainable parameters: 932362\n"
     ]
    }
   ],
   "source": [
    "# print the total number of parameters and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g0/phgwvz7x3sz18gfzz4zv9rq40000gn/T/ipykernel_57359/3466235813.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model state_dict\n",
    "# only the model's state_dict is saved\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "# load model state_dict\n",
    "# model must be created before loading the state_dict\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "model2 = NN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x3112861f0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_state_dict(torch.load(\"model.pth\"))\n",
    "# model2 is now loaded with the saved state_dict, but it is not trained\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
