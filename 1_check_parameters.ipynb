{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" # it is mps which is gpu but not cuda for mac\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module): # nn.Module is base class for all models in pytorch\n",
    "    def __init__(self): # constructor of class\n",
    "        super().__init__() # super() is used to call the constructor of parent class\n",
    "        self.flatten  = nn.Flatten() # flatten layer\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "# Create an instance of the model, including the device it is on.\n",
    "model = NN().to(device) # to() is used to move the model to the device\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # get the size of the dataset\n",
    "    model.train() # set the model to training mode\n",
    "    for batch, (X, y) in enumerate(dataloader): # iterate over the data, batch is the index of the batch, X is the input and y is the output\n",
    "        X, y = X.to(device), y.to(device) # move the data to the device\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        loss.backward() # compute the gradient of the loss with respect to the model parameters\n",
    "        optimizer.step() # adjust the parameters by the gradients collected in the backward pass\n",
    "        optimizer.zero_grad() # reset the gradients to zero\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X) \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # disable gradient computation\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() # sum up the loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # get the index of the max log-probability\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305561  [   64/60000]\n",
      "loss: 2.289518  [ 6464/60000]\n",
      "loss: 2.270289  [12864/60000]\n",
      "loss: 2.263437  [19264/60000]\n",
      "loss: 2.246391  [25664/60000]\n",
      "loss: 2.215353  [32064/60000]\n",
      "loss: 2.225014  [38464/60000]\n",
      "loss: 2.188428  [44864/60000]\n",
      "loss: 2.180696  [51264/60000]\n",
      "loss: 2.147310  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 2.141722 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.155007  [   64/60000]\n",
      "loss: 2.141639  [ 6464/60000]\n",
      "loss: 2.078445  [12864/60000]\n",
      "loss: 2.096678  [19264/60000]\n",
      "loss: 2.038934  [25664/60000]\n",
      "loss: 1.970310  [32064/60000]\n",
      "loss: 2.010129  [38464/60000]\n",
      "loss: 1.921651  [44864/60000]\n",
      "loss: 1.928320  [51264/60000]\n",
      "loss: 1.852824  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.853042 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.890126  [   64/60000]\n",
      "loss: 1.856437  [ 6464/60000]\n",
      "loss: 1.739494  [12864/60000]\n",
      "loss: 1.787724  [19264/60000]\n",
      "loss: 1.668804  [25664/60000]\n",
      "loss: 1.621364  [32064/60000]\n",
      "loss: 1.659166  [38464/60000]\n",
      "loss: 1.562853  [44864/60000]\n",
      "loss: 1.592436  [51264/60000]\n",
      "loss: 1.487021  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.506145 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.578145  [   64/60000]\n",
      "loss: 1.538575  [ 6464/60000]\n",
      "loss: 1.398337  [12864/60000]\n",
      "loss: 1.472067  [19264/60000]\n",
      "loss: 1.349830  [25664/60000]\n",
      "loss: 1.348097  [32064/60000]\n",
      "loss: 1.368386  [38464/60000]\n",
      "loss: 1.303319  [44864/60000]\n",
      "loss: 1.336321  [51264/60000]\n",
      "loss: 1.235973  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.261304 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.343188  [   64/60000]\n",
      "loss: 1.316641  [ 6464/60000]\n",
      "loss: 1.164089  [12864/60000]\n",
      "loss: 1.265410  [19264/60000]\n",
      "loss: 1.135803  [25664/60000]\n",
      "loss: 1.164123  [32064/60000]\n",
      "loss: 1.184565  [38464/60000]\n",
      "loss: 1.135045  [44864/60000]\n",
      "loss: 1.171387  [51264/60000]\n",
      "loss: 1.083799  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.103377 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5 # number of epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# check whether the model is on the device\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                  [-1, 512]         262,656\n",
      "              ReLU-5                  [-1, 512]               0\n",
      "            Linear-6                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.55\n",
      "Estimated Total Size (MB): 2.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Temporarily move the model to CPU for summary\n",
    "model_cpu = model.to(\"cpu\")\n",
    "# Run summary on the CPU (since torchsummary only supports 'cpu' or 'cuda')\n",
    "summary(model_cpu, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 669706\n",
      "Trainable parameters: 669706\n"
     ]
    }
   ],
   "source": [
    "# print the total number of parameters and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: linear_relu_stack.0.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0111, -0.0163,  0.0044,  ..., -0.0006,  0.0274,  0.0065],\n",
      "        [-0.0118,  0.0021,  0.0163,  ...,  0.0021, -0.0304,  0.0215],\n",
      "        [ 0.0244,  0.0233, -0.0097,  ..., -0.0152,  0.0350, -0.0285],\n",
      "        ...,\n",
      "        [ 0.0047, -0.0119, -0.0056,  ..., -0.0351,  0.0062,  0.0234],\n",
      "        [-0.0012,  0.0035, -0.0316,  ..., -0.0269,  0.0182,  0.0060],\n",
      "        [ 0.0156, -0.0253, -0.0042,  ..., -0.0286, -0.0180, -0.0263]],\n",
      "       requires_grad=True)\n",
      "Parameter shape: torch.Size([512, 784])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.0.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([ 3.1299e-02, -2.4866e-02, -2.4687e-03,  3.3257e-02, -1.2573e-02,\n",
      "        -2.9465e-02,  3.6867e-02, -1.1514e-02, -1.7470e-03,  8.7670e-03,\n",
      "        -2.8088e-02, -2.8737e-02, -3.4039e-02, -1.5097e-02, -3.3807e-02,\n",
      "        -4.1252e-03, -1.3162e-02,  3.1557e-02, -2.3931e-02, -9.9615e-03,\n",
      "         1.5670e-02, -1.0803e-03, -1.2151e-02, -2.0947e-02, -1.8876e-02,\n",
      "         2.2011e-02, -3.3348e-02,  4.8116e-03,  8.6803e-03,  1.7623e-02,\n",
      "         1.6403e-02, -1.2514e-02,  7.9018e-03, -2.8916e-02, -1.0766e-02,\n",
      "        -7.5647e-03, -3.5550e-03, -3.2359e-02,  3.4937e-02, -2.5400e-02,\n",
      "         1.4325e-02,  4.7796e-03, -1.8049e-02,  7.3009e-03, -1.3749e-02,\n",
      "         3.7105e-02,  2.3368e-02,  4.2428e-03,  1.8882e-02, -2.4410e-02,\n",
      "         2.9200e-03,  2.7759e-02, -1.1613e-02, -2.3947e-02, -2.8069e-02,\n",
      "        -5.4490e-03,  1.4343e-02,  9.1595e-03, -7.5708e-03,  1.9226e-02,\n",
      "         8.9526e-03, -3.6449e-03,  3.6030e-02, -1.4603e-02,  3.1513e-02,\n",
      "         1.3502e-02,  2.1687e-02, -1.2270e-02,  1.8124e-02, -2.8407e-02,\n",
      "        -1.9915e-02, -2.2935e-02, -3.6503e-02,  1.1845e-02,  2.3900e-02,\n",
      "        -3.0695e-02, -4.0309e-03, -7.0003e-03, -3.5476e-02,  5.2369e-03,\n",
      "         2.2343e-02,  2.2138e-02, -3.0757e-02, -2.6127e-02,  1.3471e-03,\n",
      "        -5.1610e-03, -1.6784e-02,  1.0939e-02, -3.5012e-02,  5.6542e-03,\n",
      "         7.6482e-03,  2.9593e-02,  3.2525e-02, -3.0391e-02,  2.1811e-02,\n",
      "         7.6460e-03,  1.5949e-02,  1.9918e-02, -1.3567e-02,  1.2244e-02,\n",
      "        -1.9767e-02,  1.3861e-02,  1.7690e-02,  2.1168e-02,  3.2005e-02,\n",
      "         8.1267e-03, -2.6708e-03, -7.3254e-04,  4.2772e-04,  3.4820e-02,\n",
      "         1.0113e-02,  3.2301e-02,  2.1028e-02, -2.8408e-02,  4.1496e-02,\n",
      "        -9.7464e-03,  1.5589e-02, -7.5156e-03,  1.1652e-02, -1.5019e-02,\n",
      "         3.7560e-02,  3.2533e-03,  3.2581e-02,  1.7274e-02,  2.8570e-02,\n",
      "         8.2888e-03,  1.9975e-02, -2.7682e-02,  1.7965e-02, -1.7655e-02,\n",
      "        -2.5013e-02,  3.1347e-02,  3.7203e-02, -3.6390e-02, -1.2477e-02,\n",
      "         2.7538e-02,  3.6468e-02,  1.5251e-03, -3.3562e-02, -1.9580e-02,\n",
      "        -1.7604e-02,  2.5937e-02, -7.0241e-03,  1.3931e-02, -2.0829e-02,\n",
      "         3.0020e-02,  3.4157e-02, -8.3373e-03, -2.4119e-02, -9.1882e-03,\n",
      "        -9.0172e-03, -3.0521e-02,  2.7748e-02, -2.6568e-02,  1.7120e-02,\n",
      "        -9.6135e-03, -3.5436e-02,  1.1635e-02,  3.9039e-02,  3.7904e-02,\n",
      "         2.8246e-02,  1.9714e-02, -2.4613e-02,  2.9571e-02,  1.0809e-02,\n",
      "         2.1170e-02,  2.3050e-02, -3.3391e-02, -1.0468e-02,  1.5682e-03,\n",
      "        -2.5103e-02, -1.4289e-02, -5.3376e-03, -1.1421e-02,  1.4830e-02,\n",
      "         3.9414e-02,  1.8518e-02,  1.5456e-02, -1.1434e-03, -2.0770e-02,\n",
      "        -1.2762e-02, -2.4125e-02,  4.0295e-03,  3.5649e-02,  2.3033e-02,\n",
      "        -1.7503e-02,  1.8628e-02, -2.5089e-02,  7.5775e-03, -2.7324e-02,\n",
      "         4.4273e-03, -2.0846e-02,  3.0332e-02, -2.7007e-02, -1.6700e-02,\n",
      "         2.5385e-02,  3.6013e-02,  3.4034e-02, -1.9578e-02, -9.6259e-03,\n",
      "        -5.6950e-03,  2.0621e-02, -3.3474e-02, -1.1836e-02,  1.8623e-02,\n",
      "         7.8949e-04,  3.1272e-02, -3.2451e-02, -2.0683e-02, -2.0711e-03,\n",
      "        -2.3011e-02, -1.6059e-03, -8.6199e-03,  2.0143e-02, -1.8260e-02,\n",
      "        -1.1082e-04, -4.2506e-03, -1.9791e-02, -2.4167e-02,  6.9828e-03,\n",
      "        -1.3237e-02,  1.7496e-02,  2.1691e-02,  2.1745e-02, -2.9442e-02,\n",
      "         3.2760e-03,  1.2327e-04, -2.7517e-02, -3.3203e-02,  1.7948e-02,\n",
      "        -4.1579e-03,  3.5348e-02,  8.8844e-03, -2.3033e-02, -1.1942e-02,\n",
      "        -6.0597e-03,  3.4291e-02, -3.0517e-02,  2.9601e-02, -2.6078e-02,\n",
      "         1.5574e-02,  1.0520e-02, -1.3986e-02,  1.9278e-03, -2.7844e-02,\n",
      "         2.1302e-02, -2.1205e-02,  2.0413e-02,  2.7193e-02,  4.4633e-02,\n",
      "        -7.0556e-03, -2.3985e-02, -1.8159e-02,  1.2458e-02,  2.5090e-03,\n",
      "        -2.5556e-02,  8.9525e-03,  1.7325e-02, -2.8504e-02,  1.5161e-02,\n",
      "        -1.0499e-02, -8.4576e-03, -1.5134e-02,  1.0953e-02,  3.5435e-02,\n",
      "        -1.5580e-02,  2.9059e-02, -3.0180e-02,  6.7077e-03,  9.2657e-03,\n",
      "         1.2116e-02, -2.4062e-02,  1.7881e-02,  3.7909e-02,  2.0068e-02,\n",
      "        -8.0369e-03,  3.2289e-02, -2.5012e-02, -3.2694e-02,  6.4473e-03,\n",
      "        -2.0563e-02,  3.4726e-03, -5.8274e-03,  2.3828e-02,  1.3723e-02,\n",
      "         2.6953e-02,  3.4542e-02, -2.7218e-02,  2.2548e-02, -1.0528e-02,\n",
      "        -5.4055e-03, -8.8234e-03,  2.7123e-02, -3.0904e-02,  2.1617e-02,\n",
      "        -7.1774e-03, -3.2940e-02, -1.7699e-02,  2.5203e-02, -2.8679e-02,\n",
      "        -1.7260e-02,  3.3906e-02,  1.7779e-03,  1.1330e-02,  1.3766e-02,\n",
      "         2.9129e-02, -1.4812e-02,  2.7559e-02,  1.7467e-02,  3.0450e-02,\n",
      "        -2.0223e-02,  7.5231e-04, -2.9010e-02, -2.7132e-02, -1.4500e-02,\n",
      "        -2.7627e-03,  8.4280e-03, -1.3470e-02,  3.2618e-02, -8.3868e-03,\n",
      "        -2.1763e-02,  1.8390e-02, -1.0882e-02,  1.2659e-02,  3.2330e-02,\n",
      "         1.4918e-02, -2.9912e-02,  2.5727e-02, -1.8225e-02,  1.4592e-02,\n",
      "         4.3929e-03,  4.9089e-03, -3.4810e-04,  3.8420e-02, -2.5337e-02,\n",
      "         3.0074e-02,  9.2367e-03,  1.0044e-02,  1.7084e-02,  1.1968e-02,\n",
      "        -2.0558e-02,  2.8163e-02, -6.8584e-03, -2.3192e-02, -2.1855e-02,\n",
      "         1.3029e-02,  1.7606e-02, -1.8323e-02,  8.9034e-03,  2.0999e-02,\n",
      "         3.4973e-02, -4.8141e-03,  2.8852e-02,  1.7286e-02,  7.8676e-03,\n",
      "        -2.4323e-02,  3.5893e-02, -6.7025e-05,  1.7287e-03, -8.6086e-04,\n",
      "        -5.8470e-03, -3.0538e-02, -5.9944e-03, -2.9180e-02, -3.1429e-02,\n",
      "         2.9475e-03,  3.8736e-03,  3.6221e-02, -3.7608e-03,  3.5515e-02,\n",
      "        -1.9281e-02, -4.6984e-03, -5.4278e-04,  1.0123e-02, -2.5584e-02,\n",
      "        -2.9408e-02,  1.6776e-02, -3.6946e-03,  3.7872e-02,  2.7256e-02,\n",
      "        -1.2370e-02, -3.9490e-02,  6.7058e-03,  2.7221e-02, -5.2538e-03,\n",
      "         1.6104e-02,  2.2908e-02, -2.1802e-02, -2.0318e-02, -8.3841e-03,\n",
      "         1.1511e-02, -2.6728e-02, -3.6500e-03, -2.0268e-02, -1.4500e-02,\n",
      "        -2.0321e-02, -2.6775e-02,  3.5673e-03, -7.8840e-03, -3.2130e-02,\n",
      "         9.6583e-03, -1.9651e-03,  3.2119e-02,  1.4388e-02,  2.4268e-02,\n",
      "        -3.2213e-02, -1.9120e-02, -2.4276e-02, -1.9339e-02,  9.2263e-03,\n",
      "        -2.1061e-02,  1.1721e-02, -1.8989e-02,  1.5017e-03, -2.5178e-02,\n",
      "         3.3932e-02,  2.7741e-03, -2.6030e-02,  5.9817e-03, -1.6307e-02,\n",
      "         1.5907e-02, -8.9982e-03, -4.8762e-03, -1.1482e-02,  9.6751e-03,\n",
      "        -1.8039e-02,  5.3085e-03,  1.0920e-02,  1.6371e-02,  1.1237e-02,\n",
      "        -3.2413e-02, -2.0875e-02, -3.1973e-02, -1.8599e-02,  3.8776e-03,\n",
      "        -2.6364e-03, -9.5433e-03, -4.0878e-03, -5.4976e-03,  1.5441e-02,\n",
      "         1.5280e-02, -1.6320e-02, -2.6514e-02, -3.4665e-02,  1.6044e-02,\n",
      "        -1.9787e-02,  2.5679e-02,  1.3723e-02, -5.7191e-03, -3.1612e-02,\n",
      "        -1.5877e-02,  2.2682e-02,  2.9266e-02,  3.9722e-02,  1.8007e-02,\n",
      "        -4.1188e-03,  2.8856e-02, -2.3937e-02,  8.8238e-04,  1.9004e-02,\n",
      "         2.2901e-02,  2.6575e-02,  4.1701e-03, -2.1226e-02,  2.0712e-02,\n",
      "         9.9247e-03,  7.0419e-03, -3.5314e-02,  2.5485e-02,  1.9705e-02,\n",
      "        -2.0205e-02, -8.3748e-03, -3.3654e-02, -1.4061e-02, -2.9694e-02,\n",
      "        -1.1170e-02,  3.6333e-02,  4.4244e-03, -2.5612e-02, -5.6549e-03,\n",
      "        -1.8023e-02,  4.9357e-03,  1.1637e-02, -2.4017e-02,  3.2717e-02,\n",
      "         2.7862e-02, -2.6288e-02, -2.5246e-02, -6.5978e-03,  7.3019e-03,\n",
      "        -1.3901e-02,  1.2564e-02, -1.6922e-02, -2.3098e-02,  1.5124e-03,\n",
      "         2.2050e-02, -7.3732e-03, -2.6270e-02, -3.3003e-02,  9.7479e-03,\n",
      "         9.9669e-03, -2.9922e-03, -3.6913e-03, -1.6972e-02, -2.2801e-02,\n",
      "        -1.5953e-02, -1.7318e-03,  2.4411e-02,  1.2804e-02,  2.3136e-02,\n",
      "         1.0506e-02,  8.1458e-03], requires_grad=True)\n",
      "Parameter shape: torch.Size([512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[ 0.0060, -0.0404,  0.0409,  ..., -0.0087,  0.0380, -0.0380],\n",
      "        [ 0.0374,  0.0343, -0.0409,  ...,  0.0405, -0.0258, -0.0401],\n",
      "        [-0.0068,  0.0359, -0.0041,  ..., -0.0318, -0.0333,  0.0248],\n",
      "        ...,\n",
      "        [ 0.0215,  0.0106, -0.0047,  ...,  0.0391, -0.0103,  0.0243],\n",
      "        [ 0.0017, -0.0099,  0.0388,  ..., -0.0070,  0.0004, -0.0373],\n",
      "        [-0.0061, -0.0011,  0.0020,  ...,  0.0251, -0.0232, -0.0080]],\n",
      "       requires_grad=True)\n",
      "Parameter shape: torch.Size([512, 512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([-2.5476e-02, -1.7619e-02,  2.9820e-02, -4.1682e-02,  5.1316e-02,\n",
      "         2.0515e-02, -1.5586e-02, -1.4071e-02,  5.2043e-03, -5.3436e-03,\n",
      "        -1.3081e-02, -3.9084e-02, -1.8473e-02, -2.5499e-02,  4.4634e-02,\n",
      "         2.1549e-02, -1.2320e-02,  2.1829e-03, -3.4707e-02, -1.7355e-02,\n",
      "         2.5484e-03, -3.1082e-02,  3.4280e-03,  4.1625e-02,  3.6498e-02,\n",
      "        -2.0247e-02,  4.1307e-02,  4.4127e-02, -7.8209e-03,  2.0925e-02,\n",
      "         3.4389e-02,  4.9457e-02, -2.3438e-02,  2.7885e-02,  3.2803e-03,\n",
      "         2.9183e-02, -2.5333e-02, -2.0955e-02,  2.9191e-02,  4.0100e-02,\n",
      "         3.7891e-02,  5.7123e-03,  2.7214e-02, -1.5317e-03,  1.9736e-02,\n",
      "         3.1103e-02, -2.2017e-02,  4.0519e-02, -2.1396e-02, -4.0632e-02,\n",
      "         4.4841e-02,  1.6935e-02, -4.1904e-02,  4.1550e-02, -1.1171e-02,\n",
      "         4.5811e-02,  1.0036e-02,  1.0717e-03,  2.8193e-02, -2.4332e-04,\n",
      "        -2.6100e-02,  1.7906e-02,  3.6393e-02,  8.1166e-03,  2.4557e-02,\n",
      "         5.7112e-02,  9.9007e-03, -2.5064e-02, -1.8400e-02, -3.4320e-02,\n",
      "        -1.8613e-02,  2.7306e-02, -2.8629e-02,  2.4040e-02, -1.7851e-02,\n",
      "         1.4590e-02,  2.4615e-02,  2.9008e-02,  3.7531e-02, -3.1117e-02,\n",
      "        -2.0103e-02,  1.4893e-02,  4.4686e-02, -5.0338e-03,  5.6050e-03,\n",
      "         1.3599e-02,  3.2897e-03, -2.7224e-02,  3.6321e-02,  2.3088e-02,\n",
      "        -4.2332e-02,  4.6581e-02,  1.8180e-02, -1.9422e-02,  4.1358e-02,\n",
      "         4.5054e-02,  3.5155e-02, -3.9750e-02,  2.2206e-02,  1.8848e-03,\n",
      "        -1.1416e-02, -1.9776e-02,  9.6325e-03,  3.7596e-02,  4.0143e-02,\n",
      "         5.5477e-03,  7.8994e-03,  1.3522e-02,  2.3163e-02,  1.5443e-02,\n",
      "         5.1737e-03, -1.8521e-02, -2.4241e-02,  4.5881e-02, -2.1807e-02,\n",
      "         8.5568e-03, -1.0933e-02,  2.6250e-02,  9.8067e-04,  2.0947e-02,\n",
      "         2.6063e-02,  1.0883e-02, -3.9222e-02, -2.5709e-03, -6.9909e-03,\n",
      "         3.7336e-02,  3.4436e-02, -1.9126e-02,  4.7775e-02, -8.0682e-03,\n",
      "        -2.5866e-02, -3.7991e-03,  1.5489e-02,  2.6314e-02, -1.6046e-02,\n",
      "        -3.8199e-02,  1.1684e-02,  1.8682e-02,  2.2804e-02,  1.0543e-02,\n",
      "         3.9853e-02, -2.9483e-02, -1.6637e-02,  2.7936e-02, -2.4088e-02,\n",
      "         2.7976e-02,  1.0033e-02, -3.5464e-02, -1.4156e-02, -8.8943e-03,\n",
      "         8.4619e-03,  1.0530e-02,  2.4090e-02, -1.2093e-02,  4.5686e-03,\n",
      "        -3.9910e-02, -1.4959e-02, -1.6477e-02, -1.8899e-02, -4.8031e-03,\n",
      "        -4.2033e-02, -3.7007e-02,  2.2842e-02,  2.1983e-02, -3.7084e-02,\n",
      "        -1.4794e-02, -2.7985e-02, -3.5270e-02,  4.2553e-02,  1.3348e-02,\n",
      "        -2.2922e-02,  8.3498e-03,  4.2784e-02,  1.7344e-02,  1.2841e-02,\n",
      "         2.8733e-03,  6.3424e-03, -2.9331e-02,  2.5630e-02, -2.2832e-02,\n",
      "         1.1533e-02,  1.2818e-02,  3.4549e-02,  1.4224e-02,  3.3348e-02,\n",
      "        -1.1052e-02, -6.2125e-03, -1.3410e-02,  4.2624e-02, -1.0105e-02,\n",
      "         1.2677e-02,  2.3771e-02,  2.0723e-02, -1.5304e-02,  1.0221e-02,\n",
      "        -4.4761e-02, -2.0492e-02,  2.7616e-02, -2.0102e-02, -4.0986e-02,\n",
      "         4.0594e-02, -1.1485e-02,  2.3350e-02,  2.2161e-02,  3.6684e-02,\n",
      "         8.5757e-03,  3.1805e-02, -9.1031e-03,  9.4152e-03,  2.9894e-02,\n",
      "        -3.7040e-02,  2.6661e-02,  2.1589e-02, -3.3983e-03,  3.0983e-02,\n",
      "         3.5140e-02, -6.9103e-03, -1.3478e-02, -4.1985e-02, -2.2630e-02,\n",
      "        -2.1224e-02, -3.4560e-02, -6.8860e-03, -1.7163e-02, -2.0525e-02,\n",
      "         3.0276e-02, -1.9503e-02,  4.3498e-02,  1.6348e-03, -2.9663e-03,\n",
      "         1.1802e-02,  7.4503e-03,  3.1018e-02,  3.7893e-02, -8.2747e-03,\n",
      "        -2.1615e-02, -1.1838e-02, -2.0386e-02,  1.2487e-02,  2.6661e-02,\n",
      "        -4.0346e-02,  1.1851e-02,  5.7572e-03, -3.4720e-02,  5.2755e-02,\n",
      "         2.7505e-02, -7.4205e-03,  2.6007e-02,  3.0880e-02, -2.7807e-02,\n",
      "        -1.4257e-02, -1.9507e-02,  3.3751e-02, -1.8841e-02, -2.2719e-02,\n",
      "         4.0760e-02,  8.8891e-03, -1.0369e-03, -6.0805e-03,  2.8924e-02,\n",
      "         4.8691e-03, -5.7397e-03,  4.1956e-02,  4.2427e-02,  5.8965e-02,\n",
      "        -1.9512e-02,  8.5254e-03,  1.3432e-02,  3.0704e-02, -3.5157e-02,\n",
      "        -1.4346e-02,  2.7569e-02,  1.5383e-02, -6.9476e-03, -2.3403e-02,\n",
      "        -2.5659e-02, -8.5157e-03, -1.9065e-02,  4.1630e-02, -3.8566e-02,\n",
      "        -7.6196e-04,  1.3747e-03, -3.3705e-02,  3.4992e-02, -4.0118e-02,\n",
      "         2.8762e-02, -3.2085e-02, -2.9648e-02, -1.5691e-02, -1.8978e-02,\n",
      "         2.6108e-02,  2.9532e-02,  6.5953e-04,  3.2803e-02,  1.6324e-02,\n",
      "         5.5625e-02,  6.1635e-04,  1.8582e-02,  4.1665e-02,  2.0652e-02,\n",
      "        -3.0701e-02,  2.7689e-02, -2.9335e-02,  1.2904e-02, -4.0264e-02,\n",
      "         4.6689e-02, -4.4504e-02,  3.6376e-02, -1.2275e-02, -1.7921e-02,\n",
      "         4.2959e-03, -2.0945e-02, -1.7627e-02, -2.5882e-02,  2.9147e-02,\n",
      "        -1.8263e-02, -3.7157e-02,  1.6420e-02,  5.3934e-02,  2.0513e-03,\n",
      "         6.0014e-03,  2.4452e-02,  1.8352e-02, -2.6566e-02, -4.0615e-02,\n",
      "        -1.9630e-02, -3.8610e-02, -1.9737e-02,  1.9633e-02, -3.4408e-02,\n",
      "        -1.9376e-02, -1.8807e-02,  5.1022e-06, -2.7094e-02, -9.8760e-03,\n",
      "        -2.3611e-02,  2.0650e-02, -3.7669e-02,  3.6731e-02,  1.5150e-02,\n",
      "        -2.3685e-02, -2.7942e-02, -3.8866e-02, -3.2627e-02, -1.0179e-02,\n",
      "         2.3178e-02,  1.0953e-03,  1.0363e-02,  3.3519e-02,  1.9343e-02,\n",
      "        -3.9683e-02,  1.1286e-02,  1.9268e-02, -4.1874e-02, -4.0864e-04,\n",
      "         1.3462e-02,  1.2914e-02, -1.8689e-02,  2.9914e-02, -3.5323e-02,\n",
      "         3.3057e-03,  3.9476e-02,  1.8944e-02, -1.4071e-02, -1.9751e-02,\n",
      "         3.7608e-02,  2.4968e-02, -9.0343e-03, -6.7245e-03, -2.5506e-02,\n",
      "        -2.7516e-03,  4.6653e-02,  1.3660e-02, -2.4502e-02,  4.9496e-02,\n",
      "         2.0389e-02, -5.1143e-03,  1.1368e-02,  1.1867e-02, -9.9273e-03,\n",
      "         8.6386e-03, -3.2335e-02,  8.7534e-03,  7.4244e-03,  3.0633e-02,\n",
      "         2.0885e-02,  1.5141e-02,  6.1563e-04,  2.9806e-02, -3.4819e-02,\n",
      "         2.7311e-02,  4.4715e-02, -1.1817e-02,  4.8512e-02,  8.2169e-04,\n",
      "        -3.1790e-02,  1.6694e-02, -3.0602e-02, -1.8952e-02, -1.3130e-02,\n",
      "         3.0923e-02, -1.7289e-03, -1.0305e-02, -2.1791e-02,  4.2601e-02,\n",
      "        -2.6607e-02,  2.5111e-02, -2.8464e-02, -8.5412e-03,  2.3208e-02,\n",
      "         9.7403e-03, -2.8043e-02,  1.2366e-02,  1.8835e-02, -7.6585e-03,\n",
      "         3.9128e-02,  1.2441e-02,  7.2055e-03, -1.2625e-02,  3.4900e-02,\n",
      "         2.7684e-02, -3.1002e-02,  3.6693e-02, -2.9799e-03, -4.0441e-02,\n",
      "         3.2321e-03, -2.5990e-02,  5.9167e-03,  1.2000e-02,  2.8509e-03,\n",
      "         3.9693e-02, -1.5740e-02, -1.0556e-02, -3.4021e-02,  3.0269e-02,\n",
      "         1.1152e-02,  8.5273e-03,  1.6882e-02,  1.4188e-02,  1.7817e-02,\n",
      "         3.5432e-03,  1.8266e-02,  2.8419e-02, -3.7631e-02,  8.0272e-03,\n",
      "         1.7954e-02,  1.0494e-02, -3.3866e-02,  1.0930e-02,  1.7162e-02,\n",
      "        -4.2140e-02,  1.8427e-02,  7.4453e-03, -1.2608e-02,  1.1788e-02,\n",
      "         3.4104e-02, -3.3497e-02, -2.9512e-02,  1.5589e-02,  2.3984e-02,\n",
      "         4.0428e-02,  1.9060e-02,  2.4592e-02, -2.0933e-02,  2.4353e-02,\n",
      "        -4.0465e-02, -1.3791e-02,  4.7038e-03,  4.0609e-02,  2.3310e-02,\n",
      "         2.5741e-02, -1.2217e-02,  4.2276e-02,  2.8503e-03, -2.4246e-02,\n",
      "         4.9457e-02, -1.2035e-02,  2.5377e-02, -3.1431e-02,  1.7540e-02,\n",
      "         3.2406e-03,  1.8135e-02,  4.5612e-03,  9.4441e-04,  2.5781e-02,\n",
      "        -1.7972e-02, -8.6473e-03,  2.4420e-03, -9.6572e-03, -1.8807e-02,\n",
      "         5.1502e-02,  2.2559e-02,  4.6312e-03,  3.0699e-02, -3.5024e-02,\n",
      "         3.4219e-02,  1.0560e-02, -2.3922e-02, -3.9046e-02,  3.0311e-02,\n",
      "         3.3779e-02, -2.8842e-02,  2.3538e-02,  2.2279e-02,  3.6815e-02,\n",
      "         3.9032e-02,  1.9871e-03, -3.4831e-02,  2.2726e-02, -2.6663e-02,\n",
      "         5.7636e-03,  2.1674e-02], requires_grad=True)\n",
      "Parameter shape: torch.Size([512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.weight\n",
      "Parameter value: Parameter containing:\n",
      "tensor([[-0.0447,  0.0135, -0.0056,  ...,  0.0231,  0.0350, -0.0188],\n",
      "        [ 0.0051, -0.0010,  0.0045,  ...,  0.0078, -0.0461, -0.0199],\n",
      "        [-0.0223,  0.0226, -0.0158,  ...,  0.0271, -0.0451, -0.0110],\n",
      "        ...,\n",
      "        [ 0.0619, -0.0138,  0.0060,  ...,  0.0124,  0.0361,  0.0201],\n",
      "        [-0.0064, -0.0023,  0.0261,  ..., -0.0511,  0.0533,  0.0153],\n",
      "        [ 0.0159,  0.0281,  0.0235,  ..., -0.0438,  0.0173, -0.0296]],\n",
      "       requires_grad=True)\n",
      "Parameter shape: torch.Size([10, 512])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.bias\n",
      "Parameter value: Parameter containing:\n",
      "tensor([-0.0493, -0.0184, -0.0590, -0.0050, -0.0723,  0.1418, -0.0067,  0.0985,\n",
      "        -0.0249, -0.0741], requires_grad=True)\n",
      "Parameter shape: torch.Size([10])\n",
      "Requires gradient: True\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summarize the model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter value: {param}\")\n",
    "    print(f\"Parameter shape: {param.shape}\")\n",
    "    print(f\"Requires gradient: {param.requires_grad}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of NN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.0111, -0.0163,  0.0044,  ..., -0.0006,  0.0274,  0.0065],\n",
       "                      [-0.0118,  0.0021,  0.0163,  ...,  0.0021, -0.0304,  0.0215],\n",
       "                      [ 0.0244,  0.0233, -0.0097,  ..., -0.0152,  0.0350, -0.0285],\n",
       "                      ...,\n",
       "                      [ 0.0047, -0.0119, -0.0056,  ..., -0.0351,  0.0062,  0.0234],\n",
       "                      [-0.0012,  0.0035, -0.0316,  ..., -0.0269,  0.0182,  0.0060],\n",
       "                      [ 0.0156, -0.0253, -0.0042,  ..., -0.0286, -0.0180, -0.0263]])),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 3.1299e-02, -2.4866e-02, -2.4687e-03,  3.3257e-02, -1.2573e-02,\n",
       "                      -2.9465e-02,  3.6867e-02, -1.1514e-02, -1.7470e-03,  8.7670e-03,\n",
       "                      -2.8088e-02, -2.8737e-02, -3.4039e-02, -1.5097e-02, -3.3807e-02,\n",
       "                      -4.1252e-03, -1.3162e-02,  3.1557e-02, -2.3931e-02, -9.9615e-03,\n",
       "                       1.5670e-02, -1.0803e-03, -1.2151e-02, -2.0947e-02, -1.8876e-02,\n",
       "                       2.2011e-02, -3.3348e-02,  4.8116e-03,  8.6803e-03,  1.7623e-02,\n",
       "                       1.6403e-02, -1.2514e-02,  7.9018e-03, -2.8916e-02, -1.0766e-02,\n",
       "                      -7.5647e-03, -3.5550e-03, -3.2359e-02,  3.4937e-02, -2.5400e-02,\n",
       "                       1.4325e-02,  4.7796e-03, -1.8049e-02,  7.3009e-03, -1.3749e-02,\n",
       "                       3.7105e-02,  2.3368e-02,  4.2428e-03,  1.8882e-02, -2.4410e-02,\n",
       "                       2.9200e-03,  2.7759e-02, -1.1613e-02, -2.3947e-02, -2.8069e-02,\n",
       "                      -5.4490e-03,  1.4343e-02,  9.1595e-03, -7.5708e-03,  1.9226e-02,\n",
       "                       8.9526e-03, -3.6449e-03,  3.6030e-02, -1.4603e-02,  3.1513e-02,\n",
       "                       1.3502e-02,  2.1687e-02, -1.2270e-02,  1.8124e-02, -2.8407e-02,\n",
       "                      -1.9915e-02, -2.2935e-02, -3.6503e-02,  1.1845e-02,  2.3900e-02,\n",
       "                      -3.0695e-02, -4.0309e-03, -7.0003e-03, -3.5476e-02,  5.2369e-03,\n",
       "                       2.2343e-02,  2.2138e-02, -3.0757e-02, -2.6127e-02,  1.3471e-03,\n",
       "                      -5.1610e-03, -1.6784e-02,  1.0939e-02, -3.5012e-02,  5.6542e-03,\n",
       "                       7.6482e-03,  2.9593e-02,  3.2525e-02, -3.0391e-02,  2.1811e-02,\n",
       "                       7.6460e-03,  1.5949e-02,  1.9918e-02, -1.3567e-02,  1.2244e-02,\n",
       "                      -1.9767e-02,  1.3861e-02,  1.7690e-02,  2.1168e-02,  3.2005e-02,\n",
       "                       8.1267e-03, -2.6708e-03, -7.3254e-04,  4.2772e-04,  3.4820e-02,\n",
       "                       1.0113e-02,  3.2301e-02,  2.1028e-02, -2.8408e-02,  4.1496e-02,\n",
       "                      -9.7464e-03,  1.5589e-02, -7.5156e-03,  1.1652e-02, -1.5019e-02,\n",
       "                       3.7560e-02,  3.2533e-03,  3.2581e-02,  1.7274e-02,  2.8570e-02,\n",
       "                       8.2888e-03,  1.9975e-02, -2.7682e-02,  1.7965e-02, -1.7655e-02,\n",
       "                      -2.5013e-02,  3.1347e-02,  3.7203e-02, -3.6390e-02, -1.2477e-02,\n",
       "                       2.7538e-02,  3.6468e-02,  1.5251e-03, -3.3562e-02, -1.9580e-02,\n",
       "                      -1.7604e-02,  2.5937e-02, -7.0241e-03,  1.3931e-02, -2.0829e-02,\n",
       "                       3.0020e-02,  3.4157e-02, -8.3373e-03, -2.4119e-02, -9.1882e-03,\n",
       "                      -9.0172e-03, -3.0521e-02,  2.7748e-02, -2.6568e-02,  1.7120e-02,\n",
       "                      -9.6135e-03, -3.5436e-02,  1.1635e-02,  3.9039e-02,  3.7904e-02,\n",
       "                       2.8246e-02,  1.9714e-02, -2.4613e-02,  2.9571e-02,  1.0809e-02,\n",
       "                       2.1170e-02,  2.3050e-02, -3.3391e-02, -1.0468e-02,  1.5682e-03,\n",
       "                      -2.5103e-02, -1.4289e-02, -5.3376e-03, -1.1421e-02,  1.4830e-02,\n",
       "                       3.9414e-02,  1.8518e-02,  1.5456e-02, -1.1434e-03, -2.0770e-02,\n",
       "                      -1.2762e-02, -2.4125e-02,  4.0295e-03,  3.5649e-02,  2.3033e-02,\n",
       "                      -1.7503e-02,  1.8628e-02, -2.5089e-02,  7.5775e-03, -2.7324e-02,\n",
       "                       4.4273e-03, -2.0846e-02,  3.0332e-02, -2.7007e-02, -1.6700e-02,\n",
       "                       2.5385e-02,  3.6013e-02,  3.4034e-02, -1.9578e-02, -9.6259e-03,\n",
       "                      -5.6950e-03,  2.0621e-02, -3.3474e-02, -1.1836e-02,  1.8623e-02,\n",
       "                       7.8949e-04,  3.1272e-02, -3.2451e-02, -2.0683e-02, -2.0711e-03,\n",
       "                      -2.3011e-02, -1.6059e-03, -8.6199e-03,  2.0143e-02, -1.8260e-02,\n",
       "                      -1.1082e-04, -4.2506e-03, -1.9791e-02, -2.4167e-02,  6.9828e-03,\n",
       "                      -1.3237e-02,  1.7496e-02,  2.1691e-02,  2.1745e-02, -2.9442e-02,\n",
       "                       3.2760e-03,  1.2327e-04, -2.7517e-02, -3.3203e-02,  1.7948e-02,\n",
       "                      -4.1579e-03,  3.5348e-02,  8.8844e-03, -2.3033e-02, -1.1942e-02,\n",
       "                      -6.0597e-03,  3.4291e-02, -3.0517e-02,  2.9601e-02, -2.6078e-02,\n",
       "                       1.5574e-02,  1.0520e-02, -1.3986e-02,  1.9278e-03, -2.7844e-02,\n",
       "                       2.1302e-02, -2.1205e-02,  2.0413e-02,  2.7193e-02,  4.4633e-02,\n",
       "                      -7.0556e-03, -2.3985e-02, -1.8159e-02,  1.2458e-02,  2.5090e-03,\n",
       "                      -2.5556e-02,  8.9525e-03,  1.7325e-02, -2.8504e-02,  1.5161e-02,\n",
       "                      -1.0499e-02, -8.4576e-03, -1.5134e-02,  1.0953e-02,  3.5435e-02,\n",
       "                      -1.5580e-02,  2.9059e-02, -3.0180e-02,  6.7077e-03,  9.2657e-03,\n",
       "                       1.2116e-02, -2.4062e-02,  1.7881e-02,  3.7909e-02,  2.0068e-02,\n",
       "                      -8.0369e-03,  3.2289e-02, -2.5012e-02, -3.2694e-02,  6.4473e-03,\n",
       "                      -2.0563e-02,  3.4726e-03, -5.8274e-03,  2.3828e-02,  1.3723e-02,\n",
       "                       2.6953e-02,  3.4542e-02, -2.7218e-02,  2.2548e-02, -1.0528e-02,\n",
       "                      -5.4055e-03, -8.8234e-03,  2.7123e-02, -3.0904e-02,  2.1617e-02,\n",
       "                      -7.1774e-03, -3.2940e-02, -1.7699e-02,  2.5203e-02, -2.8679e-02,\n",
       "                      -1.7260e-02,  3.3906e-02,  1.7779e-03,  1.1330e-02,  1.3766e-02,\n",
       "                       2.9129e-02, -1.4812e-02,  2.7559e-02,  1.7467e-02,  3.0450e-02,\n",
       "                      -2.0223e-02,  7.5231e-04, -2.9010e-02, -2.7132e-02, -1.4500e-02,\n",
       "                      -2.7627e-03,  8.4280e-03, -1.3470e-02,  3.2618e-02, -8.3868e-03,\n",
       "                      -2.1763e-02,  1.8390e-02, -1.0882e-02,  1.2659e-02,  3.2330e-02,\n",
       "                       1.4918e-02, -2.9912e-02,  2.5727e-02, -1.8225e-02,  1.4592e-02,\n",
       "                       4.3929e-03,  4.9089e-03, -3.4810e-04,  3.8420e-02, -2.5337e-02,\n",
       "                       3.0074e-02,  9.2367e-03,  1.0044e-02,  1.7084e-02,  1.1968e-02,\n",
       "                      -2.0558e-02,  2.8163e-02, -6.8584e-03, -2.3192e-02, -2.1855e-02,\n",
       "                       1.3029e-02,  1.7606e-02, -1.8323e-02,  8.9034e-03,  2.0999e-02,\n",
       "                       3.4973e-02, -4.8141e-03,  2.8852e-02,  1.7286e-02,  7.8676e-03,\n",
       "                      -2.4323e-02,  3.5893e-02, -6.7025e-05,  1.7287e-03, -8.6086e-04,\n",
       "                      -5.8470e-03, -3.0538e-02, -5.9944e-03, -2.9180e-02, -3.1429e-02,\n",
       "                       2.9475e-03,  3.8736e-03,  3.6221e-02, -3.7608e-03,  3.5515e-02,\n",
       "                      -1.9281e-02, -4.6984e-03, -5.4278e-04,  1.0123e-02, -2.5584e-02,\n",
       "                      -2.9408e-02,  1.6776e-02, -3.6946e-03,  3.7872e-02,  2.7256e-02,\n",
       "                      -1.2370e-02, -3.9490e-02,  6.7058e-03,  2.7221e-02, -5.2538e-03,\n",
       "                       1.6104e-02,  2.2908e-02, -2.1802e-02, -2.0318e-02, -8.3841e-03,\n",
       "                       1.1511e-02, -2.6728e-02, -3.6500e-03, -2.0268e-02, -1.4500e-02,\n",
       "                      -2.0321e-02, -2.6775e-02,  3.5673e-03, -7.8840e-03, -3.2130e-02,\n",
       "                       9.6583e-03, -1.9651e-03,  3.2119e-02,  1.4388e-02,  2.4268e-02,\n",
       "                      -3.2213e-02, -1.9120e-02, -2.4276e-02, -1.9339e-02,  9.2263e-03,\n",
       "                      -2.1061e-02,  1.1721e-02, -1.8989e-02,  1.5017e-03, -2.5178e-02,\n",
       "                       3.3932e-02,  2.7741e-03, -2.6030e-02,  5.9817e-03, -1.6307e-02,\n",
       "                       1.5907e-02, -8.9982e-03, -4.8762e-03, -1.1482e-02,  9.6751e-03,\n",
       "                      -1.8039e-02,  5.3085e-03,  1.0920e-02,  1.6371e-02,  1.1237e-02,\n",
       "                      -3.2413e-02, -2.0875e-02, -3.1973e-02, -1.8599e-02,  3.8776e-03,\n",
       "                      -2.6364e-03, -9.5433e-03, -4.0878e-03, -5.4976e-03,  1.5441e-02,\n",
       "                       1.5280e-02, -1.6320e-02, -2.6514e-02, -3.4665e-02,  1.6044e-02,\n",
       "                      -1.9787e-02,  2.5679e-02,  1.3723e-02, -5.7191e-03, -3.1612e-02,\n",
       "                      -1.5877e-02,  2.2682e-02,  2.9266e-02,  3.9722e-02,  1.8007e-02,\n",
       "                      -4.1188e-03,  2.8856e-02, -2.3937e-02,  8.8238e-04,  1.9004e-02,\n",
       "                       2.2901e-02,  2.6575e-02,  4.1701e-03, -2.1226e-02,  2.0712e-02,\n",
       "                       9.9247e-03,  7.0419e-03, -3.5314e-02,  2.5485e-02,  1.9705e-02,\n",
       "                      -2.0205e-02, -8.3748e-03, -3.3654e-02, -1.4061e-02, -2.9694e-02,\n",
       "                      -1.1170e-02,  3.6333e-02,  4.4244e-03, -2.5612e-02, -5.6549e-03,\n",
       "                      -1.8023e-02,  4.9357e-03,  1.1637e-02, -2.4017e-02,  3.2717e-02,\n",
       "                       2.7862e-02, -2.6288e-02, -2.5246e-02, -6.5978e-03,  7.3019e-03,\n",
       "                      -1.3901e-02,  1.2564e-02, -1.6922e-02, -2.3098e-02,  1.5124e-03,\n",
       "                       2.2050e-02, -7.3732e-03, -2.6270e-02, -3.3003e-02,  9.7479e-03,\n",
       "                       9.9669e-03, -2.9922e-03, -3.6913e-03, -1.6972e-02, -2.2801e-02,\n",
       "                      -1.5953e-02, -1.7318e-03,  2.4411e-02,  1.2804e-02,  2.3136e-02,\n",
       "                       1.0506e-02,  8.1458e-03])),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.0060, -0.0404,  0.0409,  ..., -0.0087,  0.0380, -0.0380],\n",
       "                      [ 0.0374,  0.0343, -0.0409,  ...,  0.0405, -0.0258, -0.0401],\n",
       "                      [-0.0068,  0.0359, -0.0041,  ..., -0.0318, -0.0333,  0.0248],\n",
       "                      ...,\n",
       "                      [ 0.0215,  0.0106, -0.0047,  ...,  0.0391, -0.0103,  0.0243],\n",
       "                      [ 0.0017, -0.0099,  0.0388,  ..., -0.0070,  0.0004, -0.0373],\n",
       "                      [-0.0061, -0.0011,  0.0020,  ...,  0.0251, -0.0232, -0.0080]])),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([-2.5476e-02, -1.7619e-02,  2.9820e-02, -4.1682e-02,  5.1316e-02,\n",
       "                       2.0515e-02, -1.5586e-02, -1.4071e-02,  5.2043e-03, -5.3436e-03,\n",
       "                      -1.3081e-02, -3.9084e-02, -1.8473e-02, -2.5499e-02,  4.4634e-02,\n",
       "                       2.1549e-02, -1.2320e-02,  2.1829e-03, -3.4707e-02, -1.7355e-02,\n",
       "                       2.5484e-03, -3.1082e-02,  3.4280e-03,  4.1625e-02,  3.6498e-02,\n",
       "                      -2.0247e-02,  4.1307e-02,  4.4127e-02, -7.8209e-03,  2.0925e-02,\n",
       "                       3.4389e-02,  4.9457e-02, -2.3438e-02,  2.7885e-02,  3.2803e-03,\n",
       "                       2.9183e-02, -2.5333e-02, -2.0955e-02,  2.9191e-02,  4.0100e-02,\n",
       "                       3.7891e-02,  5.7123e-03,  2.7214e-02, -1.5317e-03,  1.9736e-02,\n",
       "                       3.1103e-02, -2.2017e-02,  4.0519e-02, -2.1396e-02, -4.0632e-02,\n",
       "                       4.4841e-02,  1.6935e-02, -4.1904e-02,  4.1550e-02, -1.1171e-02,\n",
       "                       4.5811e-02,  1.0036e-02,  1.0717e-03,  2.8193e-02, -2.4332e-04,\n",
       "                      -2.6100e-02,  1.7906e-02,  3.6393e-02,  8.1166e-03,  2.4557e-02,\n",
       "                       5.7112e-02,  9.9007e-03, -2.5064e-02, -1.8400e-02, -3.4320e-02,\n",
       "                      -1.8613e-02,  2.7306e-02, -2.8629e-02,  2.4040e-02, -1.7851e-02,\n",
       "                       1.4590e-02,  2.4615e-02,  2.9008e-02,  3.7531e-02, -3.1117e-02,\n",
       "                      -2.0103e-02,  1.4893e-02,  4.4686e-02, -5.0338e-03,  5.6050e-03,\n",
       "                       1.3599e-02,  3.2897e-03, -2.7224e-02,  3.6321e-02,  2.3088e-02,\n",
       "                      -4.2332e-02,  4.6581e-02,  1.8180e-02, -1.9422e-02,  4.1358e-02,\n",
       "                       4.5054e-02,  3.5155e-02, -3.9750e-02,  2.2206e-02,  1.8848e-03,\n",
       "                      -1.1416e-02, -1.9776e-02,  9.6325e-03,  3.7596e-02,  4.0143e-02,\n",
       "                       5.5477e-03,  7.8994e-03,  1.3522e-02,  2.3163e-02,  1.5443e-02,\n",
       "                       5.1737e-03, -1.8521e-02, -2.4241e-02,  4.5881e-02, -2.1807e-02,\n",
       "                       8.5568e-03, -1.0933e-02,  2.6250e-02,  9.8067e-04,  2.0947e-02,\n",
       "                       2.6063e-02,  1.0883e-02, -3.9222e-02, -2.5709e-03, -6.9909e-03,\n",
       "                       3.7336e-02,  3.4436e-02, -1.9126e-02,  4.7775e-02, -8.0682e-03,\n",
       "                      -2.5866e-02, -3.7991e-03,  1.5489e-02,  2.6314e-02, -1.6046e-02,\n",
       "                      -3.8199e-02,  1.1684e-02,  1.8682e-02,  2.2804e-02,  1.0543e-02,\n",
       "                       3.9853e-02, -2.9483e-02, -1.6637e-02,  2.7936e-02, -2.4088e-02,\n",
       "                       2.7976e-02,  1.0033e-02, -3.5464e-02, -1.4156e-02, -8.8943e-03,\n",
       "                       8.4619e-03,  1.0530e-02,  2.4090e-02, -1.2093e-02,  4.5686e-03,\n",
       "                      -3.9910e-02, -1.4959e-02, -1.6477e-02, -1.8899e-02, -4.8031e-03,\n",
       "                      -4.2033e-02, -3.7007e-02,  2.2842e-02,  2.1983e-02, -3.7084e-02,\n",
       "                      -1.4794e-02, -2.7985e-02, -3.5270e-02,  4.2553e-02,  1.3348e-02,\n",
       "                      -2.2922e-02,  8.3498e-03,  4.2784e-02,  1.7344e-02,  1.2841e-02,\n",
       "                       2.8733e-03,  6.3424e-03, -2.9331e-02,  2.5630e-02, -2.2832e-02,\n",
       "                       1.1533e-02,  1.2818e-02,  3.4549e-02,  1.4224e-02,  3.3348e-02,\n",
       "                      -1.1052e-02, -6.2125e-03, -1.3410e-02,  4.2624e-02, -1.0105e-02,\n",
       "                       1.2677e-02,  2.3771e-02,  2.0723e-02, -1.5304e-02,  1.0221e-02,\n",
       "                      -4.4761e-02, -2.0492e-02,  2.7616e-02, -2.0102e-02, -4.0986e-02,\n",
       "                       4.0594e-02, -1.1485e-02,  2.3350e-02,  2.2161e-02,  3.6684e-02,\n",
       "                       8.5757e-03,  3.1805e-02, -9.1031e-03,  9.4152e-03,  2.9894e-02,\n",
       "                      -3.7040e-02,  2.6661e-02,  2.1589e-02, -3.3983e-03,  3.0983e-02,\n",
       "                       3.5140e-02, -6.9103e-03, -1.3478e-02, -4.1985e-02, -2.2630e-02,\n",
       "                      -2.1224e-02, -3.4560e-02, -6.8860e-03, -1.7163e-02, -2.0525e-02,\n",
       "                       3.0276e-02, -1.9503e-02,  4.3498e-02,  1.6348e-03, -2.9663e-03,\n",
       "                       1.1802e-02,  7.4503e-03,  3.1018e-02,  3.7893e-02, -8.2747e-03,\n",
       "                      -2.1615e-02, -1.1838e-02, -2.0386e-02,  1.2487e-02,  2.6661e-02,\n",
       "                      -4.0346e-02,  1.1851e-02,  5.7572e-03, -3.4720e-02,  5.2755e-02,\n",
       "                       2.7505e-02, -7.4205e-03,  2.6007e-02,  3.0880e-02, -2.7807e-02,\n",
       "                      -1.4257e-02, -1.9507e-02,  3.3751e-02, -1.8841e-02, -2.2719e-02,\n",
       "                       4.0760e-02,  8.8891e-03, -1.0369e-03, -6.0805e-03,  2.8924e-02,\n",
       "                       4.8691e-03, -5.7397e-03,  4.1956e-02,  4.2427e-02,  5.8965e-02,\n",
       "                      -1.9512e-02,  8.5254e-03,  1.3432e-02,  3.0704e-02, -3.5157e-02,\n",
       "                      -1.4346e-02,  2.7569e-02,  1.5383e-02, -6.9476e-03, -2.3403e-02,\n",
       "                      -2.5659e-02, -8.5157e-03, -1.9065e-02,  4.1630e-02, -3.8566e-02,\n",
       "                      -7.6196e-04,  1.3747e-03, -3.3705e-02,  3.4992e-02, -4.0118e-02,\n",
       "                       2.8762e-02, -3.2085e-02, -2.9648e-02, -1.5691e-02, -1.8978e-02,\n",
       "                       2.6108e-02,  2.9532e-02,  6.5953e-04,  3.2803e-02,  1.6324e-02,\n",
       "                       5.5625e-02,  6.1635e-04,  1.8582e-02,  4.1665e-02,  2.0652e-02,\n",
       "                      -3.0701e-02,  2.7689e-02, -2.9335e-02,  1.2904e-02, -4.0264e-02,\n",
       "                       4.6689e-02, -4.4504e-02,  3.6376e-02, -1.2275e-02, -1.7921e-02,\n",
       "                       4.2959e-03, -2.0945e-02, -1.7627e-02, -2.5882e-02,  2.9147e-02,\n",
       "                      -1.8263e-02, -3.7157e-02,  1.6420e-02,  5.3934e-02,  2.0513e-03,\n",
       "                       6.0014e-03,  2.4452e-02,  1.8352e-02, -2.6566e-02, -4.0615e-02,\n",
       "                      -1.9630e-02, -3.8610e-02, -1.9737e-02,  1.9633e-02, -3.4408e-02,\n",
       "                      -1.9376e-02, -1.8807e-02,  5.1022e-06, -2.7094e-02, -9.8760e-03,\n",
       "                      -2.3611e-02,  2.0650e-02, -3.7669e-02,  3.6731e-02,  1.5150e-02,\n",
       "                      -2.3685e-02, -2.7942e-02, -3.8866e-02, -3.2627e-02, -1.0179e-02,\n",
       "                       2.3178e-02,  1.0953e-03,  1.0363e-02,  3.3519e-02,  1.9343e-02,\n",
       "                      -3.9683e-02,  1.1286e-02,  1.9268e-02, -4.1874e-02, -4.0864e-04,\n",
       "                       1.3462e-02,  1.2914e-02, -1.8689e-02,  2.9914e-02, -3.5323e-02,\n",
       "                       3.3057e-03,  3.9476e-02,  1.8944e-02, -1.4071e-02, -1.9751e-02,\n",
       "                       3.7608e-02,  2.4968e-02, -9.0343e-03, -6.7245e-03, -2.5506e-02,\n",
       "                      -2.7516e-03,  4.6653e-02,  1.3660e-02, -2.4502e-02,  4.9496e-02,\n",
       "                       2.0389e-02, -5.1143e-03,  1.1368e-02,  1.1867e-02, -9.9273e-03,\n",
       "                       8.6386e-03, -3.2335e-02,  8.7534e-03,  7.4244e-03,  3.0633e-02,\n",
       "                       2.0885e-02,  1.5141e-02,  6.1563e-04,  2.9806e-02, -3.4819e-02,\n",
       "                       2.7311e-02,  4.4715e-02, -1.1817e-02,  4.8512e-02,  8.2169e-04,\n",
       "                      -3.1790e-02,  1.6694e-02, -3.0602e-02, -1.8952e-02, -1.3130e-02,\n",
       "                       3.0923e-02, -1.7289e-03, -1.0305e-02, -2.1791e-02,  4.2601e-02,\n",
       "                      -2.6607e-02,  2.5111e-02, -2.8464e-02, -8.5412e-03,  2.3208e-02,\n",
       "                       9.7403e-03, -2.8043e-02,  1.2366e-02,  1.8835e-02, -7.6585e-03,\n",
       "                       3.9128e-02,  1.2441e-02,  7.2055e-03, -1.2625e-02,  3.4900e-02,\n",
       "                       2.7684e-02, -3.1002e-02,  3.6693e-02, -2.9799e-03, -4.0441e-02,\n",
       "                       3.2321e-03, -2.5990e-02,  5.9167e-03,  1.2000e-02,  2.8509e-03,\n",
       "                       3.9693e-02, -1.5740e-02, -1.0556e-02, -3.4021e-02,  3.0269e-02,\n",
       "                       1.1152e-02,  8.5273e-03,  1.6882e-02,  1.4188e-02,  1.7817e-02,\n",
       "                       3.5432e-03,  1.8266e-02,  2.8419e-02, -3.7631e-02,  8.0272e-03,\n",
       "                       1.7954e-02,  1.0494e-02, -3.3866e-02,  1.0930e-02,  1.7162e-02,\n",
       "                      -4.2140e-02,  1.8427e-02,  7.4453e-03, -1.2608e-02,  1.1788e-02,\n",
       "                       3.4104e-02, -3.3497e-02, -2.9512e-02,  1.5589e-02,  2.3984e-02,\n",
       "                       4.0428e-02,  1.9060e-02,  2.4592e-02, -2.0933e-02,  2.4353e-02,\n",
       "                      -4.0465e-02, -1.3791e-02,  4.7038e-03,  4.0609e-02,  2.3310e-02,\n",
       "                       2.5741e-02, -1.2217e-02,  4.2276e-02,  2.8503e-03, -2.4246e-02,\n",
       "                       4.9457e-02, -1.2035e-02,  2.5377e-02, -3.1431e-02,  1.7540e-02,\n",
       "                       3.2406e-03,  1.8135e-02,  4.5612e-03,  9.4441e-04,  2.5781e-02,\n",
       "                      -1.7972e-02, -8.6473e-03,  2.4420e-03, -9.6572e-03, -1.8807e-02,\n",
       "                       5.1502e-02,  2.2559e-02,  4.6312e-03,  3.0699e-02, -3.5024e-02,\n",
       "                       3.4219e-02,  1.0560e-02, -2.3922e-02, -3.9046e-02,  3.0311e-02,\n",
       "                       3.3779e-02, -2.8842e-02,  2.3538e-02,  2.2279e-02,  3.6815e-02,\n",
       "                       3.9032e-02,  1.9871e-03, -3.4831e-02,  2.2726e-02, -2.6663e-02,\n",
       "                       5.7636e-03,  2.1674e-02])),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[-0.0447,  0.0135, -0.0056,  ...,  0.0231,  0.0350, -0.0188],\n",
       "                      [ 0.0051, -0.0010,  0.0045,  ...,  0.0078, -0.0461, -0.0199],\n",
       "                      [-0.0223,  0.0226, -0.0158,  ...,  0.0271, -0.0451, -0.0110],\n",
       "                      ...,\n",
       "                      [ 0.0619, -0.0138,  0.0060,  ...,  0.0124,  0.0361,  0.0201],\n",
       "                      [-0.0064, -0.0023,  0.0261,  ..., -0.0511,  0.0533,  0.0153],\n",
       "                      [ 0.0159,  0.0281,  0.0235,  ..., -0.0438,  0.0173, -0.0296]])),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([-0.0493, -0.0184, -0.0590, -0.0050, -0.0723,  0.1418, -0.0067,  0.0985,\n",
       "                      -0.0249, -0.0741]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: linear_relu_stack.0.weight\n",
      "Parameter value: tensor([[ 0.0111, -0.0163,  0.0044,  ..., -0.0006,  0.0274,  0.0065],\n",
      "        [-0.0118,  0.0021,  0.0163,  ...,  0.0021, -0.0304,  0.0215],\n",
      "        [ 0.0244,  0.0233, -0.0097,  ..., -0.0152,  0.0350, -0.0285],\n",
      "        ...,\n",
      "        [ 0.0047, -0.0119, -0.0056,  ..., -0.0351,  0.0062,  0.0234],\n",
      "        [-0.0012,  0.0035, -0.0316,  ..., -0.0269,  0.0182,  0.0060],\n",
      "        [ 0.0156, -0.0253, -0.0042,  ..., -0.0286, -0.0180, -0.0263]])\n",
      "Parameter shape: torch.Size([512, 784])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.0.bias\n",
      "Parameter value: tensor([ 3.1299e-02, -2.4866e-02, -2.4687e-03,  3.3257e-02, -1.2573e-02,\n",
      "        -2.9465e-02,  3.6867e-02, -1.1514e-02, -1.7470e-03,  8.7670e-03,\n",
      "        -2.8088e-02, -2.8737e-02, -3.4039e-02, -1.5097e-02, -3.3807e-02,\n",
      "        -4.1252e-03, -1.3162e-02,  3.1557e-02, -2.3931e-02, -9.9615e-03,\n",
      "         1.5670e-02, -1.0803e-03, -1.2151e-02, -2.0947e-02, -1.8876e-02,\n",
      "         2.2011e-02, -3.3348e-02,  4.8116e-03,  8.6803e-03,  1.7623e-02,\n",
      "         1.6403e-02, -1.2514e-02,  7.9018e-03, -2.8916e-02, -1.0766e-02,\n",
      "        -7.5647e-03, -3.5550e-03, -3.2359e-02,  3.4937e-02, -2.5400e-02,\n",
      "         1.4325e-02,  4.7796e-03, -1.8049e-02,  7.3009e-03, -1.3749e-02,\n",
      "         3.7105e-02,  2.3368e-02,  4.2428e-03,  1.8882e-02, -2.4410e-02,\n",
      "         2.9200e-03,  2.7759e-02, -1.1613e-02, -2.3947e-02, -2.8069e-02,\n",
      "        -5.4490e-03,  1.4343e-02,  9.1595e-03, -7.5708e-03,  1.9226e-02,\n",
      "         8.9526e-03, -3.6449e-03,  3.6030e-02, -1.4603e-02,  3.1513e-02,\n",
      "         1.3502e-02,  2.1687e-02, -1.2270e-02,  1.8124e-02, -2.8407e-02,\n",
      "        -1.9915e-02, -2.2935e-02, -3.6503e-02,  1.1845e-02,  2.3900e-02,\n",
      "        -3.0695e-02, -4.0309e-03, -7.0003e-03, -3.5476e-02,  5.2369e-03,\n",
      "         2.2343e-02,  2.2138e-02, -3.0757e-02, -2.6127e-02,  1.3471e-03,\n",
      "        -5.1610e-03, -1.6784e-02,  1.0939e-02, -3.5012e-02,  5.6542e-03,\n",
      "         7.6482e-03,  2.9593e-02,  3.2525e-02, -3.0391e-02,  2.1811e-02,\n",
      "         7.6460e-03,  1.5949e-02,  1.9918e-02, -1.3567e-02,  1.2244e-02,\n",
      "        -1.9767e-02,  1.3861e-02,  1.7690e-02,  2.1168e-02,  3.2005e-02,\n",
      "         8.1267e-03, -2.6708e-03, -7.3254e-04,  4.2772e-04,  3.4820e-02,\n",
      "         1.0113e-02,  3.2301e-02,  2.1028e-02, -2.8408e-02,  4.1496e-02,\n",
      "        -9.7464e-03,  1.5589e-02, -7.5156e-03,  1.1652e-02, -1.5019e-02,\n",
      "         3.7560e-02,  3.2533e-03,  3.2581e-02,  1.7274e-02,  2.8570e-02,\n",
      "         8.2888e-03,  1.9975e-02, -2.7682e-02,  1.7965e-02, -1.7655e-02,\n",
      "        -2.5013e-02,  3.1347e-02,  3.7203e-02, -3.6390e-02, -1.2477e-02,\n",
      "         2.7538e-02,  3.6468e-02,  1.5251e-03, -3.3562e-02, -1.9580e-02,\n",
      "        -1.7604e-02,  2.5937e-02, -7.0241e-03,  1.3931e-02, -2.0829e-02,\n",
      "         3.0020e-02,  3.4157e-02, -8.3373e-03, -2.4119e-02, -9.1882e-03,\n",
      "        -9.0172e-03, -3.0521e-02,  2.7748e-02, -2.6568e-02,  1.7120e-02,\n",
      "        -9.6135e-03, -3.5436e-02,  1.1635e-02,  3.9039e-02,  3.7904e-02,\n",
      "         2.8246e-02,  1.9714e-02, -2.4613e-02,  2.9571e-02,  1.0809e-02,\n",
      "         2.1170e-02,  2.3050e-02, -3.3391e-02, -1.0468e-02,  1.5682e-03,\n",
      "        -2.5103e-02, -1.4289e-02, -5.3376e-03, -1.1421e-02,  1.4830e-02,\n",
      "         3.9414e-02,  1.8518e-02,  1.5456e-02, -1.1434e-03, -2.0770e-02,\n",
      "        -1.2762e-02, -2.4125e-02,  4.0295e-03,  3.5649e-02,  2.3033e-02,\n",
      "        -1.7503e-02,  1.8628e-02, -2.5089e-02,  7.5775e-03, -2.7324e-02,\n",
      "         4.4273e-03, -2.0846e-02,  3.0332e-02, -2.7007e-02, -1.6700e-02,\n",
      "         2.5385e-02,  3.6013e-02,  3.4034e-02, -1.9578e-02, -9.6259e-03,\n",
      "        -5.6950e-03,  2.0621e-02, -3.3474e-02, -1.1836e-02,  1.8623e-02,\n",
      "         7.8949e-04,  3.1272e-02, -3.2451e-02, -2.0683e-02, -2.0711e-03,\n",
      "        -2.3011e-02, -1.6059e-03, -8.6199e-03,  2.0143e-02, -1.8260e-02,\n",
      "        -1.1082e-04, -4.2506e-03, -1.9791e-02, -2.4167e-02,  6.9828e-03,\n",
      "        -1.3237e-02,  1.7496e-02,  2.1691e-02,  2.1745e-02, -2.9442e-02,\n",
      "         3.2760e-03,  1.2327e-04, -2.7517e-02, -3.3203e-02,  1.7948e-02,\n",
      "        -4.1579e-03,  3.5348e-02,  8.8844e-03, -2.3033e-02, -1.1942e-02,\n",
      "        -6.0597e-03,  3.4291e-02, -3.0517e-02,  2.9601e-02, -2.6078e-02,\n",
      "         1.5574e-02,  1.0520e-02, -1.3986e-02,  1.9278e-03, -2.7844e-02,\n",
      "         2.1302e-02, -2.1205e-02,  2.0413e-02,  2.7193e-02,  4.4633e-02,\n",
      "        -7.0556e-03, -2.3985e-02, -1.8159e-02,  1.2458e-02,  2.5090e-03,\n",
      "        -2.5556e-02,  8.9525e-03,  1.7325e-02, -2.8504e-02,  1.5161e-02,\n",
      "        -1.0499e-02, -8.4576e-03, -1.5134e-02,  1.0953e-02,  3.5435e-02,\n",
      "        -1.5580e-02,  2.9059e-02, -3.0180e-02,  6.7077e-03,  9.2657e-03,\n",
      "         1.2116e-02, -2.4062e-02,  1.7881e-02,  3.7909e-02,  2.0068e-02,\n",
      "        -8.0369e-03,  3.2289e-02, -2.5012e-02, -3.2694e-02,  6.4473e-03,\n",
      "        -2.0563e-02,  3.4726e-03, -5.8274e-03,  2.3828e-02,  1.3723e-02,\n",
      "         2.6953e-02,  3.4542e-02, -2.7218e-02,  2.2548e-02, -1.0528e-02,\n",
      "        -5.4055e-03, -8.8234e-03,  2.7123e-02, -3.0904e-02,  2.1617e-02,\n",
      "        -7.1774e-03, -3.2940e-02, -1.7699e-02,  2.5203e-02, -2.8679e-02,\n",
      "        -1.7260e-02,  3.3906e-02,  1.7779e-03,  1.1330e-02,  1.3766e-02,\n",
      "         2.9129e-02, -1.4812e-02,  2.7559e-02,  1.7467e-02,  3.0450e-02,\n",
      "        -2.0223e-02,  7.5231e-04, -2.9010e-02, -2.7132e-02, -1.4500e-02,\n",
      "        -2.7627e-03,  8.4280e-03, -1.3470e-02,  3.2618e-02, -8.3868e-03,\n",
      "        -2.1763e-02,  1.8390e-02, -1.0882e-02,  1.2659e-02,  3.2330e-02,\n",
      "         1.4918e-02, -2.9912e-02,  2.5727e-02, -1.8225e-02,  1.4592e-02,\n",
      "         4.3929e-03,  4.9089e-03, -3.4810e-04,  3.8420e-02, -2.5337e-02,\n",
      "         3.0074e-02,  9.2367e-03,  1.0044e-02,  1.7084e-02,  1.1968e-02,\n",
      "        -2.0558e-02,  2.8163e-02, -6.8584e-03, -2.3192e-02, -2.1855e-02,\n",
      "         1.3029e-02,  1.7606e-02, -1.8323e-02,  8.9034e-03,  2.0999e-02,\n",
      "         3.4973e-02, -4.8141e-03,  2.8852e-02,  1.7286e-02,  7.8676e-03,\n",
      "        -2.4323e-02,  3.5893e-02, -6.7025e-05,  1.7287e-03, -8.6086e-04,\n",
      "        -5.8470e-03, -3.0538e-02, -5.9944e-03, -2.9180e-02, -3.1429e-02,\n",
      "         2.9475e-03,  3.8736e-03,  3.6221e-02, -3.7608e-03,  3.5515e-02,\n",
      "        -1.9281e-02, -4.6984e-03, -5.4278e-04,  1.0123e-02, -2.5584e-02,\n",
      "        -2.9408e-02,  1.6776e-02, -3.6946e-03,  3.7872e-02,  2.7256e-02,\n",
      "        -1.2370e-02, -3.9490e-02,  6.7058e-03,  2.7221e-02, -5.2538e-03,\n",
      "         1.6104e-02,  2.2908e-02, -2.1802e-02, -2.0318e-02, -8.3841e-03,\n",
      "         1.1511e-02, -2.6728e-02, -3.6500e-03, -2.0268e-02, -1.4500e-02,\n",
      "        -2.0321e-02, -2.6775e-02,  3.5673e-03, -7.8840e-03, -3.2130e-02,\n",
      "         9.6583e-03, -1.9651e-03,  3.2119e-02,  1.4388e-02,  2.4268e-02,\n",
      "        -3.2213e-02, -1.9120e-02, -2.4276e-02, -1.9339e-02,  9.2263e-03,\n",
      "        -2.1061e-02,  1.1721e-02, -1.8989e-02,  1.5017e-03, -2.5178e-02,\n",
      "         3.3932e-02,  2.7741e-03, -2.6030e-02,  5.9817e-03, -1.6307e-02,\n",
      "         1.5907e-02, -8.9982e-03, -4.8762e-03, -1.1482e-02,  9.6751e-03,\n",
      "        -1.8039e-02,  5.3085e-03,  1.0920e-02,  1.6371e-02,  1.1237e-02,\n",
      "        -3.2413e-02, -2.0875e-02, -3.1973e-02, -1.8599e-02,  3.8776e-03,\n",
      "        -2.6364e-03, -9.5433e-03, -4.0878e-03, -5.4976e-03,  1.5441e-02,\n",
      "         1.5280e-02, -1.6320e-02, -2.6514e-02, -3.4665e-02,  1.6044e-02,\n",
      "        -1.9787e-02,  2.5679e-02,  1.3723e-02, -5.7191e-03, -3.1612e-02,\n",
      "        -1.5877e-02,  2.2682e-02,  2.9266e-02,  3.9722e-02,  1.8007e-02,\n",
      "        -4.1188e-03,  2.8856e-02, -2.3937e-02,  8.8238e-04,  1.9004e-02,\n",
      "         2.2901e-02,  2.6575e-02,  4.1701e-03, -2.1226e-02,  2.0712e-02,\n",
      "         9.9247e-03,  7.0419e-03, -3.5314e-02,  2.5485e-02,  1.9705e-02,\n",
      "        -2.0205e-02, -8.3748e-03, -3.3654e-02, -1.4061e-02, -2.9694e-02,\n",
      "        -1.1170e-02,  3.6333e-02,  4.4244e-03, -2.5612e-02, -5.6549e-03,\n",
      "        -1.8023e-02,  4.9357e-03,  1.1637e-02, -2.4017e-02,  3.2717e-02,\n",
      "         2.7862e-02, -2.6288e-02, -2.5246e-02, -6.5978e-03,  7.3019e-03,\n",
      "        -1.3901e-02,  1.2564e-02, -1.6922e-02, -2.3098e-02,  1.5124e-03,\n",
      "         2.2050e-02, -7.3732e-03, -2.6270e-02, -3.3003e-02,  9.7479e-03,\n",
      "         9.9669e-03, -2.9922e-03, -3.6913e-03, -1.6972e-02, -2.2801e-02,\n",
      "        -1.5953e-02, -1.7318e-03,  2.4411e-02,  1.2804e-02,  2.3136e-02,\n",
      "         1.0506e-02,  8.1458e-03])\n",
      "Parameter shape: torch.Size([512])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.weight\n",
      "Parameter value: tensor([[ 0.0060, -0.0404,  0.0409,  ..., -0.0087,  0.0380, -0.0380],\n",
      "        [ 0.0374,  0.0343, -0.0409,  ...,  0.0405, -0.0258, -0.0401],\n",
      "        [-0.0068,  0.0359, -0.0041,  ..., -0.0318, -0.0333,  0.0248],\n",
      "        ...,\n",
      "        [ 0.0215,  0.0106, -0.0047,  ...,  0.0391, -0.0103,  0.0243],\n",
      "        [ 0.0017, -0.0099,  0.0388,  ..., -0.0070,  0.0004, -0.0373],\n",
      "        [-0.0061, -0.0011,  0.0020,  ...,  0.0251, -0.0232, -0.0080]])\n",
      "Parameter shape: torch.Size([512, 512])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.2.bias\n",
      "Parameter value: tensor([-2.5476e-02, -1.7619e-02,  2.9820e-02, -4.1682e-02,  5.1316e-02,\n",
      "         2.0515e-02, -1.5586e-02, -1.4071e-02,  5.2043e-03, -5.3436e-03,\n",
      "        -1.3081e-02, -3.9084e-02, -1.8473e-02, -2.5499e-02,  4.4634e-02,\n",
      "         2.1549e-02, -1.2320e-02,  2.1829e-03, -3.4707e-02, -1.7355e-02,\n",
      "         2.5484e-03, -3.1082e-02,  3.4280e-03,  4.1625e-02,  3.6498e-02,\n",
      "        -2.0247e-02,  4.1307e-02,  4.4127e-02, -7.8209e-03,  2.0925e-02,\n",
      "         3.4389e-02,  4.9457e-02, -2.3438e-02,  2.7885e-02,  3.2803e-03,\n",
      "         2.9183e-02, -2.5333e-02, -2.0955e-02,  2.9191e-02,  4.0100e-02,\n",
      "         3.7891e-02,  5.7123e-03,  2.7214e-02, -1.5317e-03,  1.9736e-02,\n",
      "         3.1103e-02, -2.2017e-02,  4.0519e-02, -2.1396e-02, -4.0632e-02,\n",
      "         4.4841e-02,  1.6935e-02, -4.1904e-02,  4.1550e-02, -1.1171e-02,\n",
      "         4.5811e-02,  1.0036e-02,  1.0717e-03,  2.8193e-02, -2.4332e-04,\n",
      "        -2.6100e-02,  1.7906e-02,  3.6393e-02,  8.1166e-03,  2.4557e-02,\n",
      "         5.7112e-02,  9.9007e-03, -2.5064e-02, -1.8400e-02, -3.4320e-02,\n",
      "        -1.8613e-02,  2.7306e-02, -2.8629e-02,  2.4040e-02, -1.7851e-02,\n",
      "         1.4590e-02,  2.4615e-02,  2.9008e-02,  3.7531e-02, -3.1117e-02,\n",
      "        -2.0103e-02,  1.4893e-02,  4.4686e-02, -5.0338e-03,  5.6050e-03,\n",
      "         1.3599e-02,  3.2897e-03, -2.7224e-02,  3.6321e-02,  2.3088e-02,\n",
      "        -4.2332e-02,  4.6581e-02,  1.8180e-02, -1.9422e-02,  4.1358e-02,\n",
      "         4.5054e-02,  3.5155e-02, -3.9750e-02,  2.2206e-02,  1.8848e-03,\n",
      "        -1.1416e-02, -1.9776e-02,  9.6325e-03,  3.7596e-02,  4.0143e-02,\n",
      "         5.5477e-03,  7.8994e-03,  1.3522e-02,  2.3163e-02,  1.5443e-02,\n",
      "         5.1737e-03, -1.8521e-02, -2.4241e-02,  4.5881e-02, -2.1807e-02,\n",
      "         8.5568e-03, -1.0933e-02,  2.6250e-02,  9.8067e-04,  2.0947e-02,\n",
      "         2.6063e-02,  1.0883e-02, -3.9222e-02, -2.5709e-03, -6.9909e-03,\n",
      "         3.7336e-02,  3.4436e-02, -1.9126e-02,  4.7775e-02, -8.0682e-03,\n",
      "        -2.5866e-02, -3.7991e-03,  1.5489e-02,  2.6314e-02, -1.6046e-02,\n",
      "        -3.8199e-02,  1.1684e-02,  1.8682e-02,  2.2804e-02,  1.0543e-02,\n",
      "         3.9853e-02, -2.9483e-02, -1.6637e-02,  2.7936e-02, -2.4088e-02,\n",
      "         2.7976e-02,  1.0033e-02, -3.5464e-02, -1.4156e-02, -8.8943e-03,\n",
      "         8.4619e-03,  1.0530e-02,  2.4090e-02, -1.2093e-02,  4.5686e-03,\n",
      "        -3.9910e-02, -1.4959e-02, -1.6477e-02, -1.8899e-02, -4.8031e-03,\n",
      "        -4.2033e-02, -3.7007e-02,  2.2842e-02,  2.1983e-02, -3.7084e-02,\n",
      "        -1.4794e-02, -2.7985e-02, -3.5270e-02,  4.2553e-02,  1.3348e-02,\n",
      "        -2.2922e-02,  8.3498e-03,  4.2784e-02,  1.7344e-02,  1.2841e-02,\n",
      "         2.8733e-03,  6.3424e-03, -2.9331e-02,  2.5630e-02, -2.2832e-02,\n",
      "         1.1533e-02,  1.2818e-02,  3.4549e-02,  1.4224e-02,  3.3348e-02,\n",
      "        -1.1052e-02, -6.2125e-03, -1.3410e-02,  4.2624e-02, -1.0105e-02,\n",
      "         1.2677e-02,  2.3771e-02,  2.0723e-02, -1.5304e-02,  1.0221e-02,\n",
      "        -4.4761e-02, -2.0492e-02,  2.7616e-02, -2.0102e-02, -4.0986e-02,\n",
      "         4.0594e-02, -1.1485e-02,  2.3350e-02,  2.2161e-02,  3.6684e-02,\n",
      "         8.5757e-03,  3.1805e-02, -9.1031e-03,  9.4152e-03,  2.9894e-02,\n",
      "        -3.7040e-02,  2.6661e-02,  2.1589e-02, -3.3983e-03,  3.0983e-02,\n",
      "         3.5140e-02, -6.9103e-03, -1.3478e-02, -4.1985e-02, -2.2630e-02,\n",
      "        -2.1224e-02, -3.4560e-02, -6.8860e-03, -1.7163e-02, -2.0525e-02,\n",
      "         3.0276e-02, -1.9503e-02,  4.3498e-02,  1.6348e-03, -2.9663e-03,\n",
      "         1.1802e-02,  7.4503e-03,  3.1018e-02,  3.7893e-02, -8.2747e-03,\n",
      "        -2.1615e-02, -1.1838e-02, -2.0386e-02,  1.2487e-02,  2.6661e-02,\n",
      "        -4.0346e-02,  1.1851e-02,  5.7572e-03, -3.4720e-02,  5.2755e-02,\n",
      "         2.7505e-02, -7.4205e-03,  2.6007e-02,  3.0880e-02, -2.7807e-02,\n",
      "        -1.4257e-02, -1.9507e-02,  3.3751e-02, -1.8841e-02, -2.2719e-02,\n",
      "         4.0760e-02,  8.8891e-03, -1.0369e-03, -6.0805e-03,  2.8924e-02,\n",
      "         4.8691e-03, -5.7397e-03,  4.1956e-02,  4.2427e-02,  5.8965e-02,\n",
      "        -1.9512e-02,  8.5254e-03,  1.3432e-02,  3.0704e-02, -3.5157e-02,\n",
      "        -1.4346e-02,  2.7569e-02,  1.5383e-02, -6.9476e-03, -2.3403e-02,\n",
      "        -2.5659e-02, -8.5157e-03, -1.9065e-02,  4.1630e-02, -3.8566e-02,\n",
      "        -7.6196e-04,  1.3747e-03, -3.3705e-02,  3.4992e-02, -4.0118e-02,\n",
      "         2.8762e-02, -3.2085e-02, -2.9648e-02, -1.5691e-02, -1.8978e-02,\n",
      "         2.6108e-02,  2.9532e-02,  6.5953e-04,  3.2803e-02,  1.6324e-02,\n",
      "         5.5625e-02,  6.1635e-04,  1.8582e-02,  4.1665e-02,  2.0652e-02,\n",
      "        -3.0701e-02,  2.7689e-02, -2.9335e-02,  1.2904e-02, -4.0264e-02,\n",
      "         4.6689e-02, -4.4504e-02,  3.6376e-02, -1.2275e-02, -1.7921e-02,\n",
      "         4.2959e-03, -2.0945e-02, -1.7627e-02, -2.5882e-02,  2.9147e-02,\n",
      "        -1.8263e-02, -3.7157e-02,  1.6420e-02,  5.3934e-02,  2.0513e-03,\n",
      "         6.0014e-03,  2.4452e-02,  1.8352e-02, -2.6566e-02, -4.0615e-02,\n",
      "        -1.9630e-02, -3.8610e-02, -1.9737e-02,  1.9633e-02, -3.4408e-02,\n",
      "        -1.9376e-02, -1.8807e-02,  5.1022e-06, -2.7094e-02, -9.8760e-03,\n",
      "        -2.3611e-02,  2.0650e-02, -3.7669e-02,  3.6731e-02,  1.5150e-02,\n",
      "        -2.3685e-02, -2.7942e-02, -3.8866e-02, -3.2627e-02, -1.0179e-02,\n",
      "         2.3178e-02,  1.0953e-03,  1.0363e-02,  3.3519e-02,  1.9343e-02,\n",
      "        -3.9683e-02,  1.1286e-02,  1.9268e-02, -4.1874e-02, -4.0864e-04,\n",
      "         1.3462e-02,  1.2914e-02, -1.8689e-02,  2.9914e-02, -3.5323e-02,\n",
      "         3.3057e-03,  3.9476e-02,  1.8944e-02, -1.4071e-02, -1.9751e-02,\n",
      "         3.7608e-02,  2.4968e-02, -9.0343e-03, -6.7245e-03, -2.5506e-02,\n",
      "        -2.7516e-03,  4.6653e-02,  1.3660e-02, -2.4502e-02,  4.9496e-02,\n",
      "         2.0389e-02, -5.1143e-03,  1.1368e-02,  1.1867e-02, -9.9273e-03,\n",
      "         8.6386e-03, -3.2335e-02,  8.7534e-03,  7.4244e-03,  3.0633e-02,\n",
      "         2.0885e-02,  1.5141e-02,  6.1563e-04,  2.9806e-02, -3.4819e-02,\n",
      "         2.7311e-02,  4.4715e-02, -1.1817e-02,  4.8512e-02,  8.2169e-04,\n",
      "        -3.1790e-02,  1.6694e-02, -3.0602e-02, -1.8952e-02, -1.3130e-02,\n",
      "         3.0923e-02, -1.7289e-03, -1.0305e-02, -2.1791e-02,  4.2601e-02,\n",
      "        -2.6607e-02,  2.5111e-02, -2.8464e-02, -8.5412e-03,  2.3208e-02,\n",
      "         9.7403e-03, -2.8043e-02,  1.2366e-02,  1.8835e-02, -7.6585e-03,\n",
      "         3.9128e-02,  1.2441e-02,  7.2055e-03, -1.2625e-02,  3.4900e-02,\n",
      "         2.7684e-02, -3.1002e-02,  3.6693e-02, -2.9799e-03, -4.0441e-02,\n",
      "         3.2321e-03, -2.5990e-02,  5.9167e-03,  1.2000e-02,  2.8509e-03,\n",
      "         3.9693e-02, -1.5740e-02, -1.0556e-02, -3.4021e-02,  3.0269e-02,\n",
      "         1.1152e-02,  8.5273e-03,  1.6882e-02,  1.4188e-02,  1.7817e-02,\n",
      "         3.5432e-03,  1.8266e-02,  2.8419e-02, -3.7631e-02,  8.0272e-03,\n",
      "         1.7954e-02,  1.0494e-02, -3.3866e-02,  1.0930e-02,  1.7162e-02,\n",
      "        -4.2140e-02,  1.8427e-02,  7.4453e-03, -1.2608e-02,  1.1788e-02,\n",
      "         3.4104e-02, -3.3497e-02, -2.9512e-02,  1.5589e-02,  2.3984e-02,\n",
      "         4.0428e-02,  1.9060e-02,  2.4592e-02, -2.0933e-02,  2.4353e-02,\n",
      "        -4.0465e-02, -1.3791e-02,  4.7038e-03,  4.0609e-02,  2.3310e-02,\n",
      "         2.5741e-02, -1.2217e-02,  4.2276e-02,  2.8503e-03, -2.4246e-02,\n",
      "         4.9457e-02, -1.2035e-02,  2.5377e-02, -3.1431e-02,  1.7540e-02,\n",
      "         3.2406e-03,  1.8135e-02,  4.5612e-03,  9.4441e-04,  2.5781e-02,\n",
      "        -1.7972e-02, -8.6473e-03,  2.4420e-03, -9.6572e-03, -1.8807e-02,\n",
      "         5.1502e-02,  2.2559e-02,  4.6312e-03,  3.0699e-02, -3.5024e-02,\n",
      "         3.4219e-02,  1.0560e-02, -2.3922e-02, -3.9046e-02,  3.0311e-02,\n",
      "         3.3779e-02, -2.8842e-02,  2.3538e-02,  2.2279e-02,  3.6815e-02,\n",
      "         3.9032e-02,  1.9871e-03, -3.4831e-02,  2.2726e-02, -2.6663e-02,\n",
      "         5.7636e-03,  2.1674e-02])\n",
      "Parameter shape: torch.Size([512])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.weight\n",
      "Parameter value: tensor([[-0.0447,  0.0135, -0.0056,  ...,  0.0231,  0.0350, -0.0188],\n",
      "        [ 0.0051, -0.0010,  0.0045,  ...,  0.0078, -0.0461, -0.0199],\n",
      "        [-0.0223,  0.0226, -0.0158,  ...,  0.0271, -0.0451, -0.0110],\n",
      "        ...,\n",
      "        [ 0.0619, -0.0138,  0.0060,  ...,  0.0124,  0.0361,  0.0201],\n",
      "        [-0.0064, -0.0023,  0.0261,  ..., -0.0511,  0.0533,  0.0153],\n",
      "        [ 0.0159,  0.0281,  0.0235,  ..., -0.0438,  0.0173, -0.0296]])\n",
      "Parameter shape: torch.Size([10, 512])\n",
      "--------------------------------------------------\n",
      "Parameter name: linear_relu_stack.4.bias\n",
      "Parameter value: tensor([-0.0493, -0.0184, -0.0590, -0.0050, -0.0723,  0.1418, -0.0067,  0.0985,\n",
      "        -0.0249, -0.0741])\n",
      "Parameter shape: torch.Size([10])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print state_dict \n",
    "for name, param in model.state_dict().items():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter value: {param}\")\n",
    "    print(f\"Parameter shape: {param.shape}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0111, -0.0163,  0.0044,  ..., -0.0006,  0.0274,  0.0065],\n",
       "        [-0.0118,  0.0021,  0.0163,  ...,  0.0021, -0.0304,  0.0215],\n",
       "        [ 0.0244,  0.0233, -0.0097,  ..., -0.0152,  0.0350, -0.0285],\n",
       "        ...,\n",
       "        [ 0.0047, -0.0119, -0.0056,  ..., -0.0351,  0.0062,  0.0234],\n",
       "        [-0.0012,  0.0035, -0.0316,  ..., -0.0269,  0.0182,  0.0060],\n",
       "        [ 0.0156, -0.0253, -0.0042,  ..., -0.0286, -0.0180, -0.0263]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first layer weight\n",
    "model.state_dict()[\"linear_relu_stack.0.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Weights of fc1 layer: Parameter containing:\n",
      "tensor([[ 0.0111, -0.0163,  0.0044,  ..., -0.0006,  0.0274,  0.0065],\n",
      "        [-0.0118,  0.0021,  0.0163,  ...,  0.0021, -0.0304,  0.0215],\n",
      "        [ 0.0244,  0.0233, -0.0097,  ..., -0.0152,  0.0350, -0.0285],\n",
      "        ...,\n",
      "        [ 0.0047, -0.0119, -0.0056,  ..., -0.0351,  0.0062,  0.0234],\n",
      "        [-0.0012,  0.0035, -0.0316,  ..., -0.0269,  0.0182,  0.0060],\n",
      "        [ 0.0156, -0.0253, -0.0042,  ..., -0.0286, -0.0180, -0.0263]],\n",
      "       requires_grad=True)\n",
      "Bias of fc1 layer: Parameter containing:\n",
      "tensor([ 3.1299e-02, -2.4866e-02, -2.4687e-03,  3.3257e-02, -1.2573e-02,\n",
      "        -2.9465e-02,  3.6867e-02, -1.1514e-02, -1.7470e-03,  8.7670e-03,\n",
      "        -2.8088e-02, -2.8737e-02, -3.4039e-02, -1.5097e-02, -3.3807e-02,\n",
      "        -4.1252e-03, -1.3162e-02,  3.1557e-02, -2.3931e-02, -9.9615e-03,\n",
      "         1.5670e-02, -1.0803e-03, -1.2151e-02, -2.0947e-02, -1.8876e-02,\n",
      "         2.2011e-02, -3.3348e-02,  4.8116e-03,  8.6803e-03,  1.7623e-02,\n",
      "         1.6403e-02, -1.2514e-02,  7.9018e-03, -2.8916e-02, -1.0766e-02,\n",
      "        -7.5647e-03, -3.5550e-03, -3.2359e-02,  3.4937e-02, -2.5400e-02,\n",
      "         1.4325e-02,  4.7796e-03, -1.8049e-02,  7.3009e-03, -1.3749e-02,\n",
      "         3.7105e-02,  2.3368e-02,  4.2428e-03,  1.8882e-02, -2.4410e-02,\n",
      "         2.9200e-03,  2.7759e-02, -1.1613e-02, -2.3947e-02, -2.8069e-02,\n",
      "        -5.4490e-03,  1.4343e-02,  9.1595e-03, -7.5708e-03,  1.9226e-02,\n",
      "         8.9526e-03, -3.6449e-03,  3.6030e-02, -1.4603e-02,  3.1513e-02,\n",
      "         1.3502e-02,  2.1687e-02, -1.2270e-02,  1.8124e-02, -2.8407e-02,\n",
      "        -1.9915e-02, -2.2935e-02, -3.6503e-02,  1.1845e-02,  2.3900e-02,\n",
      "        -3.0695e-02, -4.0309e-03, -7.0003e-03, -3.5476e-02,  5.2369e-03,\n",
      "         2.2343e-02,  2.2138e-02, -3.0757e-02, -2.6127e-02,  1.3471e-03,\n",
      "        -5.1610e-03, -1.6784e-02,  1.0939e-02, -3.5012e-02,  5.6542e-03,\n",
      "         7.6482e-03,  2.9593e-02,  3.2525e-02, -3.0391e-02,  2.1811e-02,\n",
      "         7.6460e-03,  1.5949e-02,  1.9918e-02, -1.3567e-02,  1.2244e-02,\n",
      "        -1.9767e-02,  1.3861e-02,  1.7690e-02,  2.1168e-02,  3.2005e-02,\n",
      "         8.1267e-03, -2.6708e-03, -7.3254e-04,  4.2772e-04,  3.4820e-02,\n",
      "         1.0113e-02,  3.2301e-02,  2.1028e-02, -2.8408e-02,  4.1496e-02,\n",
      "        -9.7464e-03,  1.5589e-02, -7.5156e-03,  1.1652e-02, -1.5019e-02,\n",
      "         3.7560e-02,  3.2533e-03,  3.2581e-02,  1.7274e-02,  2.8570e-02,\n",
      "         8.2888e-03,  1.9975e-02, -2.7682e-02,  1.7965e-02, -1.7655e-02,\n",
      "        -2.5013e-02,  3.1347e-02,  3.7203e-02, -3.6390e-02, -1.2477e-02,\n",
      "         2.7538e-02,  3.6468e-02,  1.5251e-03, -3.3562e-02, -1.9580e-02,\n",
      "        -1.7604e-02,  2.5937e-02, -7.0241e-03,  1.3931e-02, -2.0829e-02,\n",
      "         3.0020e-02,  3.4157e-02, -8.3373e-03, -2.4119e-02, -9.1882e-03,\n",
      "        -9.0172e-03, -3.0521e-02,  2.7748e-02, -2.6568e-02,  1.7120e-02,\n",
      "        -9.6135e-03, -3.5436e-02,  1.1635e-02,  3.9039e-02,  3.7904e-02,\n",
      "         2.8246e-02,  1.9714e-02, -2.4613e-02,  2.9571e-02,  1.0809e-02,\n",
      "         2.1170e-02,  2.3050e-02, -3.3391e-02, -1.0468e-02,  1.5682e-03,\n",
      "        -2.5103e-02, -1.4289e-02, -5.3376e-03, -1.1421e-02,  1.4830e-02,\n",
      "         3.9414e-02,  1.8518e-02,  1.5456e-02, -1.1434e-03, -2.0770e-02,\n",
      "        -1.2762e-02, -2.4125e-02,  4.0295e-03,  3.5649e-02,  2.3033e-02,\n",
      "        -1.7503e-02,  1.8628e-02, -2.5089e-02,  7.5775e-03, -2.7324e-02,\n",
      "         4.4273e-03, -2.0846e-02,  3.0332e-02, -2.7007e-02, -1.6700e-02,\n",
      "         2.5385e-02,  3.6013e-02,  3.4034e-02, -1.9578e-02, -9.6259e-03,\n",
      "        -5.6950e-03,  2.0621e-02, -3.3474e-02, -1.1836e-02,  1.8623e-02,\n",
      "         7.8949e-04,  3.1272e-02, -3.2451e-02, -2.0683e-02, -2.0711e-03,\n",
      "        -2.3011e-02, -1.6059e-03, -8.6199e-03,  2.0143e-02, -1.8260e-02,\n",
      "        -1.1082e-04, -4.2506e-03, -1.9791e-02, -2.4167e-02,  6.9828e-03,\n",
      "        -1.3237e-02,  1.7496e-02,  2.1691e-02,  2.1745e-02, -2.9442e-02,\n",
      "         3.2760e-03,  1.2327e-04, -2.7517e-02, -3.3203e-02,  1.7948e-02,\n",
      "        -4.1579e-03,  3.5348e-02,  8.8844e-03, -2.3033e-02, -1.1942e-02,\n",
      "        -6.0597e-03,  3.4291e-02, -3.0517e-02,  2.9601e-02, -2.6078e-02,\n",
      "         1.5574e-02,  1.0520e-02, -1.3986e-02,  1.9278e-03, -2.7844e-02,\n",
      "         2.1302e-02, -2.1205e-02,  2.0413e-02,  2.7193e-02,  4.4633e-02,\n",
      "        -7.0556e-03, -2.3985e-02, -1.8159e-02,  1.2458e-02,  2.5090e-03,\n",
      "        -2.5556e-02,  8.9525e-03,  1.7325e-02, -2.8504e-02,  1.5161e-02,\n",
      "        -1.0499e-02, -8.4576e-03, -1.5134e-02,  1.0953e-02,  3.5435e-02,\n",
      "        -1.5580e-02,  2.9059e-02, -3.0180e-02,  6.7077e-03,  9.2657e-03,\n",
      "         1.2116e-02, -2.4062e-02,  1.7881e-02,  3.7909e-02,  2.0068e-02,\n",
      "        -8.0369e-03,  3.2289e-02, -2.5012e-02, -3.2694e-02,  6.4473e-03,\n",
      "        -2.0563e-02,  3.4726e-03, -5.8274e-03,  2.3828e-02,  1.3723e-02,\n",
      "         2.6953e-02,  3.4542e-02, -2.7218e-02,  2.2548e-02, -1.0528e-02,\n",
      "        -5.4055e-03, -8.8234e-03,  2.7123e-02, -3.0904e-02,  2.1617e-02,\n",
      "        -7.1774e-03, -3.2940e-02, -1.7699e-02,  2.5203e-02, -2.8679e-02,\n",
      "        -1.7260e-02,  3.3906e-02,  1.7779e-03,  1.1330e-02,  1.3766e-02,\n",
      "         2.9129e-02, -1.4812e-02,  2.7559e-02,  1.7467e-02,  3.0450e-02,\n",
      "        -2.0223e-02,  7.5231e-04, -2.9010e-02, -2.7132e-02, -1.4500e-02,\n",
      "        -2.7627e-03,  8.4280e-03, -1.3470e-02,  3.2618e-02, -8.3868e-03,\n",
      "        -2.1763e-02,  1.8390e-02, -1.0882e-02,  1.2659e-02,  3.2330e-02,\n",
      "         1.4918e-02, -2.9912e-02,  2.5727e-02, -1.8225e-02,  1.4592e-02,\n",
      "         4.3929e-03,  4.9089e-03, -3.4810e-04,  3.8420e-02, -2.5337e-02,\n",
      "         3.0074e-02,  9.2367e-03,  1.0044e-02,  1.7084e-02,  1.1968e-02,\n",
      "        -2.0558e-02,  2.8163e-02, -6.8584e-03, -2.3192e-02, -2.1855e-02,\n",
      "         1.3029e-02,  1.7606e-02, -1.8323e-02,  8.9034e-03,  2.0999e-02,\n",
      "         3.4973e-02, -4.8141e-03,  2.8852e-02,  1.7286e-02,  7.8676e-03,\n",
      "        -2.4323e-02,  3.5893e-02, -6.7025e-05,  1.7287e-03, -8.6086e-04,\n",
      "        -5.8470e-03, -3.0538e-02, -5.9944e-03, -2.9180e-02, -3.1429e-02,\n",
      "         2.9475e-03,  3.8736e-03,  3.6221e-02, -3.7608e-03,  3.5515e-02,\n",
      "        -1.9281e-02, -4.6984e-03, -5.4278e-04,  1.0123e-02, -2.5584e-02,\n",
      "        -2.9408e-02,  1.6776e-02, -3.6946e-03,  3.7872e-02,  2.7256e-02,\n",
      "        -1.2370e-02, -3.9490e-02,  6.7058e-03,  2.7221e-02, -5.2538e-03,\n",
      "         1.6104e-02,  2.2908e-02, -2.1802e-02, -2.0318e-02, -8.3841e-03,\n",
      "         1.1511e-02, -2.6728e-02, -3.6500e-03, -2.0268e-02, -1.4500e-02,\n",
      "        -2.0321e-02, -2.6775e-02,  3.5673e-03, -7.8840e-03, -3.2130e-02,\n",
      "         9.6583e-03, -1.9651e-03,  3.2119e-02,  1.4388e-02,  2.4268e-02,\n",
      "        -3.2213e-02, -1.9120e-02, -2.4276e-02, -1.9339e-02,  9.2263e-03,\n",
      "        -2.1061e-02,  1.1721e-02, -1.8989e-02,  1.5017e-03, -2.5178e-02,\n",
      "         3.3932e-02,  2.7741e-03, -2.6030e-02,  5.9817e-03, -1.6307e-02,\n",
      "         1.5907e-02, -8.9982e-03, -4.8762e-03, -1.1482e-02,  9.6751e-03,\n",
      "        -1.8039e-02,  5.3085e-03,  1.0920e-02,  1.6371e-02,  1.1237e-02,\n",
      "        -3.2413e-02, -2.0875e-02, -3.1973e-02, -1.8599e-02,  3.8776e-03,\n",
      "        -2.6364e-03, -9.5433e-03, -4.0878e-03, -5.4976e-03,  1.5441e-02,\n",
      "         1.5280e-02, -1.6320e-02, -2.6514e-02, -3.4665e-02,  1.6044e-02,\n",
      "        -1.9787e-02,  2.5679e-02,  1.3723e-02, -5.7191e-03, -3.1612e-02,\n",
      "        -1.5877e-02,  2.2682e-02,  2.9266e-02,  3.9722e-02,  1.8007e-02,\n",
      "        -4.1188e-03,  2.8856e-02, -2.3937e-02,  8.8238e-04,  1.9004e-02,\n",
      "         2.2901e-02,  2.6575e-02,  4.1701e-03, -2.1226e-02,  2.0712e-02,\n",
      "         9.9247e-03,  7.0419e-03, -3.5314e-02,  2.5485e-02,  1.9705e-02,\n",
      "        -2.0205e-02, -8.3748e-03, -3.3654e-02, -1.4061e-02, -2.9694e-02,\n",
      "        -1.1170e-02,  3.6333e-02,  4.4244e-03, -2.5612e-02, -5.6549e-03,\n",
      "        -1.8023e-02,  4.9357e-03,  1.1637e-02, -2.4017e-02,  3.2717e-02,\n",
      "         2.7862e-02, -2.6288e-02, -2.5246e-02, -6.5978e-03,  7.3019e-03,\n",
      "        -1.3901e-02,  1.2564e-02, -1.6922e-02, -2.3098e-02,  1.5124e-03,\n",
      "         2.2050e-02, -7.3732e-03, -2.6270e-02, -3.3003e-02,  9.7479e-03,\n",
      "         9.9669e-03, -2.9922e-03, -3.6913e-03, -1.6972e-02, -2.2801e-02,\n",
      "        -1.5953e-02, -1.7318e-03,  2.4411e-02,  1.2804e-02,  2.3136e-02,\n",
      "         1.0506e-02,  8.1458e-03], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Access the weights and bias of the first layer\n",
    "print(model.linear_relu_stack)\n",
    "# Access the first layer\n",
    "fc1_weights = model.linear_relu_stack[0].weight\n",
    "fc1_bias = model.linear_relu_stack[0].bias\n",
    "# Print the weights and bias\n",
    "print(\"Weights of fc1 layer:\", fc1_weights)\n",
    "print(\"Bias of fc1 layer:\", fc1_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model state_dict\n",
    "# only the model's state_dict is saved\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "# load model state_dict\n",
    "# model must be created before loading the state_dict\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "model2 = NN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of NN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g0/phgwvz7x3sz18gfzz4zv9rq40000gn/T/ipykernel_72634/309678342.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load(\"model.pth\"))\n",
    "# model2 is now loaded with the saved state_dict, but it is not trained\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
